{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjin/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  1.13 ; cuda:  cu117\n",
      "detectron2: 0.6\n",
      "/home/djjin/Mygit/X-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "os.environ[\"DATASET\"] = \"../datasets\"\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from hdecoder.BaseModel import BaseModel\n",
    "from hdecoder import build_model\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt\n",
    "from datasets.utils.vcoco_utils import get_random_images, walk_through_dir\n",
    "from utils.visualizer import draw_hoi_results, draw_obj_attentions, draw_hoi_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.arguments import load_vcoco_opt_command, load_vcoco_parser\n",
    "\n",
    "cmdline_args = load_vcoco_parser()\n",
    "# cmdline_args.conf_files = [os.path.join(home_dir, \"configs/hdecoder/vcoco_large.yaml\")]\n",
    "# model_path = '../backup/data/output/XDecoder_HOI_230814/Large_90E_0.5904mAP/00234900/default/raw_model_states.pt'\n",
    "\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/hdecoder/vcoco.yaml\")]\n",
    "# model_path = '../backup/data/output/XDecoder_HOI_230817/Small_90E_0.4857mAP/00232200/default/raw_model_states.pt'\n",
    "model_path = '../data/output/test/00216000_best/default/raw_model_states.pt'\n",
    "\n",
    "cmdline_args.overrides = ['WEIGHT', 'true', 'RESUME_FROM', model_path] \n",
    "\n",
    "opt = load_vcoco_opt_command(cmdline_args)\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_pth = os.path.join(opt['RESUME_FROM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/output/00216000_best/default/raw_model_states.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/djjin/Mygit/X-Decoder/notebooks/10_hoi_model_inference_test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/10_hoi_model_inference_test.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m BaseModel(opt, build_model(opt))\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_pth)\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/BaseModel.py:38\u001b[0m, in \u001b[0;36mBaseModel.from_pretrained\u001b[0;34m(self, load_dir)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mself\u001b[39m, load_dir):\n\u001b[0;32m---> 38\u001b[0m     state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(load_dir, map_location\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt[\u001b[39m\"\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     39\u001b[0m     state_dict \u001b[39m=\u001b[39m align_and_update_state_dicts(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstate_dict(), state_dict)\n\u001b[1;32m     40\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(state_dict, strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/output/00216000_best/default/raw_model_states.pt'"
     ]
    }
   ],
   "source": [
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained_pth).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "t.append(transforms.Resize(800, interpolation=Image.BICUBIC))\n",
    "transform = transforms.Compose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(home_dir)\n",
    "img_root = home_dir + '/datasets/v-coco/images/val2014'\n",
    "walk_through_dir(img_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = get_random_images(img_root, nums=50, seed=10)\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "org_images = []\n",
    "for image_pth in image_paths:\n",
    "    image_ori = Image.open(image_pth).convert('RGB')\n",
    "    \n",
    "    width = image_ori.size[0]\n",
    "    height = image_ori.size[1]\n",
    "    orig_size = (height, width)\n",
    "    result = model.model.hoi_inference(image_ori, orig_size, transform, thr=0.5)\n",
    "    results.append(result[-1])\n",
    "    org_images.append(image_ori)\n",
    "\n",
    "draw_hoi_results(org_images, results, is_save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_self_attn = model.model.hoid_head.encoder.transformer.encoder.layers[-1].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_multihead_attn = model.model.hoid_head.hoi_decoder.hopd_decoder.layers[-1].multihead_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "org_images = []\n",
    "for image_pth in image_paths:\n",
    "    image_ori = Image.open(image_pth).convert('RGB')\n",
    "    \n",
    "    width = image_ori.size[0]\n",
    "    height = image_ori.size[1]\n",
    "    orig_size = (height, width)\n",
    "    \n",
    "    conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
    "    \n",
    "    hooks = [\n",
    "        model.model.backbone.register_forward_hook(\n",
    "            lambda self, input, output: conv_features.append(output)\n",
    "        ),\n",
    "        encoder_self_attn.register_forward_hook(\n",
    "            lambda self, input, output: enc_attn_weights.append(output[1])\n",
    "        ),\n",
    "        decoder_multihead_attn.register_forward_hook(\n",
    "            lambda self, input, output: dec_attn_weights.append(output[1])\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    result = model.model.hoi_inference(image_ori, orig_size, transform, thr=0.5, return_only_outputs=True)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "\n",
    "    conv_features = conv_features[0]\n",
    "    enc_attn_weights = enc_attn_weights[0]\n",
    "    dec_attn_weights = dec_attn_weights[0]\n",
    "\n",
    "    # print(conv_features)\n",
    "    tensor_orig_size = torch.tensor([width, height, width, height], dtype=torch.float32).to(opt[\"device\"])\n",
    "    draw_obj_attentions(result, image_ori, tensor_orig_size, conv_features, dec_attn_weights, thr=0.5, cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "org_images = []\n",
    "for image_pth in image_paths:\n",
    "    image_ori = Image.open(image_pth).convert('RGB')\n",
    "    \n",
    "    width = image_ori.size[0]\n",
    "    height = image_ori.size[1]\n",
    "    orig_size = (height, width)\n",
    "    \n",
    "    conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
    "    \n",
    "    hooks = [\n",
    "        model.model.backbone.register_forward_hook(\n",
    "            lambda self, input, output: conv_features.append(output)\n",
    "        ),\n",
    "        encoder_self_attn.register_forward_hook(\n",
    "            lambda self, input, output: enc_attn_weights.append(output[1])\n",
    "        ),\n",
    "        decoder_multihead_attn.register_forward_hook(\n",
    "            lambda self, input, output: dec_attn_weights.append(output[1])\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    result = model.model.hoi_inference(image_ori, orig_size, transform, thr=0.5)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "\n",
    "    conv_features = conv_features[0]\n",
    "    enc_attn_weights = enc_attn_weights[0]\n",
    "    dec_attn_weights = dec_attn_weights[0]\n",
    "    print(result)\n",
    "    draw_hoi_attention(result, image_ori, conv_features, dec_attn_weights, cmap='jet', alpha=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_Decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
