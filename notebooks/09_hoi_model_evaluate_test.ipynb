{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  1.13 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "os.environ[\"DATASET\"] = \"../datasets\"\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from hdecoder.BaseModel import BaseModel\n",
    "from hdecoder import build_model\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:utils.arguments:Overrided DONT_LOAD_MODEL from True to False\n"
     ]
    }
   ],
   "source": [
    "from utils.arguments import load_vcoco_opt_command, load_vcoco_parser\n",
    "\n",
    "cmdline_args = load_vcoco_parser()\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/hdecoder/vcoco.yaml\")]\n",
    "\n",
    "model_path = '../data/output/test/00216000_best/default/raw_model_states.pt'\n",
    "cmdline_args.overrides = ['DONT_LOAD_MODEL', 'false', 'PYLEARN_MODEL', model_path] \n",
    "\n",
    "opt = load_vcoco_opt_command(cmdline_args)\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(opt['base_path'])\n",
    "print(opt[\"RESUME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trainer.distributed_trainer:Setting SAVE_DIR as data/output/test\n",
      "INFO:trainer.distributed_trainer:Using CUDA\n",
      "WARNING:trainer.utils.mpi_adapter:----------------\n",
      "WARNING:trainer.utils.mpi_adapter:MPI Adapter data\n",
      "WARNING:trainer.utils.mpi_adapter:----------------\n",
      "WARNING:trainer.utils.mpi_adapter:environment info: no MPI\n",
      "WARNING:trainer.utils.mpi_adapter:init method url: tcp://127.0.0.1:2222\n",
      "WARNING:trainer.utils.mpi_adapter:world size: 1\n",
      "WARNING:trainer.utils.mpi_adapter:local size: 1\n",
      "WARNING:trainer.utils.mpi_adapter:rank: 0\n",
      "WARNING:trainer.utils.mpi_adapter:local rank: 0\n",
      "WARNING:trainer.utils.mpi_adapter:master address: 127.0.0.1\n",
      "WARNING:trainer.utils.mpi_adapter:master port: 2222\n",
      "WARNING:trainer.utils.mpi_adapter:----------------\n",
      "INFO:trainer.distributed_trainer:Save config file to data/output/test/conf_copy.yaml\n",
      "INFO:trainer.distributed_trainer:Base learning rate: 0.0001\n",
      "INFO:trainer.distributed_trainer:Number of GPUs: 1\n",
      "INFO:trainer.distributed_trainer:Gradient accumulation steps: 1\n",
      "INFO:trainer.default_trainer:Imported base_dir at base_path ../\n",
      "INFO:trainer.default_trainer:Pipeline for training: HDecoderPipeline\n",
      "INFO:trainer.default_trainer:-----------------------------------------------\n",
      "INFO:trainer.default_trainer:Evaluating model ... \n",
      "INFO:base_dir.pipeline.HDecoderPipeline:CDNHOI(\n",
      "  (backbone): D2FocalNet(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=96, out_features=196, bias=True)\n",
      "              (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=96, out_features=196, bias=True)\n",
      "              (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (drop_path): DropPath(drop_prob=0.027)\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchEmbed(\n",
      "          (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=192, out_features=388, bias=True)\n",
      "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (drop_path): DropPath(drop_prob=0.055)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=192, out_features=388, bias=True)\n",
      "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (drop_path): DropPath(drop_prob=0.082)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchEmbed(\n",
      "          (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.109)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.136)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.164)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.191)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.218)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.245)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchEmbed(\n",
      "          (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=768, out_features=1540, bias=True)\n",
      "              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (drop_path): DropPath(drop_prob=0.273)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=768, out_features=1540, bias=True)\n",
      "              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (drop_path): DropPath(drop_prob=0.300)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (hoid_head): CDN(\n",
      "    (encoder): TransformerEncoderHOI(\n",
      "      (adapter_1): Conv2d(\n",
      "        96, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (adapter_2): Conv2d(\n",
      "        192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_2): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (adapter_3): Conv2d(\n",
      "        384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_3): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (input_proj): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (transformer): TransformerEncoderOnly(\n",
      "        (encoder): TransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (3): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (4): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (5): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 256\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "    )\n",
      "    (hoi_decoder): HDecoder(\n",
      "      (hopd_decoder): TransformerDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (interaction_decoder): TransformerDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (query_embed): Embedding(100, 512)\n",
      "    (obj_class_embed): Linear(in_features=512, out_features=82, bias=True)\n",
      "    (verb_class_embed): Linear(in_features=512, out_features=29, bias=True)\n",
      "    (sub_bbox_embed): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (2): Linear(in_features=512, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (obj_bbox_embed): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (2): Linear(in_features=512, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): SetCriterionHOI(\n",
      "    (matcher): HungarianMatcherHOI()\n",
      "  )\n",
      "  (postprocessors): PostProcessHOI()\n",
      ")\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw1.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw2.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_1, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_2, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.bias, Model Shape: torch.Size([196]) <-> Ckpt Shape: torch.Size([196])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.weight, Model Shape: torch.Size([196, 96]) <-> Ckpt Shape: torch.Size([196, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([96, 1, 5, 5]) <-> Ckpt Shape: torch.Size([96, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([96, 1, 7, 7]) <-> Ckpt Shape: torch.Size([96, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.weight, Model Shape: torch.Size([96, 96, 1, 1]) <-> Ckpt Shape: torch.Size([96, 96, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw1.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw2.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_1, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_2, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.bias, Model Shape: torch.Size([196]) <-> Ckpt Shape: torch.Size([196])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.weight, Model Shape: torch.Size([196, 96]) <-> Ckpt Shape: torch.Size([196, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([96, 1, 5, 5]) <-> Ckpt Shape: torch.Size([96, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([96, 1, 7, 7]) <-> Ckpt Shape: torch.Size([96, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.weight, Model Shape: torch.Size([96, 96, 1, 1]) <-> Ckpt Shape: torch.Size([96, 96, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.weight, Model Shape: torch.Size([192, 96, 3, 3]) <-> Ckpt Shape: torch.Size([192, 96, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw1.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw2.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.bias, Model Shape: torch.Size([388]) <-> Ckpt Shape: torch.Size([388])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.weight, Model Shape: torch.Size([388, 192]) <-> Ckpt Shape: torch.Size([388, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw1.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw2.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.bias, Model Shape: torch.Size([388]) <-> Ckpt Shape: torch.Size([388])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.weight, Model Shape: torch.Size([388, 192]) <-> Ckpt Shape: torch.Size([388, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.weight, Model Shape: torch.Size([384, 192, 3, 3]) <-> Ckpt Shape: torch.Size([384, 192, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.weight, Model Shape: torch.Size([768, 384, 3, 3]) <-> Ckpt Shape: torch.Size([768, 384, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw1.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw2.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.bias, Model Shape: torch.Size([1540]) <-> Ckpt Shape: torch.Size([1540])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.weight, Model Shape: torch.Size([1540, 768]) <-> Ckpt Shape: torch.Size([1540, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw1.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw2.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.bias, Model Shape: torch.Size([1540]) <-> Ckpt Shape: torch.Size([1540])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.weight, Model Shape: torch.Size([1540, 768]) <-> Ckpt Shape: torch.Size([1540, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.norm0.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.norm0.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.norm3.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.norm3.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.norm.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.norm.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.proj.weight, Model Shape: torch.Size([96, 3, 7, 7]) <-> Ckpt Shape: torch.Size([96, 3, 7, 7])\n",
      "INFO:utils.model:Loaded criterion.empty_weight, Model Shape: torch.Size([82]) <-> Ckpt Shape: torch.Size([82])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.weight, Model Shape: torch.Size([512, 96, 1, 1]) <-> Ckpt Shape: torch.Size([512, 96, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.weight, Model Shape: torch.Size([512, 192, 1, 1]) <-> Ckpt Shape: torch.Size([512, 192, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.weight, Model Shape: torch.Size([512, 384, 1, 1]) <-> Ckpt Shape: torch.Size([512, 384, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.input_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.input_proj.weight, Model Shape: torch.Size([512, 768, 1, 1]) <-> Ckpt Shape: torch.Size([512, 768, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.mask_features.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.mask_features.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.norm3.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.norm3.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.norm3.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.norm3.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.norm3.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.norm3.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.hopd_decoder.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.norm3.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.norm3.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.norm3.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.norm3.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.norm3.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.norm3.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.hoi_decoder.interaction_decoder.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.obj_bbox_embed.layers.0.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.obj_bbox_embed.layers.0.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.obj_bbox_embed.layers.1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.obj_bbox_embed.layers.1.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.obj_bbox_embed.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])\n",
      "INFO:utils.model:Loaded hoid_head.obj_bbox_embed.layers.2.weight, Model Shape: torch.Size([4, 512]) <-> Ckpt Shape: torch.Size([4, 512])\n",
      "INFO:utils.model:Loaded hoid_head.obj_class_embed.bias, Model Shape: torch.Size([82]) <-> Ckpt Shape: torch.Size([82])\n",
      "INFO:utils.model:Loaded hoid_head.obj_class_embed.weight, Model Shape: torch.Size([82, 512]) <-> Ckpt Shape: torch.Size([82, 512])\n",
      "INFO:utils.model:Loaded hoid_head.query_embed.weight, Model Shape: torch.Size([100, 512]) <-> Ckpt Shape: torch.Size([100, 512])\n",
      "INFO:utils.model:Loaded hoid_head.sub_bbox_embed.layers.0.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.sub_bbox_embed.layers.0.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.sub_bbox_embed.layers.1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.sub_bbox_embed.layers.1.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.sub_bbox_embed.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])\n",
      "INFO:utils.model:Loaded hoid_head.sub_bbox_embed.layers.2.weight, Model Shape: torch.Size([4, 512]) <-> Ckpt Shape: torch.Size([4, 512])\n",
      "INFO:utils.model:Loaded hoid_head.verb_class_embed.bias, Model Shape: torch.Size([29]) <-> Ckpt Shape: torch.Size([29])\n",
      "INFO:utils.model:Loaded hoid_head.verb_class_embed.weight, Model Shape: torch.Size([29, 512]) <-> Ckpt Shape: torch.Size([29, 512])\n",
      "INFO:trainer.default_trainer:Evaluation start ...\n",
      "INFO:detectron2.data.common:Serializing 4946 elements to byte tensors and concatenating them all ...\n",
      "INFO:detectron2.data.common:Serialized dataset takes 3.69 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[[[ 0.6782,  0.6611,  0.6440,  ...,  0.3015,  0.1987,  0.1302],\n",
      "          [ 0.6782,  0.6611,  0.6440,  ...,  0.3015,  0.2159,  0.1474],\n",
      "          [ 0.6611,  0.6440,  0.6440,  ...,  0.3186,  0.2501,  0.2159],\n",
      "          ...,\n",
      "          [ 0.0617,  0.0446,  0.0446,  ...,  1.0207,  0.9865,  0.9693],\n",
      "          [ 0.0446,  0.0275,  0.0446,  ...,  1.0036,  0.9693,  0.9522],\n",
      "          [ 0.0275,  0.0275,  0.0446,  ...,  1.0036,  0.9693,  0.9522]],\n",
      "\n",
      "         [[ 1.5735,  1.5385,  1.5035,  ...,  0.9958,  0.9433,  0.8732],\n",
      "          [ 1.5735,  1.5385,  1.5035,  ...,  0.9958,  0.9433,  0.8908],\n",
      "          [ 1.5560,  1.5210,  1.5035,  ...,  0.9783,  0.9608,  0.9258],\n",
      "          ...,\n",
      "          [-0.3172, -0.3172, -0.2997,  ...,  0.9083,  0.8557,  0.8032],\n",
      "          [-0.3347, -0.3347, -0.3172,  ...,  0.9083,  0.8557,  0.8032],\n",
      "          [-0.3347, -0.3347, -0.3172,  ...,  0.9083,  0.8557,  0.8032]],\n",
      "\n",
      "         [[ 2.3115,  2.3115,  2.2941,  ...,  2.1198,  2.0501,  1.9804],\n",
      "          [ 2.2941,  2.2941,  2.2941,  ...,  2.1024,  2.0327,  1.9804],\n",
      "          [ 2.2593,  2.2593,  2.2767,  ...,  2.0501,  2.0153,  1.9804],\n",
      "          ...,\n",
      "          [-0.3028, -0.3028, -0.3028,  ...,  0.7255,  0.7081,  0.6906],\n",
      "          [-0.2854, -0.2854, -0.2854,  ...,  0.7081,  0.6906,  0.6906],\n",
      "          [-0.2854, -0.2854, -0.2854,  ...,  0.7081,  0.6906,  0.6906]]],\n",
      "\n",
      "\n",
      "        [[[-0.9144, -0.8801, -0.8116,  ..., -0.8116, -0.8288, -0.8288],\n",
      "          [-0.9315, -0.8801, -0.7774,  ..., -0.8116, -0.8288, -0.8288],\n",
      "          [-0.9486, -0.8801, -0.7260,  ..., -0.8288, -0.8459, -0.8459],\n",
      "          ...,\n",
      "          [ 1.4660,  1.4488,  1.4317,  ...,  0.4727,  0.4899,  0.5070],\n",
      "          [ 1.4488,  1.4317,  1.4146,  ...,  0.5070,  0.5070,  0.5070],\n",
      "          [ 1.4317,  1.4317,  1.4146,  ...,  0.5412,  0.5241,  0.5070]],\n",
      "\n",
      "         [[-0.8775, -0.8249, -0.7024,  ..., -0.8074, -0.8249, -0.8424],\n",
      "          [-0.8775, -0.8249, -0.6499,  ..., -0.8074, -0.8249, -0.8424],\n",
      "          [-0.8775, -0.7899, -0.5798,  ..., -0.8074, -0.8424, -0.8599],\n",
      "          ...,\n",
      "          [ 1.5560,  1.5385,  1.5210,  ...,  0.5406,  0.5581,  0.5756],\n",
      "          [ 1.5385,  1.5385,  1.5210,  ...,  0.5756,  0.5756,  0.5756],\n",
      "          [ 1.5385,  1.5385,  1.5210,  ...,  0.6106,  0.5931,  0.5756]],\n",
      "\n",
      "         [[-0.6166, -0.6688, -0.7734,  ..., -0.7211, -0.7386, -0.7386],\n",
      "          [-0.6863, -0.7211, -0.8083,  ..., -0.7211, -0.7386, -0.7386],\n",
      "          [-0.7734, -0.7908, -0.8431,  ..., -0.7211, -0.7386, -0.7560],\n",
      "          ...,\n",
      "          [ 1.1264,  1.1089,  1.0915,  ...,  0.1155,  0.1329,  0.1503],\n",
      "          [ 1.1612,  1.1438,  1.1264,  ...,  0.1503,  0.1503,  0.1503],\n",
      "          [ 1.1961,  1.1786,  1.1612,  ...,  0.1852,  0.1678,  0.1503]]]],\n",
      "       device='cuda:0')\n",
      "{'res2': tensor([[[[-4.3114e-01,  7.6113e-01,  5.0956e-01,  ..., -2.7304e-01,\n",
      "            2.4522e-01,  6.6862e-01],\n",
      "          [-9.0116e-01, -8.8872e-01, -1.3342e+00,  ...,  8.4286e-01,\n",
      "           -6.4258e-01, -9.9569e-01],\n",
      "          [-3.6656e-01,  2.6991e-01, -2.7344e+00,  ..., -1.6330e+00,\n",
      "           -1.1201e+00, -1.5890e-01],\n",
      "          ...,\n",
      "          [-1.1461e+00,  9.8072e-02, -5.2555e-01,  ..., -1.6940e+00,\n",
      "            3.2050e-01,  1.2899e+00],\n",
      "          [-7.7269e-01,  5.0488e-01,  1.7409e+00,  ..., -1.1704e+00,\n",
      "            1.8928e+00,  3.0596e-01],\n",
      "          [ 8.9114e-01, -3.2432e-01,  1.1956e+00,  ..., -1.6175e+00,\n",
      "            2.2858e-01, -1.9758e+00]],\n",
      "\n",
      "         [[ 6.6354e-01,  5.1887e-01,  5.6363e-01,  ..., -4.6921e-01,\n",
      "           -5.9507e-01, -2.8438e-01],\n",
      "          [ 8.4604e-01,  1.9132e-01,  7.3039e-01,  ..., -3.3827e-02,\n",
      "           -3.8060e-01, -2.9157e-01],\n",
      "          [ 4.3958e-01, -3.5230e-01,  5.2988e-01,  ...,  1.6818e-02,\n",
      "           -2.5913e-01, -3.1712e-01],\n",
      "          ...,\n",
      "          [-1.7744e+00, -8.7248e-01, -5.8235e-01,  ...,  1.1573e-02,\n",
      "           -4.1745e-01,  3.1666e-01],\n",
      "          [-1.9345e+00, -3.3552e-01, -2.8899e-02,  ...,  9.3923e-02,\n",
      "           -9.6209e-02,  3.7257e-01],\n",
      "          [-1.2951e+00,  2.1664e-01,  1.5220e-01,  ...,  3.1600e-01,\n",
      "            4.4996e-01,  7.0922e-01]],\n",
      "\n",
      "         [[-7.6013e-01,  2.9623e-02, -3.6048e-02,  ..., -3.9487e-01,\n",
      "           -2.4515e-01, -2.5327e-01],\n",
      "          [-1.0558e+00,  1.7452e+00,  2.1112e+00,  ...,  6.8517e-01,\n",
      "            1.4823e+00,  1.1638e+00],\n",
      "          [-7.8351e-01,  6.2267e-01,  1.1049e+00,  ...,  7.7151e-01,\n",
      "            5.6987e-01,  1.2226e+00],\n",
      "          ...,\n",
      "          [ 2.8299e-01,  1.2699e+00,  9.6187e-01,  ..., -2.8457e+00,\n",
      "           -1.1090e-01,  2.8339e+00],\n",
      "          [-3.0374e-02,  2.1063e-01, -6.2466e-01,  ..., -1.3991e+00,\n",
      "            2.8501e+00,  1.0892e+00],\n",
      "          [ 5.1675e-01, -1.0073e-02,  2.1515e+00,  ...,  9.6973e-01,\n",
      "            2.2144e-01,  4.3054e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.5832e-03,  9.4797e-01,  3.1898e-01,  ..., -3.6438e-01,\n",
      "            3.0633e-01,  3.6440e-01],\n",
      "          [ 2.6774e-01,  1.7427e+00,  1.1556e+00,  ..., -3.5434e-01,\n",
      "            8.3364e-01,  9.8588e-02],\n",
      "          [ 2.2380e-01, -7.2993e-02, -2.3530e-01,  ...,  2.2653e-01,\n",
      "            2.2037e+00,  1.4073e+00],\n",
      "          ...,\n",
      "          [ 8.4699e-01,  2.4720e+00, -3.4220e-01,  ...,  9.4491e-02,\n",
      "            3.7361e-01,  7.3226e-01],\n",
      "          [-5.0594e-02,  8.4558e-01,  1.9256e-01,  ...,  1.9944e+00,\n",
      "            8.7604e-01, -9.9644e-01],\n",
      "          [ 1.1448e+00, -5.7320e-01,  1.0513e+00,  ...,  1.5716e+00,\n",
      "            1.0573e+00,  9.2805e-01]],\n",
      "\n",
      "         [[ 6.0564e-01,  1.4224e+00,  1.9800e+00,  ...,  2.1745e+00,\n",
      "            2.3489e+00,  2.1599e+00],\n",
      "          [ 8.6599e-01, -7.0043e-01,  7.5266e-01,  ...,  1.0101e+00,\n",
      "            7.6682e-01,  8.8628e-01],\n",
      "          [ 1.9350e+00, -1.1073e+00, -3.4962e-01,  ..., -1.2876e-01,\n",
      "            2.5363e-01,  1.1809e+00],\n",
      "          ...,\n",
      "          [-6.8286e-02, -8.6210e-01,  4.5844e-01,  ...,  2.2438e-01,\n",
      "            8.1988e-01, -1.2169e+00],\n",
      "          [ 4.1300e-01,  7.3166e-01,  1.5992e+00,  ...,  1.0618e+00,\n",
      "            4.2988e-01, -1.4456e+00],\n",
      "          [ 4.1527e-01, -1.0231e+00, -4.1250e-01,  ..., -1.6068e+00,\n",
      "            6.7637e-01, -3.5292e-01]],\n",
      "\n",
      "         [[-1.0356e+00, -1.0651e+00, -7.7499e-01,  ..., -7.6867e-01,\n",
      "           -9.9906e-01, -1.1087e+00],\n",
      "          [ 2.2563e-01,  2.8933e-01, -2.3301e-02,  ..., -8.7560e-01,\n",
      "           -6.9436e-01, -3.9532e-01],\n",
      "          [ 6.9804e-01,  2.2500e-01,  2.3870e-01,  ..., -6.1344e-01,\n",
      "            5.4307e-02, -6.1874e-01],\n",
      "          ...,\n",
      "          [ 2.8386e-01,  5.4524e-01,  3.3209e-01,  ..., -2.2257e+00,\n",
      "            1.3286e-01, -3.9981e-01],\n",
      "          [ 3.9481e-01,  3.5206e-01, -4.0952e-01,  ..., -1.9655e-01,\n",
      "            1.2880e-01, -1.1635e+00],\n",
      "          [ 2.8741e-01,  5.5950e-01,  5.2615e-01,  ...,  3.7307e-01,\n",
      "            5.1039e-01,  9.9755e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.4022e-01, -1.2610e+00,  7.4359e-01,  ..., -1.2121e+00,\n",
      "           -2.9953e-01, -2.3177e-02],\n",
      "          [-1.0528e+00, -1.5468e-01, -3.0958e-01,  ...,  1.3471e+00,\n",
      "            9.1023e-01,  1.3010e+00],\n",
      "          [-4.3980e-01,  8.5546e-01,  4.5583e-01,  ..., -1.5314e-01,\n",
      "            2.2150e-01,  9.8555e-01],\n",
      "          ...,\n",
      "          [-1.8345e-01,  1.6566e-01, -1.3532e-01,  ...,  1.7023e+00,\n",
      "            9.1699e-01,  7.1700e-01],\n",
      "          [-4.5412e-01,  8.3255e-01,  5.3161e-04,  ..., -7.1932e-01,\n",
      "            9.8591e-01,  6.3767e-01],\n",
      "          [-1.5680e+00, -7.1033e-01, -1.4516e+00,  ...,  1.2013e+00,\n",
      "           -4.6273e-01, -8.4362e-01]],\n",
      "\n",
      "         [[ 1.6173e+00,  3.1925e+00,  3.4206e+00,  ..., -5.5212e-01,\n",
      "           -5.5729e-01, -6.4948e-01],\n",
      "          [ 2.8303e+00,  2.9378e+00,  6.3792e-01,  ..., -6.5890e-01,\n",
      "           -5.0956e-01, -5.2991e-01],\n",
      "          [ 3.6435e+00,  1.7783e+00,  9.1032e-01,  ..., -5.9831e-01,\n",
      "           -5.0760e-01, -3.6883e-01],\n",
      "          ...,\n",
      "          [ 1.1153e+00,  3.4122e-02,  3.9492e-01,  ...,  2.4163e-01,\n",
      "            6.2233e-01,  6.1017e-01],\n",
      "          [ 9.7713e-01,  8.7966e-02,  3.3642e-01,  ...,  5.4373e-01,\n",
      "            6.2951e-01,  8.7111e-01],\n",
      "          [ 1.0800e+00,  8.3106e-01,  8.3195e-01,  ...,  9.6582e-02,\n",
      "            4.5375e-01,  7.7521e-01]],\n",
      "\n",
      "         [[-4.1418e-02, -1.2801e+00,  1.6344e+00,  ...,  6.3613e-01,\n",
      "            9.9544e-01,  1.3004e+00],\n",
      "          [ 4.2598e-01,  3.6417e-01, -3.4844e-02,  ...,  3.5454e-01,\n",
      "            3.2009e-01, -6.2781e-01],\n",
      "          [ 6.4199e-01, -5.9075e-01, -1.2787e+00,  ..., -1.2929e+00,\n",
      "           -1.0537e+00, -2.8388e-01],\n",
      "          ...,\n",
      "          [-1.4257e+00, -7.3322e-01,  3.9736e-01,  ...,  1.9560e+00,\n",
      "           -2.0438e+00, -6.5477e-01],\n",
      "          [-7.7460e-01,  9.5250e-01,  1.7902e+00,  ...,  1.7150e+00,\n",
      "            2.1091e+00,  6.3257e-01],\n",
      "          [-1.1488e+00,  8.6328e-01,  5.4255e-01,  ...,  3.4546e-01,\n",
      "            5.7882e-02,  8.3689e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.4077e-01, -1.2584e+00,  6.0981e-01,  ...,  5.2210e-01,\n",
      "            6.3910e-01,  1.0888e+00],\n",
      "          [ 3.8426e-01, -1.4077e+00,  5.8002e-01,  ..., -2.9804e-01,\n",
      "           -2.7430e-01,  1.5163e+00],\n",
      "          [-4.2078e-01, -3.7216e-01,  1.3567e+00,  ..., -7.9795e-01,\n",
      "           -6.9868e-01,  1.5144e+00],\n",
      "          ...,\n",
      "          [-1.0861e-01, -6.5920e-01, -9.6402e-01,  ..., -3.8791e-01,\n",
      "            5.4476e-01,  8.4730e-01],\n",
      "          [ 8.1067e-02,  2.4355e-01,  2.3287e-01,  ...,  5.2950e-01,\n",
      "            8.0056e-01,  6.0091e-01],\n",
      "          [ 1.6007e-01,  8.5754e-01,  5.9073e-01,  ...,  4.9367e-01,\n",
      "            4.0081e-01,  1.8805e-01]],\n",
      "\n",
      "         [[ 9.9360e-01,  2.4691e-01,  6.9548e-02,  ...,  6.4400e-01,\n",
      "            8.9402e-01,  7.5704e-01],\n",
      "          [-2.3504e-02, -1.7981e+00, -8.3341e-01,  ...,  2.0635e+00,\n",
      "            1.1544e+00, -7.0044e-01],\n",
      "          [ 5.0441e-01, -8.2492e-01, -4.6151e-01,  ...,  1.3000e+00,\n",
      "            1.1005e-02, -7.1536e-01],\n",
      "          ...,\n",
      "          [ 6.3347e-01,  2.7085e-01,  1.6977e+00,  ...,  1.8552e+00,\n",
      "            2.0298e-01,  2.1929e-01],\n",
      "          [ 1.0657e-01,  5.9197e-01,  4.2328e-01,  ...,  2.7773e-01,\n",
      "            7.8879e-01,  2.2989e-01],\n",
      "          [-2.3903e-01,  6.0740e-01, -3.5252e-01,  ..., -9.6226e-02,\n",
      "            2.3049e-01,  7.0600e-01]],\n",
      "\n",
      "         [[-5.6401e-01,  4.2865e-02, -8.6008e-03,  ..., -2.6245e-01,\n",
      "           -2.2162e-02, -1.9804e-03],\n",
      "          [-2.9849e-01,  1.3177e+00,  5.1961e-01,  ...,  9.2473e-01,\n",
      "            2.0669e-01,  2.6146e-01],\n",
      "          [ 1.6162e-01,  3.8779e-01,  4.6248e-01,  ...,  5.0954e-01,\n",
      "           -1.4620e-02,  2.3608e-01],\n",
      "          ...,\n",
      "          [-6.3639e-01, -2.1831e-01, -2.8723e-01,  ..., -2.6007e-01,\n",
      "            6.4015e-01,  1.0352e+00],\n",
      "          [-1.2943e+00, -1.6028e+00, -1.0874e+00,  ...,  4.3275e-01,\n",
      "            5.2662e-01,  6.6394e-01],\n",
      "          [-5.1508e-01,  3.8465e-01, -1.5975e-01,  ...,  9.6958e-01,\n",
      "            6.7183e-01, -1.5562e-01]]]], device='cuda:0'), 'res3': tensor([[[[-0.4485,  1.5609,  1.9910,  ...,  1.9893,  2.1151,  0.5478],\n",
      "          [ 0.1780,  1.7038,  1.6711,  ..., -0.3645,  0.3839, -0.0775],\n",
      "          [ 0.2446, -0.5786,  0.4573,  ..., -1.3115,  0.4681, -0.2234],\n",
      "          ...,\n",
      "          [-0.9729, -1.8627, -1.1099,  ...,  2.4694, -0.7971,  0.0279],\n",
      "          [-0.5515, -1.9557, -0.3181,  ...,  1.1294,  1.1729,  0.3406],\n",
      "          [ 0.4692, -0.0598,  0.3900,  ..., -0.7378, -0.2176, -0.4339]],\n",
      "\n",
      "         [[-0.3053, -0.0134, -0.6565,  ..., -0.5507, -0.3124, -0.7020],\n",
      "          [-0.9425,  0.2801,  0.3371,  ..., -0.4446, -0.5897, -0.3615],\n",
      "          [-0.0760, -0.5644, -0.3710,  ..., -1.0631, -0.5332, -0.6515],\n",
      "          ...,\n",
      "          [-1.6716, -1.4058,  0.0746,  ..., -0.8955, -0.8157, -0.8852],\n",
      "          [-0.5774, -1.4676, -1.6459,  ..., -0.3571,  0.0898,  0.2770],\n",
      "          [-0.7299, -1.6496, -0.8309,  ..., -0.8666, -0.3316, -1.2416]],\n",
      "\n",
      "         [[-0.3075,  0.5245,  0.6113,  ..., -0.1881, -0.1079,  0.1528],\n",
      "          [ 0.9412,  0.4330,  0.4639,  ...,  0.2862,  0.5143,  0.2152],\n",
      "          [ 0.2768,  0.1191, -0.0318,  ..., -0.2819,  0.4588, -0.2236],\n",
      "          ...,\n",
      "          [-0.1620, -0.9007, -0.9122,  ..., -1.1340,  0.3315, -0.3201],\n",
      "          [-0.6404,  1.5364,  0.3844,  ..., -1.3482, -0.0059,  0.3647],\n",
      "          [-0.0089,  0.3461, -1.5038,  ..., -0.7847, -1.0128, -0.1307]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2274,  0.1247,  0.2190,  ..., -0.5913, -0.2740, -0.6063],\n",
      "          [ 0.8633, -1.2158,  0.1431,  ..., -0.6315, -0.8194, -0.3640],\n",
      "          [ 0.8888, -0.3469, -0.1275,  ..., -0.0185, -0.7333,  0.0556],\n",
      "          ...,\n",
      "          [-0.3037,  0.0162,  0.3094,  ..., -0.0500, -1.7010, -0.6414],\n",
      "          [ 0.6984, -0.1377, -0.7510,  ...,  0.8049,  0.8607, -1.2612],\n",
      "          [-0.0757,  0.2866, -0.0515,  ..., -1.0506,  0.1535, -0.0196]],\n",
      "\n",
      "         [[ 0.0478,  0.1113, -0.5409,  ..., -0.1270,  0.2599,  0.0954],\n",
      "          [-1.2402,  3.0070,  0.5574,  ...,  0.4793,  1.5290, -0.0365],\n",
      "          [-1.0113,  1.9392, -0.0387,  ...,  1.1813,  0.7681, -0.3639],\n",
      "          ...,\n",
      "          [-0.8158,  0.8584,  0.5891,  ..., -0.8132,  1.4848,  0.0998],\n",
      "          [-1.4198,  1.1993,  1.1522,  ..., -0.5839,  0.2699,  1.4604],\n",
      "          [-1.6448,  0.7404, -0.5780,  ..., -0.2658,  0.3563, -0.2312]],\n",
      "\n",
      "         [[-0.4035, -1.2182, -0.5386,  ..., -0.2727, -0.6500, -0.9640],\n",
      "          [ 0.5780, -0.1609, -1.1819,  ...,  1.1280,  0.8941, -1.5729],\n",
      "          [-0.0647,  0.4875,  0.6837,  ...,  1.7237,  1.6338, -0.0209],\n",
      "          ...,\n",
      "          [ 0.4809,  2.7033,  1.0745,  ..., -1.2366,  0.8859, -0.6529],\n",
      "          [-0.2593,  2.2074,  0.4595,  ...,  1.1128,  0.8754,  0.8962],\n",
      "          [ 1.1137,  1.4017,  2.1106,  ...,  0.1353,  3.0267,  0.7463]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3860, -0.0597,  0.1586,  ..., -0.1790, -0.4715,  0.7933],\n",
      "          [ 0.3908,  1.5846, -0.3269,  ..., -0.4786,  0.5317,  0.8010],\n",
      "          [ 0.2034,  0.1325, -0.8578,  ..., -0.3555,  0.7341,  0.6870],\n",
      "          ...,\n",
      "          [-2.0181, -1.3137, -0.7401,  ..., -0.7453, -0.9242, -0.2135],\n",
      "          [-1.1255,  1.0329,  0.8427,  ..., -0.7769, -1.4518,  0.1399],\n",
      "          [-0.7711, -0.0163,  0.3861,  ..., -0.3864, -2.1359, -0.5390]],\n",
      "\n",
      "         [[-0.2644, -0.4686, -0.4528,  ..., -0.6142,  0.0736, -0.7381],\n",
      "          [-0.0569,  0.2778,  0.1354,  ..., -0.4852,  0.6208, -0.4319],\n",
      "          [ 0.2764,  0.4264, -0.0508,  ..., -0.2669, -0.0036, -0.5739],\n",
      "          ...,\n",
      "          [ 0.1215, -0.1433, -0.1922,  ..., -0.0910, -0.6940, -1.0015],\n",
      "          [ 0.3724,  0.9209,  1.5218,  ...,  0.4946,  1.0929, -0.5775],\n",
      "          [-0.0434,  0.1491, -0.0789,  ..., -0.4134, -0.5853, -0.1390]],\n",
      "\n",
      "         [[ 0.1221, -0.4483,  0.4181,  ..., -0.1938,  0.3352, -1.6711],\n",
      "          [ 0.0606, -0.4565,  1.3508,  ...,  0.1708,  0.1555,  0.4963],\n",
      "          [ 0.0611, -0.5972,  0.6148,  ...,  0.9398,  0.3845,  0.6996],\n",
      "          ...,\n",
      "          [-0.3021,  0.7465,  0.3086,  ...,  0.2903,  0.0920,  0.2248],\n",
      "          [ 0.0736, -0.3054, -0.5929,  ...,  0.3085,  0.4883,  0.0262],\n",
      "          [-0.6665, -0.2049, -0.1497,  ..., -0.7406, -0.3332, -0.4450]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8283,  0.2644,  0.6432,  ...,  0.9538,  0.2699, -0.1080],\n",
      "          [-0.2741,  0.2577,  0.9512,  ...,  0.1188, -0.3250,  0.9260],\n",
      "          [-0.5049,  0.2156,  0.6965,  ...,  1.3408,  0.0201,  0.2256],\n",
      "          ...,\n",
      "          [ 0.5834,  0.5872,  0.5553,  ..., -0.7746,  0.8865,  1.0763],\n",
      "          [-0.1980, -0.2637,  0.0266,  ...,  0.0155, -0.5944,  0.8400],\n",
      "          [ 0.3450,  1.2388,  1.4646,  ..., -0.2848, -0.7073,  0.3257]],\n",
      "\n",
      "         [[-1.7015, -1.1889, -1.6158,  ..., -0.1057, -1.0736, -0.1790],\n",
      "          [-1.8927,  0.5016,  0.2552,  ...,  2.4300,  0.0083,  0.6372],\n",
      "          [-2.8005,  0.1123, -0.5227,  ...,  0.9845,  0.2428, -0.4162],\n",
      "          ...,\n",
      "          [-0.5865,  1.0802, -0.4369,  ..., -1.3469, -0.3862, -1.1682],\n",
      "          [-2.1737,  1.2137,  0.2363,  ...,  0.5694,  0.3817,  0.3920],\n",
      "          [-1.5646, -0.3809,  0.2045,  ..., -1.3559, -0.2403, -0.1821]],\n",
      "\n",
      "         [[-1.6166,  1.9183,  0.9551,  ...,  0.1662,  0.8632,  0.7117],\n",
      "          [-0.5772,  1.6876,  0.6279,  ...,  1.8455,  1.6534,  1.5642],\n",
      "          [-0.6708,  1.0236,  0.8185,  ...,  2.1837,  1.1114,  1.2244],\n",
      "          ...,\n",
      "          [-0.6480, -1.1665, -0.1221,  ..., -0.6729,  0.4460, -0.5887],\n",
      "          [ 0.5932, -0.1667,  0.7256,  ...,  0.3995, -0.9715,  0.9176],\n",
      "          [-0.2399, -0.6063, -0.2711,  ..., -0.2615, -0.2410,  0.1530]]]],\n",
      "       device='cuda:0'), 'res4': tensor([[[[-1.4017e-01,  1.2337e-01,  2.4208e-01,  ...,  1.3398e-01,\n",
      "            8.9762e-02,  5.9230e-02],\n",
      "          [-5.0897e-02, -3.9969e-01, -7.8575e-01,  ..., -4.3363e-01,\n",
      "           -5.1384e-02,  4.1182e-01],\n",
      "          [-4.1913e-01, -9.3943e-01, -1.0314e+00,  ..., -1.3056e+00,\n",
      "           -6.7654e-01, -3.4936e-01],\n",
      "          ...,\n",
      "          [-2.2379e-01, -2.6945e-01,  6.1543e-01,  ..., -4.1918e-01,\n",
      "            4.7170e-01,  9.4686e-01],\n",
      "          [ 3.1201e-01,  2.6721e-01,  8.0714e-01,  ...,  2.0173e-02,\n",
      "            6.2335e-01,  9.8010e-01],\n",
      "          [ 1.0234e+00,  1.3137e+00,  8.1988e-01,  ...,  6.7580e-02,\n",
      "            1.9392e-01,  2.4655e-01]],\n",
      "\n",
      "         [[-3.9267e-01, -9.3609e-02,  8.0044e-02,  ...,  4.1855e-01,\n",
      "            5.7086e-01,  2.2762e-01],\n",
      "          [-1.2955e-01, -3.2066e-01, -2.2826e-01,  ..., -2.4460e-02,\n",
      "            3.0746e-01,  5.2030e-01],\n",
      "          [ 1.1290e-02, -2.4410e-01,  2.1227e-02,  ..., -2.5010e-02,\n",
      "            4.3157e-01,  5.9363e-01],\n",
      "          ...,\n",
      "          [-2.0294e-02,  5.6200e-01,  1.0642e+00,  ..., -5.2701e-01,\n",
      "           -6.2209e-01, -8.7845e-01],\n",
      "          [ 7.2744e-01,  2.8107e-01, -5.8103e-03,  ..., -5.0678e-01,\n",
      "           -3.5741e-01, -3.1970e-01],\n",
      "          [ 5.4590e-01, -1.3872e-01, -7.0229e-01,  ..., -6.4089e-01,\n",
      "           -3.8160e-01, -8.2596e-01]],\n",
      "\n",
      "         [[-2.5662e+00, -2.9345e+00, -2.6883e+00,  ..., -1.5089e+00,\n",
      "           -1.5965e+00, -1.5086e+00],\n",
      "          [-2.5390e+00, -2.1956e+00, -2.4538e+00,  ..., -1.5914e+00,\n",
      "           -1.6384e+00, -1.2193e+00],\n",
      "          [-1.6424e+00, -1.4679e+00, -1.0545e+00,  ..., -1.0653e+00,\n",
      "           -1.2664e+00, -7.8975e-01],\n",
      "          ...,\n",
      "          [ 1.6720e+00,  1.8787e+00,  1.4436e+00,  ..., -4.0997e-01,\n",
      "           -8.0383e-02, -2.6024e-01],\n",
      "          [ 1.0451e+00,  1.0738e+00,  1.2546e+00,  ..., -9.4431e-01,\n",
      "           -4.8171e-01, -3.0895e-01],\n",
      "          [ 6.8567e-01,  1.1495e+00,  1.4636e+00,  ..., -1.3116e+00,\n",
      "           -1.3115e+00, -1.2691e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.7555e-01,  4.9384e-02, -1.5907e-02,  ..., -2.1234e-01,\n",
      "           -2.2194e-01, -1.2878e-01],\n",
      "          [-3.2370e-01, -6.3553e-01, -6.5977e-01,  ..., -6.4319e-01,\n",
      "           -6.3960e-01, -5.2610e-01],\n",
      "          [-9.2836e-02, -7.6661e-01, -5.1424e-01,  ..., -4.7866e-01,\n",
      "           -6.5845e-01, -6.1484e-01],\n",
      "          ...,\n",
      "          [-1.4753e+00, -1.5375e+00, -1.4839e+00,  ...,  6.7192e-01,\n",
      "            4.4987e-01,  9.9091e-02],\n",
      "          [-1.8687e+00, -2.1165e+00, -1.4825e+00,  ...,  7.5676e-01,\n",
      "            2.6281e-01,  9.4822e-01],\n",
      "          [-1.8215e+00, -2.0125e+00, -1.3982e+00,  ...,  3.2086e-01,\n",
      "           -6.5733e-02,  3.6315e-01]],\n",
      "\n",
      "         [[ 1.6494e+00,  2.1975e+00,  2.6491e+00,  ...,  2.6670e+00,\n",
      "            2.2366e+00,  1.8288e+00],\n",
      "          [ 2.2639e+00,  1.9870e+00,  3.2053e+00,  ...,  2.6067e+00,\n",
      "            2.2761e+00,  2.0184e+00],\n",
      "          [ 2.7075e+00,  3.4795e+00,  3.7323e+00,  ...,  3.5033e+00,\n",
      "            3.1479e+00,  2.3754e+00],\n",
      "          ...,\n",
      "          [ 7.9444e-01,  1.1676e+00,  1.1139e+00,  ...,  2.3582e-01,\n",
      "            1.6920e-01,  5.1760e-01],\n",
      "          [ 1.7837e+00,  1.7356e+00,  1.0599e+00,  ...,  4.3013e-01,\n",
      "           -8.7523e-02,  1.7061e-01],\n",
      "          [ 2.2664e+00,  2.1085e+00,  7.9777e-01,  ...,  1.1642e+00,\n",
      "            6.0606e-01,  7.8050e-01]],\n",
      "\n",
      "         [[-2.2021e-01, -3.5453e-01, -2.1604e-01,  ..., -4.4397e-01,\n",
      "           -4.4964e-02,  1.0041e-01],\n",
      "          [ 1.1494e-01,  7.3722e-02,  4.1847e-02,  ...,  8.3609e-02,\n",
      "            2.7337e-01,  2.9160e-01],\n",
      "          [ 1.8168e-01,  2.0675e-01, -5.3573e-04,  ...,  4.2586e-01,\n",
      "            5.6427e-01,  3.1118e-01],\n",
      "          ...,\n",
      "          [ 1.6743e-01,  4.6338e-01,  4.0648e-01,  ...,  9.9211e-03,\n",
      "            4.6357e-01,  3.8718e-01],\n",
      "          [ 3.1531e-01,  5.9458e-01,  6.7770e-01,  ..., -4.7119e-01,\n",
      "            3.4664e-01,  2.7167e-01],\n",
      "          [ 1.4992e-02,  2.7716e-01,  6.2441e-01,  ..., -1.1163e+00,\n",
      "           -7.1092e-01, -4.5335e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.0580e-02,  9.6222e-01,  1.9762e+00,  ...,  2.1791e-03,\n",
      "            2.5974e-01,  1.9883e+00],\n",
      "          [-4.8539e-01, -4.2895e-02,  1.5691e+00,  ..., -2.1388e-01,\n",
      "           -1.1867e-01,  1.9514e+00],\n",
      "          [-2.0364e-01,  3.9877e-01,  1.6178e+00,  ..., -8.3237e-01,\n",
      "           -6.8335e-01,  1.0804e+00],\n",
      "          ...,\n",
      "          [-6.7356e-01, -2.9575e-01, -9.6123e-01,  ..., -5.5219e-02,\n",
      "            2.3237e-01,  4.6371e-01],\n",
      "          [-5.6193e-01, -3.7382e-01, -6.3451e-01,  ...,  4.8644e-02,\n",
      "            3.4968e-01,  6.8315e-01],\n",
      "          [-4.9274e-02, -7.6785e-04, -3.2214e-01,  ...,  3.2775e-01,\n",
      "            2.5884e-01,  3.9378e-01]],\n",
      "\n",
      "         [[-8.2247e-01, -9.1912e-01, -6.6648e-01,  ...,  1.0897e+00,\n",
      "            3.6709e-01, -2.7462e-01],\n",
      "          [-7.0689e-01, -1.1120e+00, -8.6823e-01,  ...,  8.7960e-01,\n",
      "            6.0427e-01,  1.4288e-01],\n",
      "          [-4.9885e-01, -7.7517e-01, -4.0935e-01,  ...,  1.1187e+00,\n",
      "            1.2971e+00,  3.3875e-01],\n",
      "          ...,\n",
      "          [-1.5224e-01, -2.0614e-01, -2.9235e-01,  ..., -7.3463e-01,\n",
      "           -5.3159e-01, -2.7508e-01],\n",
      "          [-4.4565e-02, -4.1148e-01, -7.3529e-01,  ..., -9.1626e-01,\n",
      "           -4.6986e-01, -4.6203e-01],\n",
      "          [-2.1739e-01, -4.9992e-01, -8.7691e-01,  ..., -1.0532e+00,\n",
      "           -8.6364e-01, -6.7298e-01]],\n",
      "\n",
      "         [[-1.4233e+00, -1.5377e+00, -1.1275e+00,  ...,  5.7991e-01,\n",
      "           -4.6528e-01, -7.3367e-01],\n",
      "          [-1.2246e+00, -1.2862e+00, -1.1375e+00,  ...,  4.7922e-01,\n",
      "           -4.2679e-01, -7.0053e-01],\n",
      "          [-4.2514e-01, -4.7086e-01, -5.0519e-01,  ...,  9.0106e-01,\n",
      "           -4.8610e-01, -5.0357e-01],\n",
      "          ...,\n",
      "          [-1.3589e+00, -1.3821e+00, -1.2544e+00,  ...,  3.7667e-01,\n",
      "            1.4897e-02, -3.3697e-01],\n",
      "          [-1.9244e+00, -1.9447e+00, -1.8697e+00,  ..., -4.9019e-01,\n",
      "           -5.7155e-01, -6.1373e-01],\n",
      "          [-1.6334e+00, -1.8793e+00, -1.8685e+00,  ..., -6.4879e-01,\n",
      "           -7.5195e-01, -5.7763e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.1578e-01, -4.9804e-01, -5.9711e-01,  ..., -4.0402e-01,\n",
      "           -2.5711e-01, -2.4128e-01],\n",
      "          [-2.3424e-01,  3.0753e-01,  1.9500e-03,  ..., -7.6524e-01,\n",
      "           -5.5292e-01, -4.1823e-01],\n",
      "          [ 4.8284e-01,  1.1510e+00,  1.2123e-01,  ..., -6.4054e-01,\n",
      "           -5.6717e-01, -9.7366e-01],\n",
      "          ...,\n",
      "          [-4.1072e-01, -2.0299e-01, -6.3394e-01,  ..., -7.6792e-01,\n",
      "           -4.1288e-01, -1.0191e-01],\n",
      "          [-3.3895e-03, -8.7726e-02, -4.7694e-01,  ..., -4.8132e-01,\n",
      "           -3.9635e-01, -2.7541e-01],\n",
      "          [ 4.9841e-01,  6.8854e-01,  1.8130e-01,  ..., -3.1073e-01,\n",
      "           -3.8375e-01, -4.5512e-01]],\n",
      "\n",
      "         [[ 2.5758e-01,  7.9554e-01,  1.2507e+00,  ...,  1.2508e+00,\n",
      "            4.4132e-01, -5.3550e-01],\n",
      "          [ 1.1611e+00,  6.6042e-01,  8.5927e-01,  ..., -1.3636e-01,\n",
      "           -7.4366e-01, -1.2538e+00],\n",
      "          [ 2.0962e+00,  1.1899e+00,  9.9346e-01,  ..., -1.2371e+00,\n",
      "           -8.3851e-01, -1.1027e+00],\n",
      "          ...,\n",
      "          [ 2.2827e+00,  1.7070e+00,  1.5014e+00,  ...,  2.0623e+00,\n",
      "            1.8868e+00,  1.7940e+00],\n",
      "          [ 1.6686e+00,  1.2414e+00,  1.5639e+00,  ...,  1.5401e+00,\n",
      "            1.2527e+00,  1.3420e+00],\n",
      "          [ 1.6609e+00,  1.7969e+00,  1.9018e+00,  ...,  1.3687e+00,\n",
      "            1.2652e+00,  1.1305e+00]],\n",
      "\n",
      "         [[ 1.8536e-02,  7.9408e-04, -4.3841e-01,  ...,  7.8344e-01,\n",
      "            6.3939e-01,  3.3452e-01],\n",
      "          [-3.2785e-01,  9.9222e-02,  3.0933e-01,  ...,  1.3554e+00,\n",
      "            9.6452e-01,  3.0748e-01],\n",
      "          [-5.0347e-01,  2.9764e-01,  1.1489e+00,  ...,  1.3988e+00,\n",
      "            9.4402e-01, -1.4608e-01],\n",
      "          ...,\n",
      "          [-1.4097e-01,  3.8585e-01,  1.1016e+00,  ..., -4.4595e-01,\n",
      "           -3.0313e-02,  2.2537e-01],\n",
      "          [-2.0534e-01,  3.8750e-01,  1.1854e+00,  ..., -2.3895e-01,\n",
      "           -2.3273e-03,  2.3882e-01],\n",
      "          [-4.0814e-02,  5.8028e-01,  1.1978e+00,  ..., -3.6949e-01,\n",
      "           -3.5111e-01, -1.7334e-01]]]], device='cuda:0'), 'res5': tensor([[[[-7.3814e-02,  6.0703e-02,  1.1541e-01,  ...,  1.1833e-01,\n",
      "            6.6867e-02, -9.3742e-02],\n",
      "          [ 1.1843e-01,  3.6152e-01,  3.0427e-01,  ...,  3.0579e-01,\n",
      "            3.3601e-01,  8.4839e-02],\n",
      "          [ 1.6164e-01,  3.6104e-01,  3.1024e-01,  ...,  2.9409e-01,\n",
      "            3.3970e-01,  1.3714e-01],\n",
      "          ...,\n",
      "          [ 1.1383e+00,  3.8515e-01, -9.9550e-01,  ...,  1.3811e-01,\n",
      "            2.6286e-01,  9.0279e-02],\n",
      "          [ 7.9872e-01,  5.0508e-01, -9.2394e-01,  ...,  2.5173e-01,\n",
      "            1.5462e-01, -1.1799e-01],\n",
      "          [ 1.6121e-02, -3.0361e-01, -8.9168e-01,  ...,  1.0227e-01,\n",
      "           -8.4853e-02, -2.0832e-01]],\n",
      "\n",
      "         [[-5.4753e-01, -1.7416e-01, -8.1836e-02,  ..., -1.8991e-01,\n",
      "           -2.4126e-01, -4.7810e-01],\n",
      "          [ 1.2701e-01,  3.4765e-01,  3.3494e-01,  ...,  2.7664e-01,\n",
      "            5.3001e-01,  3.8151e-02],\n",
      "          [ 8.7013e-02,  5.6781e-02,  1.4517e-01,  ...,  1.4041e-01,\n",
      "            2.3306e-01,  4.7340e-02],\n",
      "          ...,\n",
      "          [ 9.4171e-02,  1.9433e-01,  7.0028e-01,  ...,  9.4544e-02,\n",
      "            5.1044e-01,  4.8088e-02],\n",
      "          [ 1.7726e-01,  2.2956e-01, -2.2958e-01,  ...,  1.1975e+00,\n",
      "            2.1405e+00,  1.5736e-01],\n",
      "          [-1.0956e-01, -1.1016e-02, -3.5853e-01,  ..., -1.9698e-01,\n",
      "            3.7956e-01, -6.3602e-01]],\n",
      "\n",
      "         [[-4.0018e-01, -1.9878e-01, -2.0737e-01,  ..., -1.5415e-01,\n",
      "           -2.9739e-01, -3.7385e-01],\n",
      "          [ 6.8365e-02, -1.6111e-01, -1.5157e-01,  ..., -2.9862e-01,\n",
      "           -5.1061e-01, -8.3971e-02],\n",
      "          [ 6.3612e-02, -1.8487e-01, -2.5880e-01,  ..., -3.3823e-01,\n",
      "           -3.5884e-01,  8.7134e-02],\n",
      "          ...,\n",
      "          [ 2.0410e+00,  1.8479e+00,  2.4824e+00,  ..., -3.1175e-01,\n",
      "           -4.5989e-01,  5.5429e-01],\n",
      "          [ 1.6072e+00,  2.3954e+00,  6.3622e-01,  ..., -6.0304e-01,\n",
      "           -1.0489e+00, -1.1735e-02],\n",
      "          [ 5.6956e-01, -1.6887e-01, -4.9127e-01,  ..., -9.3582e-02,\n",
      "           -1.0565e+00,  2.8016e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9782e-01, -2.8014e-01, -2.5419e-01,  ..., -2.0533e-01,\n",
      "           -2.6289e-01, -3.0977e-01],\n",
      "          [-1.9024e-01, -4.9510e-01, -2.7216e-01,  ..., -2.0290e-01,\n",
      "           -3.7657e-01, -2.1985e-01],\n",
      "          [-1.6048e-01, -3.2719e-01, -2.0089e-01,  ..., -1.4085e-01,\n",
      "           -1.9675e-01, -1.4889e-01],\n",
      "          ...,\n",
      "          [ 5.4049e-01,  3.0697e-01, -5.0225e-03,  ...,  6.5001e-02,\n",
      "           -7.9690e-02, -1.0695e-01],\n",
      "          [ 1.1548e-01,  1.7640e-01,  7.3743e-02,  ...,  1.9991e-01,\n",
      "           -3.1997e-01, -2.3149e-01],\n",
      "          [-8.9006e-02, -3.2109e-01, -2.0091e-01,  ...,  1.5197e-01,\n",
      "            1.4299e-01, -7.1368e-02]],\n",
      "\n",
      "         [[ 1.4276e-01,  1.1168e-01,  9.8050e-02,  ...,  3.4486e-02,\n",
      "            6.8080e-02,  1.1626e-01],\n",
      "          [ 8.6997e-02,  2.5901e-01,  2.1597e-01,  ...,  7.7991e-02,\n",
      "            1.5811e-01,  8.7843e-02],\n",
      "          [ 1.3018e-01,  3.6125e-01,  3.4565e-01,  ...,  2.1084e-01,\n",
      "            2.7026e-01,  1.2092e-01],\n",
      "          ...,\n",
      "          [ 3.7777e-01,  6.7506e-01,  1.2938e+00,  ...,  6.6774e-01,\n",
      "            1.0472e+00,  2.9465e-01],\n",
      "          [ 3.6440e-01,  7.9444e-01,  9.4176e-01,  ...,  1.6158e+00,\n",
      "            2.4387e+00,  7.8266e-01],\n",
      "          [ 1.4189e-01,  3.5957e-01,  6.3146e-01,  ...,  6.7951e-01,\n",
      "            1.8905e+00,  5.8961e-01]],\n",
      "\n",
      "         [[-2.7535e-03,  8.1546e-02,  8.6967e-02,  ...,  5.9121e-02,\n",
      "           -2.3647e-02, -1.4302e-01],\n",
      "          [ 9.6833e-02,  3.3158e-01,  2.3087e-01,  ...,  2.3044e-01,\n",
      "            1.8777e-01, -2.0049e-02],\n",
      "          [ 1.1796e-01,  2.8677e-01,  2.1097e-01,  ...,  2.6789e-01,\n",
      "            2.7084e-01,  5.8262e-02],\n",
      "          ...,\n",
      "          [-1.7090e+00, -2.2479e+00, -2.1965e+00,  ...,  2.1840e-01,\n",
      "            5.1731e-01,  2.3529e-01],\n",
      "          [-1.2301e+00, -2.2565e+00, -1.6243e+00,  ...,  2.9703e-01,\n",
      "            7.9718e-01,  3.4492e-01],\n",
      "          [-2.6329e-01, -7.5425e-01, -5.3862e-01,  ...,  6.8654e-02,\n",
      "            2.3249e-01,  1.0463e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.7985e-01, -9.8994e-02, -1.9881e-02,  ...,  1.3175e+00,\n",
      "            3.0697e-01,  2.9537e-01],\n",
      "          [-1.5979e-01,  3.6107e-02,  2.2419e-01,  ...,  1.4924e+00,\n",
      "            1.0563e+00,  1.1235e+00],\n",
      "          [-6.4315e-02,  3.0756e-01,  6.0886e-01,  ...,  5.8228e-01,\n",
      "            7.5389e-01,  8.2375e-01],\n",
      "          ...,\n",
      "          [ 2.6560e-01,  7.4264e-01,  7.3284e-01,  ...,  9.1465e-01,\n",
      "            9.2385e-01,  3.0070e-01],\n",
      "          [ 2.5890e-01,  7.2163e-01,  4.7548e-01,  ...,  1.0965e+00,\n",
      "            1.0597e+00,  2.4344e-01],\n",
      "          [ 2.7932e-02,  1.3222e-01,  1.3469e-01,  ...,  3.0790e-01,\n",
      "            2.1653e-01,  2.8167e-02]],\n",
      "\n",
      "         [[-6.4573e-01, -5.4530e-02, -8.5974e-02,  ...,  5.0841e-01,\n",
      "           -2.4174e-01, -3.3628e-01],\n",
      "          [ 1.2516e+00,  1.5055e+00,  7.8790e-01,  ...,  1.4097e+00,\n",
      "            3.5975e-01,  7.3448e-03],\n",
      "          [ 8.1288e-02,  5.0788e-01, -1.0327e+00,  ...,  1.2081e+00,\n",
      "            5.7621e-01,  5.2460e-01],\n",
      "          ...,\n",
      "          [-1.3343e-02,  2.0250e-01,  3.7417e-01,  ...,  1.9276e-01,\n",
      "            3.2844e-01,  7.0531e-02],\n",
      "          [ 3.5285e-02,  2.3921e-01,  2.5481e-02,  ...,  1.2284e-01,\n",
      "            3.0309e-01,  5.8108e-02],\n",
      "          [-4.1158e-01, -3.7392e-01, -3.0445e-01,  ..., -3.7986e-01,\n",
      "           -3.7793e-01, -2.3741e-01]],\n",
      "\n",
      "         [[ 7.4321e-01,  8.2973e-01,  3.3297e-01,  ...,  1.4154e+00,\n",
      "            6.9331e-01,  1.5494e+00],\n",
      "          [ 2.9140e+00,  1.7573e+00,  8.1456e-01,  ...,  1.0424e+00,\n",
      "            4.1328e-01,  2.8180e+00],\n",
      "          [ 5.4940e-01,  5.2064e-01,  1.3250e+00,  ..., -3.9109e-01,\n",
      "           -2.1408e-01,  7.3114e-01],\n",
      "          ...,\n",
      "          [ 5.8638e-01, -1.8702e-01, -9.2661e-01,  ..., -5.0321e-01,\n",
      "           -2.3582e-01,  4.5995e-01],\n",
      "          [ 5.4658e-01, -3.4543e-01, -4.5912e-01,  ..., -9.0337e-01,\n",
      "           -6.2927e-01,  4.0650e-01],\n",
      "          [ 7.3341e-01,  5.1406e-01,  2.4033e-01,  ..., -3.4729e-01,\n",
      "            3.6410e-02,  3.7874e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.7271e-01, -9.1456e-01, -7.4565e-01,  ..., -4.3851e-01,\n",
      "           -2.1374e-01, -3.1423e-01],\n",
      "          [-7.5274e-01, -1.0986e+00, -8.9079e-01,  ..., -3.5304e-01,\n",
      "            1.1177e-01, -3.1927e-01],\n",
      "          [-2.8714e-01, -4.2807e-01, -1.9379e-01,  ..., -6.7470e-01,\n",
      "           -3.8674e-01, -2.7568e-01],\n",
      "          ...,\n",
      "          [-2.3722e-03,  4.7932e-02,  7.9569e-02,  ..., -4.6711e-02,\n",
      "           -8.4276e-02, -7.0322e-02],\n",
      "          [ 2.6590e-02,  3.2816e-01,  1.8507e-01,  ..., -2.7093e-02,\n",
      "            1.2226e-02, -6.2093e-02],\n",
      "          [ 1.6611e-02,  9.4148e-02,  6.8183e-02,  ..., -1.2427e-02,\n",
      "            8.9225e-03, -8.9173e-02]],\n",
      "\n",
      "         [[-1.7087e-01, -2.8123e-01, -1.0433e-01,  ..., -3.2263e-01,\n",
      "           -1.6572e-01, -2.7555e-01],\n",
      "          [-4.7693e-01, -7.4585e-01, -5.9603e-01,  ..., -3.9433e-01,\n",
      "           -7.9961e-01, -6.4150e-01],\n",
      "          [-3.8814e-01, -8.7305e-01, -1.1152e+00,  ..., -8.8070e-01,\n",
      "           -7.8305e-01, -7.2330e-01],\n",
      "          ...,\n",
      "          [ 1.3548e-01,  5.8173e-01,  8.3294e-01,  ...,  8.0628e-01,\n",
      "            9.2486e-01,  2.4172e-01],\n",
      "          [ 2.2502e-01,  8.7881e-01,  6.0098e-01,  ...,  1.0656e+00,\n",
      "            1.1368e+00,  2.2898e-01],\n",
      "          [ 1.9943e-01,  4.0661e-01,  3.5674e-01,  ...,  5.9644e-01,\n",
      "            4.6845e-01,  1.8939e-01]],\n",
      "\n",
      "         [[-1.8585e-01, -5.1395e-01,  6.1437e-03,  ..., -1.9362e-01,\n",
      "           -7.5896e-02, -3.5306e-01],\n",
      "          [ 5.9997e-02,  2.0915e-01,  5.9094e-01,  ..., -1.2255e-01,\n",
      "           -3.5547e-01, -5.1401e-01],\n",
      "          [-4.6901e-02,  5.3386e-01,  1.2633e+00,  ..., -3.0894e-01,\n",
      "           -3.7770e-01, -4.5289e-01],\n",
      "          ...,\n",
      "          [ 4.6158e-02, -9.6360e-03, -1.6516e-01,  ..., -1.6690e-01,\n",
      "           -8.0825e-02,  3.0686e-02],\n",
      "          [-1.1507e-01, -3.2402e-01, -1.7578e-01,  ..., -5.5585e-01,\n",
      "           -4.7939e-01, -5.5232e-02],\n",
      "          [-5.0563e-02, -7.4788e-02, -3.3959e-02,  ..., -2.3705e-01,\n",
      "           -1.3822e-01, -4.7888e-02]]]], device='cuda:0')}\n",
      "1\n",
      "tensor([[[[ 1.9455,  1.9283,  1.9112,  ...,  1.3290,  1.2947,  1.2433],\n",
      "          [ 1.9283,  1.9112,  1.8941,  ...,  1.3290,  1.2947,  1.2605],\n",
      "          [ 1.8941,  1.8770,  1.8427,  ...,  1.3118,  1.3290,  1.3118],\n",
      "          ...,\n",
      "          [ 0.2330,  0.2159,  0.1816,  ...,  1.1920,  1.1748,  1.1577],\n",
      "          [ 0.2844,  0.2501,  0.2159,  ...,  1.2091,  1.1920,  1.1748],\n",
      "          [ 0.3015,  0.2672,  0.2330,  ...,  1.2091,  1.1920,  1.1748]],\n",
      "\n",
      "         [[ 1.4860,  1.4860,  1.4860,  ...,  1.0833,  1.0308,  0.9783],\n",
      "          [ 1.4685,  1.4685,  1.4685,  ...,  1.0658,  1.0308,  0.9958],\n",
      "          [ 1.3985,  1.3985,  1.3810,  ...,  1.0308,  1.0133,  1.0133],\n",
      "          ...,\n",
      "          [ 0.6457,  0.6282,  0.5931,  ...,  1.1709,  1.1534,  1.1359],\n",
      "          [ 0.6982,  0.6632,  0.6282,  ...,  1.1884,  1.1709,  1.1534],\n",
      "          [ 0.7157,  0.6807,  0.6457,  ...,  1.1884,  1.1709,  1.1534]],\n",
      "\n",
      "         [[ 1.0566,  1.0741,  1.1089,  ...,  0.8998,  0.8475,  0.7952],\n",
      "          [ 1.0392,  1.0566,  1.0915,  ...,  0.8649,  0.8301,  0.7778],\n",
      "          [ 0.9521,  0.9695,  1.0044,  ...,  0.7603,  0.7429,  0.7255],\n",
      "          ...,\n",
      "          [ 1.0044,  0.9869,  0.9521,  ...,  1.1438,  1.1264,  1.1089],\n",
      "          [ 1.0566,  1.0218,  0.9869,  ...,  1.1612,  1.1438,  1.1264],\n",
      "          [ 1.0741,  1.0392,  1.0044,  ...,  1.1612,  1.1438,  1.1264]]],\n",
      "\n",
      "\n",
      "        [[[-0.1780, -0.2465, -0.3321,  ...,  0.1474,  0.1302,  0.1302],\n",
      "          [-0.2123, -0.2808, -0.3493,  ...,  0.1474,  0.1302,  0.1474],\n",
      "          [-0.3150, -0.3493, -0.4006,  ...,  0.1474,  0.1474,  0.1645],\n",
      "          ...,\n",
      "          [-0.0581, -0.0581, -0.0753,  ...,  1.1063,  1.0892,  1.0550],\n",
      "          [-0.0753, -0.0753, -0.0924,  ...,  1.0892,  1.0550,  1.0207],\n",
      "          [-0.0753, -0.0753, -0.0924,  ...,  1.0721,  1.0378,  1.0036]],\n",
      "\n",
      "         [[ 0.2780,  0.2080,  0.1204,  ...,  0.5056,  0.4881,  0.4881],\n",
      "          [ 0.2430,  0.1730,  0.1029,  ...,  0.5056,  0.4881,  0.5056],\n",
      "          [ 0.1380,  0.1029,  0.0504,  ...,  0.5056,  0.5056,  0.5231],\n",
      "          ...,\n",
      "          [-0.3873, -0.3873, -0.4048,  ...,  0.8032,  0.7857,  0.7507],\n",
      "          [-0.4048, -0.4048, -0.4223,  ...,  0.7857,  0.7507,  0.7157],\n",
      "          [-0.4048, -0.4048, -0.4223,  ...,  0.7682,  0.7332,  0.6982]],\n",
      "\n",
      "         [[ 0.3943,  0.3246,  0.2375,  ...,  0.6035,  0.5861,  0.5861],\n",
      "          [ 0.3595,  0.2898,  0.2200,  ...,  0.6035,  0.5861,  0.6035],\n",
      "          [ 0.2549,  0.2200,  0.1678,  ...,  0.6035,  0.6035,  0.6209],\n",
      "          ...,\n",
      "          [-0.5120, -0.5120, -0.5294,  ...,  0.1852,  0.1678,  0.1678],\n",
      "          [-0.5468, -0.5468, -0.5468,  ...,  0.1678,  0.1329,  0.0980],\n",
      "          [-0.5643, -0.5643, -0.5468,  ...,  0.1503,  0.1155,  0.0806]]]],\n",
      "       device='cuda:0')\n",
      "{'res2': tensor([[[[-1.3486e+00,  6.0874e-02,  3.6448e-02,  ..., -1.2463e+00,\n",
      "           -6.1346e-01, -2.1383e+00],\n",
      "          [-5.4130e-01,  5.2804e-01,  1.9706e-01,  ...,  1.1106e+00,\n",
      "            1.0729e+00, -5.8173e-01],\n",
      "          [-4.9527e-01,  1.4659e+00, -7.8131e-01,  ..., -6.1401e-01,\n",
      "            3.4247e-01,  9.3619e-01],\n",
      "          ...,\n",
      "          [-1.9362e-02, -1.5550e+00,  2.3181e+00,  ...,  2.5199e+00,\n",
      "           -1.0465e+00, -5.6653e-01],\n",
      "          [-3.6799e-01,  2.2647e-01, -1.9019e+00,  ...,  1.1797e+00,\n",
      "           -1.0819e+00, -7.2331e-01],\n",
      "          [-7.4136e-01,  5.5713e-01,  4.0683e-01,  ..., -1.1024e+00,\n",
      "           -5.4902e-01, -1.7773e+00]],\n",
      "\n",
      "         [[-1.3056e+00, -2.1231e+00, -1.3820e+00,  ..., -1.6165e-01,\n",
      "           -5.2695e-01, -4.8059e-01],\n",
      "          [-2.0085e+00, -2.6365e+00, -2.2111e+00,  ..., -2.3693e-01,\n",
      "           -4.1920e-01, -2.2040e-01],\n",
      "          [-2.1805e+00, -2.4790e+00, -1.8999e+00,  ..., -3.3715e-01,\n",
      "           -6.1271e-01, -4.3129e-01],\n",
      "          ...,\n",
      "          [ 8.1082e-01, -4.8161e-03,  1.9524e-01,  ...,  1.0342e-01,\n",
      "            2.9773e-01,  2.5495e-01],\n",
      "          [ 8.8443e-01, -9.0316e-02,  2.8821e-01,  ..., -1.1027e-01,\n",
      "           -8.6877e-03,  1.2482e-01],\n",
      "          [ 5.1298e-01, -2.0842e-01,  2.4922e-02,  ..., -1.8950e-01,\n",
      "            3.5688e-02,  5.6006e-02]],\n",
      "\n",
      "         [[-9.3181e-01,  1.0186e+00, -8.0915e-01,  ..., -1.2091e+00,\n",
      "           -3.3432e-01,  8.2603e-01],\n",
      "          [-2.3461e-01,  5.1742e-01,  3.5199e-01,  ..., -5.9795e-01,\n",
      "           -8.5801e-01,  1.0824e+00],\n",
      "          [-1.7031e-01,  4.8286e-01,  1.2499e+00,  ..., -1.2131e+00,\n",
      "            9.9792e-01,  2.9022e-01],\n",
      "          ...,\n",
      "          [-1.0226e+00,  1.1789e+00, -8.1872e-01,  ..., -1.6425e-01,\n",
      "            4.3981e-01,  1.3664e-01],\n",
      "          [-1.0893e+00,  4.3417e-01,  2.6624e-02,  ..., -2.6783e-01,\n",
      "            2.6053e+00,  1.6760e-01],\n",
      "          [-3.8668e-01,  1.1265e+00,  4.1767e-01,  ...,  1.3496e+00,\n",
      "            1.7941e+00,  1.3727e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2546e-01,  6.4361e-01, -6.8840e-01,  ...,  1.0184e-04,\n",
      "           -6.2105e-01, -1.3819e-01],\n",
      "          [ 6.8714e-01,  1.1540e+00, -7.1649e-01,  ...,  5.4379e-01,\n",
      "            3.7359e-01,  3.7348e-01],\n",
      "          [-1.7839e-02,  7.6953e-01, -7.3247e-01,  ...,  1.0006e-01,\n",
      "            2.8874e-01,  1.3460e+00],\n",
      "          ...,\n",
      "          [ 1.1477e+00,  5.3185e-01, -4.6840e-02,  ...,  3.0507e-01,\n",
      "           -5.1412e-01,  1.1957e+00],\n",
      "          [ 4.8535e-01,  6.6906e-01,  1.2398e-02,  ...,  7.2123e-01,\n",
      "           -1.2201e+00,  3.0657e-01],\n",
      "          [-1.5432e-01,  9.3842e-01, -1.3400e-01,  ...,  1.2146e+00,\n",
      "           -8.5403e-03,  4.6820e-01]],\n",
      "\n",
      "         [[-2.6538e-01, -1.0037e+00,  6.1239e-01,  ..., -2.2327e-01,\n",
      "           -4.7515e-01, -8.7986e-01],\n",
      "          [-1.5612e+00, -3.9014e-01,  1.5785e+00,  ...,  1.7583e+00,\n",
      "            6.7822e-01, -1.1305e+00],\n",
      "          [-1.6404e+00, -9.3256e-01,  4.1393e-01,  ..., -5.0254e-01,\n",
      "            1.1292e+00,  3.3288e-01],\n",
      "          ...,\n",
      "          [ 2.1218e+00, -7.3389e-01,  1.1790e-01,  ...,  2.9798e-01,\n",
      "            1.7703e+00, -1.8137e-01],\n",
      "          [ 1.9750e+00, -5.0957e-01, -1.4421e+00,  ..., -4.7349e-01,\n",
      "            1.1689e+00,  1.9703e-01],\n",
      "          [ 1.5271e+00,  1.4711e+00,  5.0050e-01,  ...,  2.1829e-01,\n",
      "            6.1907e-01,  7.2209e-01]],\n",
      "\n",
      "         [[-1.6404e+00, -9.5521e-01, -5.0635e-01,  ..., -8.1137e-01,\n",
      "           -2.7637e-01, -1.5823e+00],\n",
      "          [-6.7644e-01,  2.7667e-03, -5.6004e-03,  ..., -1.3168e+00,\n",
      "           -6.3264e-02,  2.0986e-01],\n",
      "          [ 2.6483e-01, -1.2837e-01,  6.2960e-01,  ..., -2.0794e+00,\n",
      "            1.3864e-01, -7.6697e-01],\n",
      "          ...,\n",
      "          [-3.6070e-02, -1.1290e-01,  5.9949e-01,  ...,  5.0827e-01,\n",
      "           -1.0947e-01,  7.9231e-02],\n",
      "          [ 6.4625e-01,  3.4362e-01,  1.7377e-02,  ...,  3.4301e-02,\n",
      "           -1.6852e-01, -7.5697e-02],\n",
      "          [ 1.8161e-02, -6.1821e-02,  1.5851e-01,  ...,  2.2777e-01,\n",
      "            1.4215e-01, -8.8293e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.2797e-01, -1.4207e-01,  4.8563e-01,  ...,  1.8112e-01,\n",
      "            2.1225e-01,  4.6830e-01],\n",
      "          [ 2.8408e-01, -1.5811e-01,  2.1091e-01,  ..., -6.9149e-01,\n",
      "            1.8006e-01, -6.0947e-01],\n",
      "          [ 2.9044e-01,  1.2160e+00, -2.4842e-01,  ..., -1.3830e+00,\n",
      "           -2.4299e-01, -7.1998e-01],\n",
      "          ...,\n",
      "          [-2.0034e-01, -9.3297e-01,  2.6796e+00,  ...,  1.6186e+00,\n",
      "           -1.1266e+00,  8.0244e-02],\n",
      "          [-9.5877e-01,  2.9606e+00, -1.6193e-01,  ..., -7.6963e-02,\n",
      "           -7.8457e-01, -1.5526e+00],\n",
      "          [-1.9054e-01, -8.3757e-01,  3.6374e-01,  ..., -1.0099e+00,\n",
      "           -7.1277e-01, -1.1729e+00]],\n",
      "\n",
      "         [[ 2.1228e+00,  2.4508e+00,  1.9520e+00,  ...,  2.2856e+00,\n",
      "            2.2385e+00,  1.5050e+00],\n",
      "          [ 1.4655e+00,  2.2153e-01,  1.0207e-01,  ...,  5.1267e-01,\n",
      "            5.3017e-01,  5.4026e-01],\n",
      "          [ 9.4139e-01,  7.1122e-02, -1.7699e-02,  ...,  8.6215e-01,\n",
      "            8.3369e-01,  6.2890e-01],\n",
      "          ...,\n",
      "          [-1.6735e+00, -1.7128e-01,  1.1947e-01,  ...,  5.3135e-01,\n",
      "            5.5602e-01,  3.0374e-01],\n",
      "          [-1.4350e+00, -2.5315e-01,  8.1238e-02,  ...,  8.9067e-01,\n",
      "            7.6248e-01,  5.5388e-01],\n",
      "          [-7.3385e-01,  6.2624e-02,  5.6562e-02,  ...,  4.6915e-01,\n",
      "            4.6552e-01,  4.0204e-01]],\n",
      "\n",
      "         [[-9.2069e-01, -3.9106e-01,  2.8709e-01,  ...,  3.1395e-01,\n",
      "           -8.8200e-02, -4.7320e-01],\n",
      "          [-9.8145e-01,  1.1785e+00,  7.2576e-01,  ...,  3.8302e-01,\n",
      "            8.1389e-01,  6.2526e-01],\n",
      "          [-3.1782e-01, -3.0210e-01, -6.7075e-01,  ...,  8.7503e-01,\n",
      "           -6.3549e-01,  8.0186e-01],\n",
      "          ...,\n",
      "          [-6.4207e-01,  2.0596e-01, -1.1658e+00,  ..., -2.1866e-01,\n",
      "            7.6230e-01, -4.6303e-01],\n",
      "          [ 3.0633e-01, -1.1001e+00, -1.2308e+00,  ...,  1.2594e+00,\n",
      "            2.0405e+00,  4.6767e-01],\n",
      "          [-6.7637e-02, -4.0416e-01,  1.4814e+00,  ...,  1.1509e+00,\n",
      "            1.2543e+00,  9.0605e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.9618e-01, -5.1135e-01, -6.0340e-01,  ..., -2.8278e-01,\n",
      "            5.1613e-02,  6.2050e-01],\n",
      "          [-4.7970e-01, -6.9610e-02,  4.5434e-01,  ...,  6.4977e-01,\n",
      "            6.7502e-01,  3.1512e-01],\n",
      "          [-7.9579e-01, -3.5683e-01, -9.3914e-02,  ...,  2.0540e-01,\n",
      "            7.4261e-01,  2.8208e-01],\n",
      "          ...,\n",
      "          [-1.7884e-01,  3.2134e-01,  1.8986e-01,  ..., -1.2157e+00,\n",
      "            4.2590e-02,  1.6013e+00],\n",
      "          [ 6.5833e-01, -2.3139e-01, -8.8094e-01,  ...,  5.8335e-01,\n",
      "           -2.5889e-01,  1.1646e+00],\n",
      "          [ 1.3016e+00,  7.2063e-02, -5.0013e-01,  ...,  3.0115e-01,\n",
      "            1.5974e-01,  8.3517e-01]],\n",
      "\n",
      "         [[ 1.6492e+00,  1.9649e+00,  4.2374e-01,  ...,  1.6026e+00,\n",
      "            1.6666e+00,  1.9583e+00],\n",
      "          [ 1.7955e+00,  2.0094e-01, -3.0641e-01,  ..., -4.1814e-01,\n",
      "            6.5088e-01,  1.3966e+00],\n",
      "          [ 1.6399e+00, -1.4416e-01,  8.9377e-01,  ...,  8.8380e-01,\n",
      "           -1.0371e-01,  1.2194e+00],\n",
      "          ...,\n",
      "          [ 1.3476e-01,  2.9649e-01, -9.7044e-03,  ...,  9.9019e-01,\n",
      "            1.7718e-01, -1.0765e+00],\n",
      "          [ 5.9290e-01, -4.9235e-01,  6.5995e-01,  ...,  9.7580e-01,\n",
      "            9.0094e-02, -9.5916e-01],\n",
      "          [-2.6386e-01, -7.6353e-01, -2.8487e-01,  ..., -1.1797e-01,\n",
      "           -4.8776e-01, -9.2462e-01]],\n",
      "\n",
      "         [[-6.0758e-01, -6.4119e-01, -4.0087e-01,  ..., -5.1747e-03,\n",
      "           -1.4716e-01, -6.3833e-01],\n",
      "          [-6.8347e-01, -7.5737e-02, -4.0862e-01,  ..., -1.3069e-01,\n",
      "            1.1730e-01,  4.3055e-01],\n",
      "          [-1.5036e+00, -5.5022e-01, -3.5096e-01,  ...,  1.5314e-01,\n",
      "           -4.8536e-01,  3.0148e-01],\n",
      "          ...,\n",
      "          [ 4.9523e-01,  1.1031e+00, -3.2106e-03,  ..., -1.9026e-02,\n",
      "            5.2331e-01,  7.6603e-02],\n",
      "          [-3.9119e-01,  5.1311e-01,  5.9013e-01,  ...,  2.3586e-01,\n",
      "           -1.7592e-01,  1.6435e-01],\n",
      "          [ 4.1603e-01,  6.5996e-01,  3.9739e-01,  ...,  5.6989e-01,\n",
      "            3.9486e-01,  3.9801e-01]]]], device='cuda:0'), 'res3': tensor([[[[-7.6391e-01, -2.0825e-01,  4.2491e-01,  ..., -8.7770e-01,\n",
      "           -9.2320e-01, -8.9224e-01],\n",
      "          [ 2.4914e-01,  2.2580e+00,  2.2696e+00,  ...,  9.8997e-01,\n",
      "           -9.7514e-01,  9.5867e-01],\n",
      "          [ 2.4354e-01,  1.6771e+00,  4.8365e-01,  ...,  3.1986e-01,\n",
      "           -1.1406e+00,  6.1079e-02],\n",
      "          ...,\n",
      "          [ 1.0602e-01,  2.6539e-01,  1.3295e+00,  ...,  6.7026e-01,\n",
      "            7.4555e-01,  1.2364e+00],\n",
      "          [-1.7779e-01, -3.7111e-01,  7.2696e-01,  ...,  1.0064e+00,\n",
      "            1.1907e-01,  3.6978e-01],\n",
      "          [-6.9044e-01, -9.8474e-01, -7.2579e-01,  ..., -8.2104e-01,\n",
      "            3.5224e-01,  5.7813e-01]],\n",
      "\n",
      "         [[-2.1940e-01, -7.3275e-01, -3.8253e-01,  ..., -1.5266e+00,\n",
      "            7.5992e-02,  4.4823e-01],\n",
      "          [-2.2227e-01, -8.9569e-01,  2.5324e-02,  ...,  7.0976e-02,\n",
      "           -3.6669e-01, -8.1282e-01],\n",
      "          [-4.0711e-01, -3.7867e-01, -9.4839e-01,  ...,  3.0202e-01,\n",
      "           -4.3111e-01,  6.8722e-01],\n",
      "          ...,\n",
      "          [-4.5725e-01, -2.2380e+00, -1.4112e+00,  ..., -1.1291e+00,\n",
      "           -1.2734e+00, -5.8762e-01],\n",
      "          [-5.6814e-02, -9.0051e-01, -2.3563e-01,  ..., -1.0730e+00,\n",
      "           -1.9054e+00, -1.7700e+00],\n",
      "          [-6.1547e-01, -6.5476e-01, -1.1921e+00,  ..., -1.3511e+00,\n",
      "           -1.2705e+00, -6.5497e-01]],\n",
      "\n",
      "         [[-5.9384e-02,  1.1544e-01, -4.9584e-01,  ...,  6.4447e-01,\n",
      "            4.9498e-01,  1.6271e-01],\n",
      "          [ 1.5369e-01,  2.1995e-01,  7.1918e-01,  ..., -1.6192e-01,\n",
      "            7.0990e-01,  2.0862e-01],\n",
      "          [ 3.5857e-01, -2.0884e-01,  7.8002e-01,  ..., -4.4781e-02,\n",
      "            8.8430e-01,  4.0129e-01],\n",
      "          ...,\n",
      "          [ 8.7771e-01,  4.5844e-01, -1.2467e-01,  ..., -2.6056e-01,\n",
      "            1.6796e-01,  1.0361e-01],\n",
      "          [ 9.9504e-01, -5.9530e-01,  3.3175e-01,  ..., -1.2746e+00,\n",
      "           -7.2986e-02, -6.6454e-01],\n",
      "          [ 5.9479e-02,  9.5365e-02, -3.0570e-01,  ...,  4.3514e-01,\n",
      "           -2.7336e-01,  3.8701e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.2948e-02,  2.9967e-01, -5.1353e-02,  ...,  4.1624e-01,\n",
      "            1.4340e-02,  3.9609e-01],\n",
      "          [ 3.9967e-01, -5.6409e-01,  9.9437e-02,  ..., -3.7371e-01,\n",
      "            9.2076e-01, -8.2159e-01],\n",
      "          [ 9.3877e-01,  1.2595e-01, -4.2000e-02,  ..., -5.4121e-01,\n",
      "           -6.1523e-01, -4.8105e-01],\n",
      "          ...,\n",
      "          [-1.5005e-01,  7.9191e-01, -5.8336e-02,  ..., -1.0251e+00,\n",
      "           -6.5502e-01, -8.7315e-01],\n",
      "          [ 2.6048e-01,  2.4099e-01, -2.4799e-01,  ..., -7.7279e-01,\n",
      "           -4.8595e-01, -1.3588e-01],\n",
      "          [ 5.1468e-02, -6.6347e-01, -5.7461e-01,  ..., -2.2385e-01,\n",
      "           -8.9003e-01, -2.7142e-01]],\n",
      "\n",
      "         [[-2.3412e-01, -6.4083e-01, -2.2074e-01,  ..., -1.3965e+00,\n",
      "            1.0656e+00, -1.0583e+00],\n",
      "          [-7.3703e-01,  7.2536e-01,  1.4747e+00,  ...,  1.2382e+00,\n",
      "            1.4714e+00,  1.3286e+00],\n",
      "          [-1.1716e+00,  9.6837e-01,  1.9323e+00,  ...,  1.5405e+00,\n",
      "            3.1447e+00,  1.8779e+00],\n",
      "          ...,\n",
      "          [-1.2095e+00,  3.2982e-01,  8.7233e-01,  ...,  1.1338e-02,\n",
      "            2.4590e+00,  2.2388e-01],\n",
      "          [-1.1863e+00,  1.8855e+00, -3.3150e-01,  ...,  1.3165e+00,\n",
      "            1.7938e+00, -1.1953e-01],\n",
      "          [-4.1227e-02,  7.6913e-01,  4.6345e-01,  ..., -2.7679e-01,\n",
      "           -1.0136e+00, -2.0815e-01]],\n",
      "\n",
      "         [[-1.3563e+00,  1.9536e+00, -1.1985e-01,  ...,  2.9367e+00,\n",
      "            2.4174e+00,  1.2268e+00],\n",
      "          [ 4.9901e-02,  7.0469e-01, -6.7940e-01,  ...,  2.0707e+00,\n",
      "            1.8921e+00, -5.9258e-01],\n",
      "          [ 9.0577e-01,  7.9548e-01,  1.2636e+00,  ...,  1.8695e+00,\n",
      "           -1.0088e+00, -2.8722e-01],\n",
      "          ...,\n",
      "          [ 1.8012e+00,  2.4226e+00,  6.6720e-01,  ..., -3.3292e-01,\n",
      "            6.8878e-01,  1.9093e+00],\n",
      "          [ 1.7077e+00,  2.0683e+00,  1.2171e+00,  ...,  1.5979e+00,\n",
      "            3.3341e-01,  1.7140e+00],\n",
      "          [ 1.4663e+00,  1.2465e+00, -6.7809e-01,  ...,  9.0592e-01,\n",
      "            5.0805e-01,  4.8437e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.5577e-01,  5.6054e-03,  3.1709e-01,  ...,  1.3614e+00,\n",
      "            1.7449e+00,  8.3436e-01],\n",
      "          [ 2.7494e-01,  1.0369e+00,  1.1855e+00,  ...,  1.9475e+00,\n",
      "            9.2149e-01,  3.5533e-01],\n",
      "          [ 1.0442e-01,  8.8878e-01,  2.9587e-01,  ..., -3.7056e-01,\n",
      "           -9.7282e-01, -1.3239e-01],\n",
      "          ...,\n",
      "          [ 7.6855e-01,  7.0964e-01,  4.3024e-02,  ...,  2.3980e-01,\n",
      "            4.7686e-01,  1.1339e+00],\n",
      "          [ 9.9474e-01, -5.3455e-01, -8.3206e-01,  ..., -4.3223e-01,\n",
      "            1.1353e+00,  2.5226e-01],\n",
      "          [ 5.1729e-02,  6.5215e-01,  5.8465e-02,  ..., -9.2247e-01,\n",
      "           -6.5748e-01, -3.7095e-01]],\n",
      "\n",
      "         [[-2.6094e-01, -7.3548e-02, -7.8400e-02,  ..., -1.2492e-01,\n",
      "           -5.9325e-01, -4.7770e-01],\n",
      "          [-3.6441e-01, -6.5796e-01, -4.8335e-01,  ...,  1.1965e+00,\n",
      "           -3.6012e-01, -2.3402e-01],\n",
      "          [ 2.2376e-02, -1.4204e-01, -7.6796e-01,  ...,  7.8858e-01,\n",
      "            2.2863e-01, -5.7497e-01],\n",
      "          ...,\n",
      "          [-9.0036e-01, -8.4623e-01, -5.1275e-01,  ...,  2.3352e-01,\n",
      "           -5.5727e-01, -9.6758e-02],\n",
      "          [-3.1135e-01,  5.4828e-01,  1.9696e-01,  ...,  2.5197e-01,\n",
      "            7.8269e-02, -1.5914e+00],\n",
      "          [ 2.3625e-01, -9.8403e-01, -2.3752e-01,  ..., -3.3893e-01,\n",
      "           -6.1016e-01, -4.8678e-01]],\n",
      "\n",
      "         [[-7.8443e-01, -9.8693e-01, -1.1948e+00,  ..., -2.5874e-01,\n",
      "           -1.0380e-01, -1.7092e-01],\n",
      "          [ 2.5049e-03, -1.5229e-02,  2.2554e-01,  ..., -4.7483e-01,\n",
      "            8.8990e-02,  5.8093e-01],\n",
      "          [ 1.6437e-01, -4.6629e-01, -1.9715e-01,  ...,  3.6507e-01,\n",
      "           -2.8632e-01, -8.6056e-01],\n",
      "          ...,\n",
      "          [-8.2150e-01,  5.0886e-01, -9.6777e-01,  ..., -1.6918e-01,\n",
      "            2.7073e-02, -3.4762e-01],\n",
      "          [-1.5563e-01, -2.0979e-01,  2.4451e-01,  ...,  5.8455e-02,\n",
      "            3.5178e-01,  3.8627e-02],\n",
      "          [ 2.1615e-01,  1.6065e-01, -6.3788e-01,  ...,  2.4437e-01,\n",
      "           -1.0989e-01,  3.3917e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.9122e-01, -1.1633e+00, -8.6167e-01,  ..., -9.5805e-01,\n",
      "           -1.1706e-01, -8.1796e-01],\n",
      "          [-3.5727e-01, -1.1228e+00,  5.6191e-01,  ..., -1.4861e+00,\n",
      "            4.3147e-01,  2.1487e-01],\n",
      "          [-4.2849e-01,  7.0179e-01, -2.6282e-01,  ..., -1.8245e+00,\n",
      "           -5.0080e-01,  1.4320e-01],\n",
      "          ...,\n",
      "          [-3.1459e-01,  9.4586e-02, -5.4096e-04,  ..., -1.1010e-01,\n",
      "           -1.1956e+00, -1.3528e+00],\n",
      "          [-1.5068e-01, -6.1372e-01,  2.7000e-01,  ...,  9.2134e-01,\n",
      "            7.6886e-01, -7.3182e-02],\n",
      "          [-1.1938e-01,  3.1749e-02,  2.8377e-01,  ..., -6.3204e-02,\n",
      "            5.7168e-01, -3.7669e-01]],\n",
      "\n",
      "         [[ 3.0854e-01, -7.7055e-02, -5.5526e-02,  ..., -4.4301e-01,\n",
      "           -6.0008e-01,  3.1518e-01],\n",
      "          [-1.2043e+00,  9.4612e-01, -1.1768e+00,  ...,  8.9793e-01,\n",
      "            1.8010e+00,  4.8245e-02],\n",
      "          [-2.0947e+00,  2.6990e-01,  8.2680e-01,  ..., -7.2998e-01,\n",
      "            4.6547e-01,  1.7196e-01],\n",
      "          ...,\n",
      "          [-1.9447e+00,  1.9527e+00,  4.0717e-02,  ...,  1.2767e+00,\n",
      "            7.6639e-01, -3.4608e-01],\n",
      "          [-1.2251e+00,  4.6136e-01,  5.6215e-01,  ...,  9.5507e-01,\n",
      "            9.8585e-01, -4.5108e-01],\n",
      "          [-1.2758e+00, -6.6333e-01, -9.3142e-01,  ..., -1.4539e+00,\n",
      "           -1.5695e+00, -1.1424e+00]],\n",
      "\n",
      "         [[ 1.3799e+00,  8.8420e-01,  1.8078e+00,  ...,  3.9079e-01,\n",
      "           -9.4918e-02, -1.1458e-01],\n",
      "          [-6.9283e-01,  2.9110e-01,  1.4059e+00,  ...,  1.0659e+00,\n",
      "           -9.5291e-01, -7.5530e-01],\n",
      "          [ 1.9707e-01,  1.5429e+00,  1.9811e+00,  ...,  2.7665e+00,\n",
      "            1.4929e+00,  7.8688e-01],\n",
      "          ...,\n",
      "          [ 1.1471e+00,  1.7434e+00, -5.8379e-01,  ...,  7.0128e-01,\n",
      "           -1.3287e+00,  1.4970e+00],\n",
      "          [ 1.3631e+00,  3.1324e+00,  6.2570e-01,  ...,  2.0514e+00,\n",
      "            3.1542e-01,  2.7893e+00],\n",
      "          [ 1.5601e+00,  6.8216e-01,  1.1978e+00,  ...,  2.2913e+00,\n",
      "            1.4592e-01,  1.3640e-01]]]], device='cuda:0'), 'res4': tensor([[[[-5.6877e-01, -2.9237e-01, -3.1042e-02,  ...,  1.3532e+00,\n",
      "            6.8263e-01, -1.0221e-02],\n",
      "          [-5.1132e-01, -3.6990e-01, -1.0481e-02,  ..., -3.7711e-01,\n",
      "           -2.4662e-01,  9.3648e-01],\n",
      "          [-1.5845e-01,  2.4432e-01,  2.6045e-01,  ..., -1.6697e-01,\n",
      "           -6.6507e-01,  1.3001e+00],\n",
      "          ...,\n",
      "          [-7.2525e-01, -6.7917e-01, -5.9054e-01,  ..., -6.7674e-01,\n",
      "           -3.2064e-01,  2.9852e-01],\n",
      "          [-2.9636e-01, -3.3434e-01, -3.7062e-01,  ..., -4.3171e-02,\n",
      "            2.5433e-01,  8.8973e-01],\n",
      "          [-7.6863e-03,  1.4806e-01,  1.7652e-01,  ...,  1.9062e-01,\n",
      "            1.1303e-01,  5.5313e-01]],\n",
      "\n",
      "         [[ 2.8872e-01,  9.9511e-01, -1.0495e-01,  ..., -2.5045e-01,\n",
      "           -8.1385e-01, -1.5031e+00],\n",
      "          [-3.6750e-02,  8.9506e-01,  8.7844e-01,  ..., -1.6754e-01,\n",
      "           -3.0268e-01, -8.9289e-01],\n",
      "          [-6.0780e-01,  5.4096e-01,  6.4189e-01,  ...,  2.1056e-01,\n",
      "            3.7413e-01, -4.5124e-01],\n",
      "          ...,\n",
      "          [-8.2780e-01, -7.2477e-01, -5.3546e-01,  ..., -2.1773e-01,\n",
      "            2.1600e-01,  3.6675e-01],\n",
      "          [-7.5947e-01, -6.1450e-01, -4.2857e-01,  ..., -3.0176e-01,\n",
      "            1.1289e-01,  3.9812e-01],\n",
      "          [-8.9445e-01, -6.7522e-01, -3.5898e-01,  ..., -1.7219e-01,\n",
      "            4.0559e-02, -6.0164e-02]],\n",
      "\n",
      "         [[-2.8340e-01, -1.8032e-01, -4.3420e-01,  ..., -2.4397e-01,\n",
      "           -4.4355e-01, -9.5683e-01],\n",
      "          [ 5.6507e-01,  4.6495e-01,  3.2437e-01,  ..., -2.9354e-01,\n",
      "           -4.2543e-01, -7.2833e-01],\n",
      "          [ 9.9249e-01,  9.6374e-01,  3.1125e-01,  ...,  5.5747e-01,\n",
      "           -3.7378e-02, -2.1968e-01],\n",
      "          ...,\n",
      "          [-5.4727e-01, -4.2078e-01, -4.6259e-01,  ...,  7.0173e-01,\n",
      "            4.0281e-01,  3.1790e-01],\n",
      "          [-1.0754e+00, -1.0821e+00, -1.0720e+00,  ...,  5.6310e-01,\n",
      "            3.0397e-01,  1.8340e-01],\n",
      "          [-9.4437e-01, -1.0799e+00, -7.0918e-01,  ...,  4.0633e-01,\n",
      "            3.1151e-01,  1.3766e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6058e-01, -5.7910e-01,  4.3766e-01,  ..., -5.3387e-01,\n",
      "           -1.1345e+00, -5.7215e-01],\n",
      "          [-7.3583e-02, -4.1003e-01,  2.0110e-01,  ..., -9.8005e-01,\n",
      "           -1.1722e+00, -1.3012e+00],\n",
      "          [ 4.4529e-01, -1.7483e-03,  1.2652e-01,  ..., -5.1514e-01,\n",
      "           -5.0847e-01, -6.6831e-01],\n",
      "          ...,\n",
      "          [-7.3491e-02,  1.9726e-02,  7.5391e-01,  ..., -4.5969e-01,\n",
      "           -7.7640e-01, -4.1364e-01],\n",
      "          [-3.8088e-01, -9.5147e-02,  8.9630e-01,  ..., -2.9540e-01,\n",
      "           -6.6403e-01, -1.1915e-01],\n",
      "          [-2.1069e-01,  9.0427e-02,  9.2075e-01,  ...,  2.1680e-01,\n",
      "            3.9111e-02,  3.9438e-01]],\n",
      "\n",
      "         [[ 6.5398e-01,  1.0939e+00,  1.3279e+00,  ...,  1.7187e+00,\n",
      "            1.9347e+00,  1.8475e+00],\n",
      "          [ 5.3158e-01,  7.8521e-01,  1.5622e+00,  ...,  1.7746e+00,\n",
      "            2.0182e+00,  1.8205e+00],\n",
      "          [ 5.6658e-01,  7.5872e-01,  8.4701e-01,  ...,  2.2306e+00,\n",
      "            2.7433e+00,  2.2712e+00],\n",
      "          ...,\n",
      "          [ 1.9190e+00,  1.5309e+00,  1.0265e+00,  ...,  1.6100e+00,\n",
      "            1.8063e+00,  1.8603e+00],\n",
      "          [ 1.6608e+00,  9.5707e-01,  1.0018e+00,  ...,  2.1377e+00,\n",
      "            1.9378e+00,  1.8895e+00],\n",
      "          [ 1.5921e+00,  1.1867e+00,  9.8159e-01,  ...,  2.0594e+00,\n",
      "            1.5703e+00,  1.7893e+00]],\n",
      "\n",
      "         [[ 3.2689e-01,  2.8295e-01,  1.4809e-02,  ...,  1.1408e+00,\n",
      "            8.6259e-01,  1.3035e+00],\n",
      "          [ 2.3151e-03, -1.8872e-02, -1.6799e-01,  ...,  1.9790e+00,\n",
      "            1.2975e+00,  8.9017e-01],\n",
      "          [-7.6478e-01, -7.5831e-02, -2.1645e-02,  ...,  2.4221e+00,\n",
      "            1.5662e+00,  4.0976e-01],\n",
      "          ...,\n",
      "          [-1.6148e-01, -3.4124e-01, -6.4178e-01,  ...,  8.0952e-01,\n",
      "            1.0832e+00,  7.4590e-01],\n",
      "          [ 3.6227e-01,  3.5397e-01, -9.4623e-02,  ..., -9.7685e-02,\n",
      "            4.7464e-01,  4.5755e-01],\n",
      "          [ 3.2887e-01,  4.0829e-01,  1.6887e-01,  ..., -6.1476e-01,\n",
      "            6.5734e-02, -1.5514e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1952e-02,  7.0390e-02,  4.6044e-01,  ...,  1.5850e+00,\n",
      "            1.2757e+00,  1.7251e+00],\n",
      "          [ 4.4082e-01,  1.3842e-01,  6.3274e-02,  ...,  7.4388e-01,\n",
      "            4.5750e-01,  1.7220e+00],\n",
      "          [-4.3471e-01, -3.2327e-01,  1.9479e-01,  ...,  2.6752e-01,\n",
      "           -3.6938e-01,  7.4113e-01],\n",
      "          ...,\n",
      "          [ 2.6793e-02, -1.0168e-01,  1.7489e-01,  ...,  2.2705e-01,\n",
      "            3.2777e-01,  1.3521e+00],\n",
      "          [ 5.8271e-01,  9.5079e-01,  1.3310e+00,  ...,  1.1497e-01,\n",
      "            2.6915e-01,  1.4498e+00],\n",
      "          [ 1.0150e+00,  2.3018e+00,  2.0092e+00,  ...,  3.9738e-01,\n",
      "            3.3979e-01,  1.0973e+00]],\n",
      "\n",
      "         [[ 3.3602e-01,  5.8367e-01,  1.0361e+00,  ..., -6.6373e-01,\n",
      "           -6.1780e-01, -5.1679e-01],\n",
      "          [ 3.1421e-01,  2.8828e-01,  5.6529e-01,  ..., -3.0404e-01,\n",
      "            4.2751e-02,  1.1318e-01],\n",
      "          [ 6.5943e-02,  1.6793e-01,  2.6827e-01,  ...,  7.1256e-01,\n",
      "            1.1516e+00,  9.3761e-01],\n",
      "          ...,\n",
      "          [-1.5261e-01, -7.9244e-01, -8.0959e-01,  ..., -5.5748e-01,\n",
      "           -7.2965e-01, -7.2304e-01],\n",
      "          [-2.9195e-01, -6.3227e-01, -5.4949e-01,  ..., -1.4829e+00,\n",
      "           -9.7110e-01, -7.1468e-01],\n",
      "          [-7.2854e-01, -8.6784e-01, -5.7557e-01,  ..., -1.5254e+00,\n",
      "           -9.5507e-01, -9.2083e-01]],\n",
      "\n",
      "         [[-2.8151e-01,  9.8436e-02,  5.2985e-01,  ..., -1.3244e+00,\n",
      "           -1.3127e+00, -1.1389e+00],\n",
      "          [-9.9322e-02,  1.6350e-01,  3.7115e-01,  ...,  1.2787e-01,\n",
      "           -6.4765e-02, -1.0597e-01],\n",
      "          [ 2.4300e-02,  1.6891e-01, -5.7378e-02,  ...,  9.8557e-01,\n",
      "            6.9442e-01,  7.8833e-01],\n",
      "          ...,\n",
      "          [-2.0512e-01, -1.9890e-01,  1.6201e-01,  ..., -6.6224e-01,\n",
      "           -8.1547e-01, -9.0454e-01],\n",
      "          [-6.4306e-01, -4.8545e-01, -6.8426e-02,  ..., -6.4380e-01,\n",
      "           -1.2892e+00, -9.0304e-01],\n",
      "          [-8.1126e-01, -5.0282e-01,  2.4881e-02,  ..., -1.1527e+00,\n",
      "           -1.3670e+00, -1.2286e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.1194e-01,  3.7439e-01, -2.8024e-01,  ...,  7.9377e-01,\n",
      "           -4.1346e-01, -7.3470e-01],\n",
      "          [ 1.0025e+00,  5.5461e-01,  4.8693e-01,  ...,  2.2243e+00,\n",
      "            1.0958e+00,  7.7565e-01],\n",
      "          [ 2.1245e-01,  2.5318e-01,  3.2088e-01,  ...,  2.1399e+00,\n",
      "            2.0842e+00,  2.5004e+00],\n",
      "          ...,\n",
      "          [-6.6825e-01, -1.1535e+00, -8.0210e-01,  ...,  8.4265e-01,\n",
      "            7.3101e-01,  6.6972e-01],\n",
      "          [-1.1825e+00, -1.2610e+00, -9.6658e-01,  ...,  5.0638e-01,\n",
      "            3.9960e-01,  6.6044e-01],\n",
      "          [-1.2407e+00, -1.1772e+00, -8.8400e-01,  ...,  5.8970e-01,\n",
      "            8.2101e-02,  1.5605e-01]],\n",
      "\n",
      "         [[ 1.8475e-01,  6.8266e-01,  1.3533e+00,  ...,  1.5562e+00,\n",
      "            1.1359e+00,  1.1377e+00],\n",
      "          [ 7.2754e-02,  3.2779e-01,  8.3296e-01,  ...,  8.1480e-01,\n",
      "            3.2005e-01,  1.6915e-01],\n",
      "          [ 8.4270e-01,  5.1996e-01,  5.3125e-01,  ...,  6.8074e-01,\n",
      "            8.0866e-02, -5.1122e-02],\n",
      "          ...,\n",
      "          [ 2.5999e-01, -9.5859e-02, -6.7460e-01,  ...,  5.6909e-01,\n",
      "            1.7303e+00,  1.8954e+00],\n",
      "          [ 3.1170e-01,  4.4940e-01,  2.5927e-01,  ...,  1.2064e+00,\n",
      "            2.2147e+00,  1.8600e+00],\n",
      "          [ 1.1133e+00,  1.4389e+00,  1.1824e+00,  ...,  2.5549e+00,\n",
      "            2.6650e+00,  1.8543e+00]],\n",
      "\n",
      "         [[ 4.2142e-01, -3.7836e-01, -5.7204e-01,  ...,  9.6340e-01,\n",
      "            1.0659e+00,  8.9332e-01],\n",
      "          [-1.6570e-01, -5.3660e-01, -6.5004e-01,  ...,  5.8029e-01,\n",
      "            8.8252e-01,  7.9532e-01],\n",
      "          [-1.8258e-01, -2.8808e-01, -2.2145e-01,  ...,  1.7268e-01,\n",
      "            6.7717e-02, -9.1686e-02],\n",
      "          ...,\n",
      "          [ 6.4777e-01,  6.4188e-01,  5.6908e-02,  ..., -2.0712e-01,\n",
      "           -2.3352e-01, -3.5032e-01],\n",
      "          [-6.2340e-03, -3.3328e-01, -4.8949e-01,  ..., -4.7075e-01,\n",
      "           -3.3879e-01, -6.1722e-02],\n",
      "          [ 1.7036e-02, -4.9568e-01, -6.6157e-01,  ..., -2.4007e-01,\n",
      "           -3.8121e-01, -2.2413e-01]]]], device='cuda:0'), 'res5': tensor([[[[-7.6167e-02,  5.7158e-02,  1.4334e-01,  ..., -1.1368e-02,\n",
      "           -3.6295e-02, -8.2823e-02],\n",
      "          [ 4.9520e-01,  1.3208e+00,  6.2029e-01,  ..., -3.4691e-01,\n",
      "           -5.0149e-01, -1.7670e-01],\n",
      "          [ 9.3824e-01,  1.7774e+00,  9.1764e-01,  ..., -1.4975e+00,\n",
      "           -9.5278e-01, -3.8956e-01],\n",
      "          ...,\n",
      "          [ 2.4670e-01,  7.8425e-01,  3.4722e-01,  ...,  4.8116e-01,\n",
      "            5.8841e-01,  1.7930e-01],\n",
      "          [ 1.3012e-01,  2.6707e-01,  1.2445e-01,  ...,  3.3080e-01,\n",
      "            4.4996e-01,  1.2473e-01],\n",
      "          [-7.1263e-02,  5.8071e-02,  3.3399e-02,  ...,  1.0396e-01,\n",
      "            1.0368e-01,  1.0871e-02]],\n",
      "\n",
      "         [[-6.3626e-01, -5.6856e-01, -1.8232e-01,  ...,  1.2171e-01,\n",
      "           -3.3210e-01, -3.4785e-01],\n",
      "          [-7.1129e-01, -7.4487e-01,  4.7101e-01,  ...,  1.2097e+00,\n",
      "            8.5577e-01,  2.6707e-01],\n",
      "          [-2.9680e-01,  2.3764e-02,  1.8905e-01,  ...,  3.2311e-01,\n",
      "           -2.9704e-01,  3.5858e-01],\n",
      "          ...,\n",
      "          [ 3.1932e-01,  7.5186e-01,  3.7263e-01,  ...,  2.6239e-01,\n",
      "            4.7492e-01,  9.9165e-02],\n",
      "          [ 3.7342e-01,  8.6067e-01,  6.7915e-01,  ...,  3.3183e-01,\n",
      "            5.1270e-01,  8.0855e-02],\n",
      "          [-1.2179e-01,  6.0112e-02,  1.7378e-02,  ..., -5.7406e-02,\n",
      "            1.4000e-02, -8.7381e-02]],\n",
      "\n",
      "         [[ 4.8846e-01,  5.2309e-01,  3.4390e-02,  ...,  3.6380e-01,\n",
      "            7.6647e-01,  6.4703e-01],\n",
      "          [ 1.2745e+00,  1.7684e+00,  2.2320e-01,  ...,  2.2390e-01,\n",
      "           -5.8748e-02,  8.6255e-01],\n",
      "          [ 1.6132e+00,  1.4003e+00,  1.6535e-01,  ...,  2.7399e-01,\n",
      "           -4.8656e-02,  8.7790e-01],\n",
      "          ...,\n",
      "          [ 4.0419e-01, -3.5040e-01, -5.5647e-01,  ..., -4.7284e-01,\n",
      "           -8.6234e-02,  6.1936e-01],\n",
      "          [ 4.6030e-01, -4.3870e-01, -8.2153e-01,  ..., -5.7253e-01,\n",
      "           -6.3370e-01,  4.4294e-01],\n",
      "          [ 2.0518e-01,  1.7739e-01, -1.8014e-03,  ..., -8.2540e-02,\n",
      "            3.0635e-02,  3.2560e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.2556e-02, -1.0582e-01, -2.5111e-01,  ..., -1.1662e-01,\n",
      "           -2.9887e-01, -2.9027e-01],\n",
      "          [ 6.7573e-02,  1.0944e-01, -6.5082e-02,  ...,  9.9931e-02,\n",
      "           -3.6668e-01, -3.9511e-01],\n",
      "          [ 1.3569e-01,  1.5706e-01, -2.5628e-02,  ...,  6.2881e-01,\n",
      "           -1.7153e-01, -2.4485e-01],\n",
      "          ...,\n",
      "          [-2.4336e-01, -6.2549e-01, -1.6183e-01,  ..., -1.7432e-01,\n",
      "           -3.0423e-01, -1.9067e-01],\n",
      "          [-1.7294e-01, -3.1189e-01, -2.5244e-02,  ..., -6.2720e-02,\n",
      "           -1.7020e-01, -1.4405e-01],\n",
      "          [-2.4353e-01, -1.7461e-01, -1.0697e-01,  ...,  7.1561e-02,\n",
      "           -4.0461e-03, -1.0851e-01]],\n",
      "\n",
      "         [[ 3.1324e-02,  7.3780e-02,  1.0569e-01,  ..., -2.1228e-04,\n",
      "            7.0264e-02,  6.1003e-02],\n",
      "          [ 4.7667e-02,  1.5342e-01,  5.1406e-01,  ..., -3.7420e-01,\n",
      "           -6.6764e-02,  1.5065e-03],\n",
      "          [-2.0324e-01, -1.1282e-01,  2.3148e-01,  ..., -9.6832e-01,\n",
      "           -2.8799e-01,  6.3232e-03],\n",
      "          ...,\n",
      "          [ 1.7047e-01,  6.5303e-01,  4.1735e-01,  ..., -3.8798e-01,\n",
      "           -5.2775e-01, -1.5637e-01],\n",
      "          [ 2.2871e-01,  7.6665e-01,  6.0773e-01,  ...,  2.4475e-02,\n",
      "           -1.7538e-02, -4.1521e-02],\n",
      "          [ 2.0136e-01,  2.5390e-01,  2.5397e-01,  ...,  1.4518e-01,\n",
      "            9.9892e-02,  1.3949e-02]],\n",
      "\n",
      "         [[ 8.6862e-02,  8.8925e-02,  1.6167e-01,  ...,  2.6272e-01,\n",
      "            3.7511e-01,  2.0105e-01],\n",
      "          [ 5.4988e-01,  7.0721e-01,  3.3096e-01,  ...,  1.0020e+00,\n",
      "            1.2905e+00,  4.2717e-01],\n",
      "          [ 7.5511e-01,  1.0800e+00,  4.0968e-01,  ...,  1.9837e+00,\n",
      "            1.1249e+00,  4.6759e-01],\n",
      "          ...,\n",
      "          [ 3.0543e-02,  1.3071e-01,  4.6394e-02,  ...,  1.2444e-01,\n",
      "            8.8272e-02,  1.1793e-02],\n",
      "          [-3.2956e-02, -8.8422e-02, -9.0155e-03,  ...,  2.2550e-03,\n",
      "           -7.3294e-02, -3.4372e-02],\n",
      "          [-6.1053e-02, -3.6768e-02, -2.0052e-05,  ...,  2.5488e-02,\n",
      "           -3.0658e-03, -1.5428e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.7659e-01, -5.4165e-01, -6.3655e-01,  ...,  1.9965e-01,\n",
      "           -1.8839e-01, -2.5414e-01],\n",
      "          [-3.0288e-01, -9.3133e-01, -7.8928e-01,  ...,  1.8494e-01,\n",
      "           -4.1369e-01, -3.6371e-01],\n",
      "          [-2.7363e-01, -1.0456e+00, -5.0188e-01,  ..., -4.4384e-01,\n",
      "           -1.2571e+00, -8.6901e-01],\n",
      "          ...,\n",
      "          [ 2.6740e-01,  4.2434e-01, -1.5546e-01,  ...,  3.4690e-01,\n",
      "            2.8584e-01,  1.4261e-01],\n",
      "          [ 2.1250e-01,  2.8012e-01, -3.6816e-01,  ...,  2.4125e-01,\n",
      "            1.8054e-01,  9.9233e-02],\n",
      "          [-1.5781e-01, -1.0283e-02, -3.3644e-01,  ...,  7.3493e-02,\n",
      "            3.1540e-02, -3.9514e-02]],\n",
      "\n",
      "         [[-4.3029e-01, -1.3261e-01, -6.0771e-01,  ...,  2.4962e-01,\n",
      "            1.6649e-01, -2.4035e-01],\n",
      "          [ 3.6967e-02,  3.2253e-01, -4.9788e-01,  ...,  6.8796e-01,\n",
      "           -3.6027e-01, -7.3979e-01],\n",
      "          [-4.1118e-02, -3.9273e-01, -1.8830e-01,  ...,  5.0968e-01,\n",
      "            3.1640e-01, -3.7381e-01],\n",
      "          ...,\n",
      "          [ 4.3911e-02,  8.1627e-02,  5.0252e-01,  ...,  1.5476e-01,\n",
      "            2.7884e-01,  1.3249e-01],\n",
      "          [-1.2269e-01, -4.1502e-01,  6.0572e-02,  ...,  5.0289e-01,\n",
      "            5.7953e-01,  7.1136e-02],\n",
      "          [-1.2586e-01, -1.6540e+00, -4.8710e-01,  ...,  1.2137e-01,\n",
      "           -7.0857e-02, -2.9277e-01]],\n",
      "\n",
      "         [[ 5.2294e-01,  1.0916e+00,  1.3910e+00,  ..., -2.8376e-01,\n",
      "           -2.0362e-01,  1.3350e-01],\n",
      "          [-1.3704e-01,  1.2303e-01,  4.2587e-01,  ...,  5.7796e-01,\n",
      "            2.0186e-02,  6.1509e-01],\n",
      "          [-3.2950e-01, -8.6045e-01, -1.7742e-01,  ...,  9.4923e-01,\n",
      "           -6.8898e-01, -1.1658e-01],\n",
      "          ...,\n",
      "          [ 2.1491e-01,  1.6832e-01, -1.9752e-01,  ..., -9.2321e-01,\n",
      "           -1.6461e+00,  3.2177e-01],\n",
      "          [ 9.4980e-01,  1.1350e+00,  5.7566e-01,  ..., -1.0432e+00,\n",
      "           -3.0690e+00,  2.4723e-01],\n",
      "          [ 3.7332e-01,  9.3919e-01,  6.3697e-01,  ..., -1.8343e-01,\n",
      "           -9.6023e-02,  1.9909e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3210e-01, -2.6514e-01,  9.6312e-03,  ..., -2.5875e-01,\n",
      "           -3.2410e-01, -1.3931e-01],\n",
      "          [-2.4003e-01, -3.6163e-01,  2.4504e-01,  ...,  2.3623e-01,\n",
      "           -3.4483e-02, -6.3614e-03],\n",
      "          [-2.5506e-01, -2.6262e-01,  3.1769e-01,  ..., -7.0804e-02,\n",
      "           -3.9192e-01, -3.5801e-01],\n",
      "          ...,\n",
      "          [ 7.1763e-02,  4.2993e-01,  4.0227e-01,  ...,  2.4086e-01,\n",
      "            5.3880e-01,  2.2925e-02],\n",
      "          [-7.6042e-01,  7.2546e-02,  5.1076e-01,  ...,  3.9196e-01,\n",
      "            1.1112e+00,  1.4027e-03],\n",
      "          [-7.4587e-01, -5.7200e-01, -1.0419e-01,  ...,  7.9101e-02,\n",
      "            2.2104e-02, -1.0612e-01]],\n",
      "\n",
      "         [[-9.5192e-02, -2.5602e-01, -3.1934e-01,  ..., -7.7393e-02,\n",
      "           -8.0495e-02, -2.9253e-02],\n",
      "          [-1.3427e-01, -3.6347e-01, -5.1497e-01,  ..., -2.4827e-01,\n",
      "           -3.7392e-01, -4.7392e-01],\n",
      "          [-4.0367e-02,  7.7201e-02, -1.1736e-02,  ..., -1.2939e-01,\n",
      "           -5.8678e-01, -6.1428e-01],\n",
      "          ...,\n",
      "          [ 1.1573e-01,  8.5570e-01,  1.2044e+00,  ...,  1.1632e-01,\n",
      "            3.0897e-02,  4.9523e-02],\n",
      "          [ 5.3411e-01,  1.3862e+00,  1.3832e+00,  ...,  7.0021e-02,\n",
      "            6.5655e-02,  2.5044e-02],\n",
      "          [ 4.2816e-01,  1.1591e+00,  4.3315e-01,  ...,  1.6296e-02,\n",
      "           -1.4131e-02, -1.0131e-02]],\n",
      "\n",
      "         [[ 3.4683e-02,  4.0959e-01,  6.2332e-01,  ..., -6.0892e-01,\n",
      "           -7.7782e-01, -2.9863e-01],\n",
      "          [ 9.2599e-02,  7.2402e-01,  9.7874e-01,  ..., -8.9950e-01,\n",
      "           -1.1139e+00, -4.5761e-01],\n",
      "          [ 2.0053e-01,  1.1898e+00,  8.3228e-01,  ...,  2.9715e-01,\n",
      "           -2.7966e-01, -2.1045e-01],\n",
      "          ...,\n",
      "          [-2.4618e-01, -5.7159e-01,  1.5283e-01,  ..., -1.4595e-01,\n",
      "           -4.2932e-01, -3.1846e-02],\n",
      "          [-6.2756e-01, -5.6405e-01, -9.0882e-02,  ..., -1.8923e-01,\n",
      "           -7.1598e-01, -1.1095e-01],\n",
      "          [-2.6617e-01, -5.2080e-01, -9.3549e-02,  ..., -1.9839e-02,\n",
      "           -7.9016e-02, -9.1974e-02]]]], device='cuda:0')}\n",
      "2\n",
      "tensor([[[[ 1.5002,  1.5002,  1.4831,  ...,  2.1167,  2.1338,  2.1338],\n",
      "          [ 1.5002,  1.5002,  1.4831,  ...,  2.1167,  2.1338,  2.1338],\n",
      "          [ 1.5173,  1.5173,  1.5002,  ...,  2.1167,  2.1338,  2.1338],\n",
      "          ...,\n",
      "          [ 1.8256,  1.8256,  1.8256,  ...,  2.9558,  2.8702,  2.8531],\n",
      "          [ 1.8085,  1.8085,  1.8085,  ...,  3.1442,  3.0586,  3.0414],\n",
      "          [ 1.8085,  1.8085,  1.8085,  ...,  3.2298,  3.1613,  3.1442]],\n",
      "\n",
      "         [[ 0.6106,  0.6106,  0.5931,  ...,  1.2409,  1.2584,  1.2584],\n",
      "          [ 0.6106,  0.6106,  0.5931,  ...,  1.2409,  1.2584,  1.2584],\n",
      "          [ 0.6282,  0.6282,  0.6106,  ...,  1.2409,  1.2759,  1.2759],\n",
      "          ...,\n",
      "          [ 1.8011,  1.8011,  1.8011,  ...,  3.0091,  2.9216,  2.9041],\n",
      "          [ 1.7836,  1.7836,  1.7836,  ...,  3.2192,  3.1317,  3.1141],\n",
      "          [ 1.7836,  1.7836,  1.7836,  ...,  3.3067,  3.2367,  3.2192]],\n",
      "\n",
      "         [[-0.0588, -0.0588, -0.0588,  ...,  0.4466,  0.4815,  0.4815],\n",
      "          [-0.0588, -0.0588, -0.0588,  ...,  0.4466,  0.4815,  0.4815],\n",
      "          [-0.0414, -0.0414, -0.0414,  ...,  0.4641,  0.4989,  0.4989],\n",
      "          ...,\n",
      "          [ 1.4052,  1.4052,  1.4227,  ...,  3.0784,  2.9913,  2.9739],\n",
      "          [ 1.3878,  1.3878,  1.4052,  ...,  3.2702,  3.1830,  3.1830],\n",
      "          [ 1.3878,  1.3878,  1.4052,  ...,  3.3573,  3.2876,  3.2876]]],\n",
      "\n",
      "\n",
      "        [[[-0.8459, -0.8459, -0.8459,  ..., -0.8116, -0.8288, -0.8288],\n",
      "          [-0.8973, -0.8973, -0.8801,  ..., -0.7774, -0.7945, -0.8116],\n",
      "          [-0.9486, -0.9486, -0.8973,  ..., -0.7260, -0.7603, -0.7774],\n",
      "          ...,\n",
      "          [-0.0753, -0.0410,  0.0104,  ...,  0.1131,  0.0960,  0.0960],\n",
      "          [-0.1438, -0.1609, -0.1438,  ...,  0.0446,  0.0446,  0.0446],\n",
      "          [-0.2123, -0.2294, -0.2465,  ..., -0.0068, -0.0068, -0.0068]],\n",
      "\n",
      "         [[-0.3522, -0.3522, -0.3873,  ..., -0.2122, -0.2297, -0.2297],\n",
      "          [-0.3347, -0.3347, -0.3347,  ..., -0.2297, -0.2472, -0.2647],\n",
      "          [-0.2997, -0.2997, -0.2822,  ..., -0.2472, -0.2822, -0.2997],\n",
      "          ...,\n",
      "          [ 0.6457,  0.6457,  0.6807,  ...,  0.6106,  0.5756,  0.5756],\n",
      "          [ 0.5931,  0.5931,  0.6106,  ...,  0.6457,  0.6282,  0.6282],\n",
      "          [ 0.5581,  0.5581,  0.5581,  ...,  0.6982,  0.6807,  0.6807]],\n",
      "\n",
      "         [[ 0.5163,  0.5163,  0.5338,  ...,  0.6035,  0.5861,  0.5686],\n",
      "          [ 0.5512,  0.5512,  0.5163,  ...,  0.6383,  0.6035,  0.5861],\n",
      "          [ 0.5861,  0.5686,  0.4989,  ...,  0.6906,  0.6383,  0.6209],\n",
      "          ...,\n",
      "          [ 0.0458,  0.0632,  0.1503,  ...,  0.1678,  0.1678,  0.1678],\n",
      "          [ 0.0283,  0.0283,  0.0458,  ...,  0.1852,  0.2026,  0.2026],\n",
      "          [ 0.0283,  0.0109, -0.0240,  ...,  0.2026,  0.2375,  0.2375]]]],\n",
      "       device='cuda:0')\n",
      "{'res2': tensor([[[[-9.3348e-01, -1.5280e+00,  5.5847e-01,  ..., -9.6693e-02,\n",
      "           -3.1894e-01, -1.7994e+00],\n",
      "          [-1.1220e+00,  5.6588e-02, -2.2900e-01,  ..., -6.9438e-01,\n",
      "            1.2580e+00, -4.2718e-01],\n",
      "          [-8.8295e-01,  3.2639e-01,  1.4897e-01,  ...,  4.4589e-01,\n",
      "            1.9516e+00, -1.8656e+00],\n",
      "          ...,\n",
      "          [-2.1169e-01, -6.1793e-01, -1.4559e+00,  ...,  1.5842e+00,\n",
      "           -1.7794e+00, -1.2173e+00],\n",
      "          [-1.2881e+00,  6.2460e-01, -5.6037e-01,  ..., -2.4403e+00,\n",
      "           -9.9367e-02, -7.8580e-01],\n",
      "          [-1.6187e+00, -4.0162e-01, -1.4190e+00,  ..., -1.0913e+00,\n",
      "           -1.3153e+00, -4.8281e-01]],\n",
      "\n",
      "         [[-1.3936e+00, -1.9116e+00, -1.5253e+00,  ..., -1.0929e+00,\n",
      "           -1.0227e+00, -7.5828e-01],\n",
      "          [-1.6059e+00, -1.4603e+00, -9.4805e-01,  ..., -1.6206e+00,\n",
      "           -1.1215e+00,  4.7701e-02],\n",
      "          [-1.1495e+00, -3.8978e-01, -5.0785e-01,  ..., -1.3522e+00,\n",
      "           -8.6051e-01,  2.4601e-01],\n",
      "          ...,\n",
      "          [ 1.5840e+00,  3.3256e-01,  7.5246e-01,  ..., -8.6719e-02,\n",
      "            1.7614e-01,  1.0764e-01],\n",
      "          [ 1.0454e+00, -6.8311e-01, -3.0141e-01,  ...,  5.1194e-01,\n",
      "            2.3876e-01,  7.9828e-02],\n",
      "          [ 3.3770e-01, -5.9779e-01, -4.6784e-01,  ...,  1.9294e-01,\n",
      "            2.6000e-01,  4.7430e-02]],\n",
      "\n",
      "         [[-2.8014e-01,  8.9494e-01,  9.5631e-02,  ...,  1.8069e-01,\n",
      "            7.7218e-01,  1.2857e+00],\n",
      "          [ 5.3382e-01,  1.4887e+00,  1.9684e-01,  ...,  4.4088e-01,\n",
      "            8.6948e-01,  5.6035e-01],\n",
      "          [ 7.4484e-01, -5.1763e-01,  1.9872e+00,  ...,  1.7157e+00,\n",
      "            7.6347e-01,  1.0088e+00],\n",
      "          ...,\n",
      "          [-1.4913e+00, -1.3458e+00,  3.4298e-01,  ...,  6.5524e-01,\n",
      "           -8.8475e-01,  9.1920e-01],\n",
      "          [-1.1008e+00,  2.1021e+00,  2.1196e+00,  ...,  9.8862e-01,\n",
      "            1.5360e+00,  1.4467e-01],\n",
      "          [-5.6311e-01,  1.8867e+00,  1.9056e+00,  ...,  1.4711e+00,\n",
      "            5.2922e-01,  1.3012e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2530e-01,  4.3429e-01,  5.9039e-01,  ..., -2.3513e-01,\n",
      "           -1.3987e-01, -3.0337e-01],\n",
      "          [ 7.9882e-01,  1.0233e+00,  7.8858e-01,  ..., -5.0667e-01,\n",
      "            3.2406e-03,  3.1385e-01],\n",
      "          [ 5.9517e-02,  4.1845e-01,  5.5855e-01,  ..., -7.1327e-02,\n",
      "           -7.5811e-01, -3.7723e-01],\n",
      "          ...,\n",
      "          [-3.9200e-01, -4.8848e-01,  2.9882e-01,  ..., -6.0800e-02,\n",
      "           -4.1044e-01,  1.3266e-01],\n",
      "          [-4.2887e-02, -8.4887e-01, -1.4670e+00,  ...,  4.7723e-01,\n",
      "           -1.0743e-01,  1.0827e-02],\n",
      "          [ 1.8482e-01,  5.0777e-01,  2.5828e-01,  ...,  4.1807e-01,\n",
      "            8.2763e-01,  5.8482e-01]],\n",
      "\n",
      "         [[-9.8525e-01, -1.5653e+00, -9.0382e-01,  ..., -1.0624e+00,\n",
      "           -1.2496e+00, -1.4935e+00],\n",
      "          [-1.7022e+00,  5.1601e-01, -3.6632e-02,  ...,  6.1352e-01,\n",
      "            6.7887e-01,  9.6225e-02],\n",
      "          [-1.8863e+00,  1.3089e+00,  1.0391e+00,  ...,  1.2982e+00,\n",
      "            1.5642e+00, -1.3773e-01],\n",
      "          ...,\n",
      "          [ 1.2489e-01,  1.1381e+00,  6.1830e-02,  ...,  1.3593e+00,\n",
      "            4.7291e-01,  1.1097e+00],\n",
      "          [-1.0257e-01,  2.4089e+00,  1.4829e-02,  ...,  9.1401e-01,\n",
      "            1.8664e+00,  1.0811e+00],\n",
      "          [-4.8073e-01,  1.2729e+00,  1.0741e-01,  ...,  1.3485e+00,\n",
      "            1.3140e+00,  1.4806e+00]],\n",
      "\n",
      "         [[-1.2894e+00, -6.1766e-01, -1.5218e-01,  ..., -3.3516e-01,\n",
      "           -8.0691e-01, -1.0429e+00],\n",
      "          [ 1.8211e-01,  4.7266e-01, -7.4978e-02,  ...,  7.2340e-01,\n",
      "            4.1055e-01,  5.5797e-01],\n",
      "          [ 2.7379e-01,  1.9694e-01,  4.8107e-01,  ...,  7.4970e-01,\n",
      "            5.7573e-01,  3.9911e-01],\n",
      "          ...,\n",
      "          [ 4.4718e-01,  9.7501e-02, -1.1338e-01,  ..., -2.1990e+00,\n",
      "           -1.8033e-01, -5.1847e-01],\n",
      "          [ 8.7660e-02,  6.9636e-01, -1.1475e-01,  ..., -6.1433e-01,\n",
      "           -2.1811e+00, -5.5926e-01],\n",
      "          [-5.3491e-01,  3.4870e-01,  1.1006e-01,  ..., -2.5684e-01,\n",
      "           -3.8991e-01, -5.9957e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.2168e-01, -1.6020e-01, -8.6853e-01,  ...,  5.2166e-01,\n",
      "           -2.4047e-01,  1.3010e+00],\n",
      "          [-2.6931e-01, -2.9185e-01, -1.9680e-01,  ...,  1.3684e+00,\n",
      "           -1.1315e+00,  3.4541e-01],\n",
      "          [ 7.4175e-01,  3.7408e-01,  2.3009e+00,  ..., -8.2851e-01,\n",
      "           -1.4731e+00, -6.7927e-01],\n",
      "          ...,\n",
      "          [-1.0914e+00,  2.9295e+00, -1.0539e+00,  ...,  1.5903e-01,\n",
      "           -1.4690e+00, -7.3431e-02],\n",
      "          [ 1.0454e+00, -1.5324e+00,  6.0993e-01,  ...,  7.8958e-01,\n",
      "            7.9474e-01, -9.1417e-01],\n",
      "          [-1.5817e+00,  1.6455e+00, -3.4400e-01,  ..., -1.6112e-01,\n",
      "           -1.7601e+00, -2.6447e-02]],\n",
      "\n",
      "         [[-9.0970e-02, -3.5595e-01, -5.2466e-01,  ..., -1.7420e-01,\n",
      "           -9.4609e-02,  6.0910e-02],\n",
      "          [-1.8416e-01, -1.0088e-01, -2.8717e-01,  ...,  1.6811e-01,\n",
      "            1.2530e-01,  4.1827e-02],\n",
      "          [-1.9580e-02,  3.0657e-01, -4.1635e-02,  ...,  2.9979e-01,\n",
      "            3.6363e-02, -2.9544e-01],\n",
      "          ...,\n",
      "          [ 3.7014e+00,  5.1726e-01,  4.7917e-01,  ...,  7.4589e-01,\n",
      "            1.2181e+00,  1.3528e+00],\n",
      "          [ 3.1209e+00,  9.8123e-02,  8.1881e-01,  ...,  7.4463e-01,\n",
      "            1.4374e+00,  1.2571e+00],\n",
      "          [ 2.0504e+00, -7.6883e-02, -1.1138e-01,  ...,  1.7551e-01,\n",
      "            4.7776e-01,  1.1354e+00]],\n",
      "\n",
      "         [[ 2.7817e-01,  7.8107e-01,  8.0399e-01,  ...,  1.5519e+00,\n",
      "            3.5446e-01, -2.7037e-01],\n",
      "          [-9.2928e-01,  1.6083e+00,  3.2307e-01,  ...,  3.1995e-01,\n",
      "           -4.8317e-01, -4.1250e-01],\n",
      "          [-4.1217e-01, -4.3630e-01, -1.4598e-01,  ...,  5.2356e-01,\n",
      "            3.1052e-01,  7.6064e-01],\n",
      "          ...,\n",
      "          [-9.2986e-01, -1.8955e+00,  7.8566e-01,  ..., -2.1242e-01,\n",
      "            8.9538e-01, -2.9899e-01],\n",
      "          [-1.0120e+00,  1.4897e-01,  2.9142e-01,  ...,  1.4260e+00,\n",
      "           -5.3189e-02,  5.5620e-01],\n",
      "          [-4.8558e-01,  8.0762e-01,  9.2035e-01,  ...,  3.5599e-01,\n",
      "            1.9878e+00,  1.1765e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0695e-01,  3.4561e-01,  3.0118e-01,  ...,  1.2293e+00,\n",
      "            3.2383e-01,  7.1273e-01],\n",
      "          [ 6.3672e-01,  2.6115e-01,  7.9231e-02,  ...,  2.8503e-01,\n",
      "            1.1864e-01,  6.9422e-01],\n",
      "          [ 1.6937e+00,  8.6364e-01,  2.4655e-01,  ..., -2.0592e-02,\n",
      "           -7.6557e-01,  2.9272e-02],\n",
      "          ...,\n",
      "          [-1.2014e+00, -7.8194e-02,  4.8143e-01,  ..., -4.9864e-01,\n",
      "           -5.1442e-01,  1.3187e+00],\n",
      "          [-2.0416e+00, -3.0643e-01,  7.2380e-01,  ...,  1.5673e-01,\n",
      "           -5.8508e-01, -5.4069e-01],\n",
      "          [-1.7824e+00,  8.2361e-01, -6.1410e-01,  ...,  9.6044e-02,\n",
      "            1.4549e+00,  1.5162e-01]],\n",
      "\n",
      "         [[ 9.5395e-01,  2.3402e+00,  1.3234e+00,  ...,  1.2387e+00,\n",
      "            1.9596e+00,  2.6109e+00],\n",
      "          [ 2.4086e+00, -8.6824e-01, -4.8897e-01,  ...,  1.2263e+00,\n",
      "            4.3673e-01,  5.8402e-01],\n",
      "          [ 2.9168e+00, -1.4150e+00,  2.4835e-01,  ..., -5.3566e-01,\n",
      "           -6.0852e-02, -3.9360e-01],\n",
      "          ...,\n",
      "          [ 1.0049e+00, -5.2911e-01, -1.2374e+00,  ..., -6.4829e-01,\n",
      "            5.6340e-01, -2.4226e-01],\n",
      "          [ 1.2931e+00,  6.9015e-02,  7.0288e-01,  ..., -1.1704e-01,\n",
      "            7.9010e-01,  5.8534e-01],\n",
      "          [ 1.3076e+00,  3.5489e-01,  9.7253e-01,  ...,  1.1649e+00,\n",
      "            5.8767e-01,  8.5324e-01]],\n",
      "\n",
      "         [[-3.7683e-01, -3.5171e-01, -7.5867e-01,  ..., -2.3810e-01,\n",
      "           -6.3593e-01, -5.2092e-01],\n",
      "          [-6.1077e-01, -5.2417e-01, -2.9589e-01,  ..., -2.6075e-02,\n",
      "           -3.5695e-01, -4.1718e-01],\n",
      "          [-4.3802e-01, -2.2516e-01, -1.0523e+00,  ...,  3.1519e-01,\n",
      "           -5.2210e-01, -4.6085e-01],\n",
      "          ...,\n",
      "          [ 5.3050e-01,  4.4073e-01,  4.6916e-01,  ...,  3.2771e-02,\n",
      "            7.4634e-02,  9.0451e-02],\n",
      "          [ 5.5388e-02,  6.5094e-01,  3.9089e-01,  ...,  3.0604e-01,\n",
      "           -1.5809e-01,  2.7691e-01],\n",
      "          [ 2.8947e-02, -8.0646e-01, -2.7742e-01,  ...,  1.4852e-01,\n",
      "            1.5687e-01,  2.5317e-01]]]], device='cuda:0'), 'res3': tensor([[[[-0.3590,  1.0819,  0.5418,  ...,  1.0978,  1.1466,  0.1394],\n",
      "          [ 0.4613,  1.0498,  0.2602,  ...,  1.7308,  1.7386,  0.3603],\n",
      "          [ 0.1432, -0.9507,  0.0864,  ...,  1.0779, -0.4996, -0.3366],\n",
      "          ...,\n",
      "          [-0.1605,  1.1929,  0.3582,  ...,  1.3570,  2.2708, -0.1170],\n",
      "          [-0.2619,  0.3931,  0.9545,  ...,  1.8070,  1.8090,  0.9733],\n",
      "          [-0.6451,  0.4569, -0.5013,  ...,  1.3505,  0.8873,  0.4268]],\n",
      "\n",
      "         [[-1.1352, -0.9025, -0.5689,  ..., -0.5883, -1.1080, -0.9742],\n",
      "          [-0.2473,  0.8693,  0.4513,  ..., -0.2332,  1.2821, -1.3921],\n",
      "          [-0.5885,  0.6610, -1.0412,  ..., -0.6741, -0.2716, -1.5906],\n",
      "          ...,\n",
      "          [-0.2818, -0.0814, -0.2970,  ..., -0.5713,  0.6676, -0.3254],\n",
      "          [ 0.5838,  1.1828,  1.4165,  ...,  0.3155, -0.9749,  0.6886],\n",
      "          [-0.1594, -0.4474, -0.0749,  ..., -0.1950,  0.1588, -0.2563]],\n",
      "\n",
      "         [[ 0.0966, -0.8957, -0.0573,  ..., -0.2173, -0.0906,  0.3095],\n",
      "          [ 0.2625,  0.9171,  0.4380,  ...,  1.4598,  0.1143,  0.6872],\n",
      "          [ 0.0885,  0.4019, -0.2105,  ...,  0.0442,  0.1666,  0.5915],\n",
      "          ...,\n",
      "          [-0.7769, -0.1425, -0.3349,  ...,  0.2900,  0.4367, -0.0623],\n",
      "          [-0.6330, -0.4125, -0.8976,  ...,  0.2326,  0.0179,  0.7020],\n",
      "          [-0.1818,  0.0730,  0.3007,  ..., -0.0318,  0.2016,  0.2717]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5254,  0.7475,  1.2491,  ...,  0.4867,  0.5539, -0.4634],\n",
      "          [ 0.9982, -0.3985,  0.4296,  ...,  0.3152,  0.8029,  0.8701],\n",
      "          [ 0.6874,  0.4049,  0.3536,  ..., -0.5442,  0.2333,  1.0124],\n",
      "          ...,\n",
      "          [-0.5292, -0.2174, -0.6973,  ..., -0.1028, -0.3562,  0.4907],\n",
      "          [-0.9040, -1.1867, -1.4233,  ..., -0.3647,  0.3983,  1.0763],\n",
      "          [-0.0455,  0.1337,  0.1297,  ...,  0.1394, -0.0342, -0.4104]],\n",
      "\n",
      "         [[-1.2653, -0.6886, -1.2834,  ..., -1.4094, -1.3762, -1.2196],\n",
      "          [-1.6216,  1.5291,  1.0595,  ...,  0.5505,  1.8916, -0.0072],\n",
      "          [-2.4338,  1.9504,  0.4215,  ..., -0.9272,  0.2352,  0.5153],\n",
      "          ...,\n",
      "          [-1.7261,  0.9564,  0.7622,  ...,  0.7816, -0.4950,  1.0112],\n",
      "          [-0.4400,  2.7330,  1.7828,  ..., -0.1615,  0.8610, -0.2543],\n",
      "          [-1.2790,  0.2890,  0.3120,  ..., -0.0455,  0.2862,  0.0975]],\n",
      "\n",
      "         [[-1.7743,  0.6620,  0.1341,  ...,  0.3269,  0.3907,  0.2821],\n",
      "          [ 0.0492,  0.1868,  0.2873,  ..., -0.4349, -0.9206, -0.0214],\n",
      "          [ 0.2866,  0.4130, -1.2537,  ...,  1.6046,  0.3762,  0.2487],\n",
      "          ...,\n",
      "          [ 0.5378, -0.3327,  0.0837,  ...,  1.4103,  0.7657, -1.2658],\n",
      "          [-0.1379, -0.6449, -1.1356,  ..., -0.4550,  1.3976, -1.6254],\n",
      "          [-0.2119, -1.1026,  0.0817,  ..., -0.1663, -1.1528,  0.1198]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3243, -1.0617, -0.0888,  ..., -0.0838, -1.2212,  1.3363],\n",
      "          [ 0.2240, -0.3234,  0.2837,  ..., -1.3174, -0.0598, -1.3035],\n",
      "          [-1.8016, -1.8596, -0.1055,  ..., -0.3610, -1.6798, -1.2832],\n",
      "          ...,\n",
      "          [ 0.5868,  0.8289, -0.3926,  ..., -1.1339, -0.7675, -1.0559],\n",
      "          [-0.3966, -0.4272, -1.5928,  ..., -0.7746, -0.5333, -0.8281],\n",
      "          [ 1.1749, -0.3879, -0.5666,  ..., -0.5599,  0.0239, -0.5930]],\n",
      "\n",
      "         [[-0.0489, -0.8820, -0.4378,  ..., -0.7657, -0.6129, -0.3290],\n",
      "          [ 0.0172, -0.2951, -0.3743,  ..., -0.4438,  0.7276, -0.6803],\n",
      "          [-0.0960,  0.5160,  0.3137,  ...,  0.8488,  1.4812, -0.0341],\n",
      "          ...,\n",
      "          [-0.0779, -0.5816, -0.2521,  ...,  0.0096, -1.5331, -0.4413],\n",
      "          [ 0.0788, -0.7940,  0.2344,  ..., -0.2792,  0.0518, -0.3744],\n",
      "          [ 0.2746,  0.2623, -0.1173,  ...,  0.2956, -0.2693,  0.0967]],\n",
      "\n",
      "         [[ 0.2041,  0.2499, -0.0550,  ..., -0.0166,  0.0521, -0.5192],\n",
      "          [ 0.4314,  0.5087,  0.2053,  ...,  1.2595, -0.4095,  0.2210],\n",
      "          [ 0.0208, -0.2059, -0.1233,  ...,  0.4184, -0.6204, -0.6303],\n",
      "          ...,\n",
      "          [ 0.0927, -0.1247,  0.3615,  ...,  0.0094,  0.3692, -0.0995],\n",
      "          [ 0.8222,  0.2979,  0.2312,  ..., -0.2256,  0.9255,  0.0579],\n",
      "          [-0.0843, -0.3884,  0.2343,  ..., -0.4576,  0.4455,  0.1055]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1127,  0.6493,  0.3393,  ..., -0.1236,  0.2412, -0.5031],\n",
      "          [ 0.1200,  0.2998,  0.7099,  ...,  1.0117,  0.8541,  0.5266],\n",
      "          [ 1.2101,  0.2318, -0.2594,  ...,  0.8149,  0.3053,  0.9941],\n",
      "          ...,\n",
      "          [ 0.3652,  0.4294,  0.4041,  ...,  0.2324, -0.2340,  0.3884],\n",
      "          [-0.1976, -0.2635, -0.5137,  ...,  0.8003, -0.3026,  0.4795],\n",
      "          [-0.7596, -1.7904,  0.0205,  ...,  0.1940, -1.1631, -0.3454]],\n",
      "\n",
      "         [[-0.0452,  0.8837, -0.4800,  ..., -0.9673,  0.2062, -0.1975],\n",
      "          [ 0.8725,  2.5446,  0.1143,  ...,  1.5656,  1.4217,  0.7375],\n",
      "          [-1.1127,  0.2820, -0.2330,  ..., -0.0897,  0.0535,  0.1238],\n",
      "          ...,\n",
      "          [-2.1553,  1.2660,  1.7549,  ...,  0.4759, -0.1424, -0.4909],\n",
      "          [-0.9415,  1.4029,  1.3133,  ..., -0.8493, -0.0592,  0.3545],\n",
      "          [-2.8007,  0.3244, -1.2638,  ..., -0.4476,  0.0674, -0.4186]],\n",
      "\n",
      "         [[ 0.1822,  0.3656, -1.0181,  ..., -0.1631,  0.5017, -0.9720],\n",
      "          [ 0.7329,  1.5196,  1.0224,  ..., -0.3463,  1.3225,  0.8154],\n",
      "          [-1.1971, -1.2643, -1.3980,  ..., -0.1199,  0.1847, -1.3026],\n",
      "          ...,\n",
      "          [ 0.4003,  0.8847,  0.4937,  ...,  0.1369,  0.7541, -1.2298],\n",
      "          [-0.4081, -0.1738,  1.5853,  ..., -0.4657,  0.3214, -1.0347],\n",
      "          [ 0.6078,  0.1616,  0.3298,  ..., -0.7436,  0.0651, -0.8471]]]],\n",
      "       device='cuda:0'), 'res4': tensor([[[[-2.2320e-01, -2.2656e-01, -1.3470e-01,  ..., -3.2385e-01,\n",
      "           -6.5522e-01, -6.0298e-01],\n",
      "          [-9.4044e-02, -2.8162e-01, -3.7517e-01,  ..., -1.0721e-01,\n",
      "            6.4214e-02,  3.6214e-01],\n",
      "          [-2.7077e-01, -7.4377e-01, -5.5414e-01,  ..., -9.2068e-01,\n",
      "           -4.8866e-01,  3.2803e-02],\n",
      "          ...,\n",
      "          [-3.4489e-01, -7.0250e-01, -4.1186e-01,  ...,  1.1459e+00,\n",
      "            5.6252e-01,  2.8005e+00],\n",
      "          [-4.5588e-01, -4.4605e-01,  2.4666e-01,  ...,  1.2946e+00,\n",
      "            1.1549e+00,  3.1836e+00],\n",
      "          [-2.8670e-01, -1.8511e-01,  3.2109e-01,  ...,  2.4367e+00,\n",
      "            2.0457e+00,  2.8530e+00]],\n",
      "\n",
      "         [[-9.0981e-01, -1.0333e+00, -1.3549e+00,  ..., -8.2286e-01,\n",
      "           -8.9531e-01, -1.3415e+00],\n",
      "          [-7.1872e-01, -6.8032e-01, -1.0183e+00,  ..., -3.6016e-01,\n",
      "           -6.5447e-01, -6.5341e-01],\n",
      "          [-6.5988e-01, -7.2604e-01, -8.0366e-01,  ..., -1.5880e-01,\n",
      "           -6.1978e-01, -1.1262e+00],\n",
      "          ...,\n",
      "          [ 1.9366e-01,  5.3398e-01,  1.3004e+00,  ...,  9.1533e-02,\n",
      "            5.1910e-01,  6.8974e-01],\n",
      "          [-1.6648e-01,  2.6300e-01,  7.6109e-01,  ..., -1.1362e-01,\n",
      "           -2.3747e-02,  6.7271e-01],\n",
      "          [-4.9610e-01,  1.6334e-01, -2.0375e-02,  ..., -3.9091e-01,\n",
      "           -4.5013e-01,  4.7156e-02]],\n",
      "\n",
      "         [[-5.4780e-01, -6.0225e-01, -6.7281e-01,  ..., -1.2731e+00,\n",
      "           -1.4129e+00, -1.5418e+00],\n",
      "          [-2.7578e-01, -6.1804e-01, -9.1479e-01,  ..., -1.0378e+00,\n",
      "           -1.4572e+00, -1.5750e+00],\n",
      "          [ 1.7030e-01, -1.4438e-01, -5.0335e-01,  ..., -2.0813e-01,\n",
      "           -1.0387e+00, -1.4245e+00],\n",
      "          ...,\n",
      "          [-1.0445e+00, -1.3833e+00, -1.4980e+00,  ...,  1.5930e+00,\n",
      "            5.4642e-01, -5.0017e-02],\n",
      "          [-2.2611e+00, -2.4301e+00, -2.2913e+00,  ...,  1.4582e+00,\n",
      "            1.1297e-01, -4.9298e-01],\n",
      "          [-2.4773e+00, -2.0725e+00, -2.2636e+00,  ...,  1.5608e+00,\n",
      "            5.0655e-01, -7.8497e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.5910e-01, -3.2167e-01, -1.4991e-01,  ..., -9.1598e-01,\n",
      "           -3.4391e-01,  8.4440e-01],\n",
      "          [-2.1007e-01, -6.1658e-01,  1.7250e-01,  ..., -1.4187e+00,\n",
      "           -5.8537e-01,  6.3326e-01],\n",
      "          [-2.7949e-01, -4.9976e-01,  2.0114e-01,  ..., -1.3250e+00,\n",
      "           -1.4264e-01,  1.5051e+00],\n",
      "          ...,\n",
      "          [-6.3872e-01, -7.3324e-01, -7.1640e-01,  ...,  5.2034e-01,\n",
      "           -1.6433e-01, -5.7639e-01],\n",
      "          [ 1.2909e-01, -7.1350e-01, -8.8195e-01,  ...,  5.6032e-01,\n",
      "            1.5879e-01, -9.6158e-02],\n",
      "          [ 7.1490e-01, -1.6871e-01, -2.7815e-01,  ...,  6.5020e-01,\n",
      "            7.6459e-01,  6.4789e-01]],\n",
      "\n",
      "         [[ 2.2793e+00,  2.9009e+00,  3.0802e+00,  ...,  2.4146e+00,\n",
      "            2.0728e+00,  1.6894e+00],\n",
      "          [ 2.6202e+00,  2.3442e+00,  2.9820e+00,  ...,  2.1644e+00,\n",
      "            1.7627e+00,  1.4654e+00],\n",
      "          [ 2.8183e+00,  3.4868e+00,  2.6952e+00,  ...,  2.3638e+00,\n",
      "            1.9537e+00,  1.3010e+00],\n",
      "          ...,\n",
      "          [ 1.0303e+00,  4.6345e-01,  3.7655e-01,  ..., -2.3623e-01,\n",
      "           -1.8413e-01,  4.1779e-01],\n",
      "          [ 1.2171e+00,  8.6773e-01,  8.9022e-01,  ..., -7.7432e-02,\n",
      "           -2.4295e-01,  3.8680e-01],\n",
      "          [ 1.1716e+00,  1.2622e+00,  1.4149e+00,  ...,  3.8523e-01,\n",
      "            3.5521e-01,  4.8286e-01]],\n",
      "\n",
      "         [[ 3.6463e-01, -8.5936e-02, -2.7437e-01,  ..., -3.2451e-01,\n",
      "            1.7114e-01,  5.5295e-01],\n",
      "          [-2.3732e-01, -5.4752e-01, -8.2408e-01,  ..., -7.9393e-01,\n",
      "           -2.6793e-01,  1.2007e-01],\n",
      "          [-2.7795e-01, -8.5575e-01, -5.8940e-01,  ..., -1.0184e+00,\n",
      "           -2.3109e-01,  3.9746e-01],\n",
      "          ...,\n",
      "          [ 7.0673e-01,  9.3897e-01,  8.2003e-01,  ..., -2.1578e-01,\n",
      "           -2.7836e-02,  2.6889e-02],\n",
      "          [ 1.2590e+00,  1.2065e+00,  1.0231e+00,  ..., -6.3134e-01,\n",
      "           -4.9642e-01, -4.6108e-01],\n",
      "          [ 1.3584e+00,  1.2436e+00,  1.3224e+00,  ..., -7.1742e-01,\n",
      "           -9.9493e-01, -6.7424e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4127e-01,  1.7257e-01,  1.0233e-01,  ...,  5.1526e-01,\n",
      "            2.1875e-01,  2.3137e-02],\n",
      "          [-1.3359e-01, -3.7451e-01, -5.0314e-01,  ..., -2.5701e-01,\n",
      "            1.3622e-01,  3.8126e-01],\n",
      "          [-8.4533e-03, -5.5701e-01, -5.9715e-01,  ..., -1.9660e-01,\n",
      "            2.6758e-01,  1.8663e-01],\n",
      "          ...,\n",
      "          [-1.4099e-02, -3.7162e-01, -9.0720e-01,  ..., -6.0229e-02,\n",
      "           -1.0186e-02,  1.2609e+00],\n",
      "          [ 1.3990e-01, -5.8005e-01, -4.0128e-01,  ..., -1.4640e-01,\n",
      "            1.7279e-01,  1.4315e+00],\n",
      "          [ 1.4944e-01,  5.5515e-02,  2.4134e-01,  ...,  5.6430e-01,\n",
      "            5.7038e-01,  8.7250e-01]],\n",
      "\n",
      "         [[ 1.8843e-01,  1.5218e-01, -3.2847e-03,  ...,  1.6248e-01,\n",
      "            1.8935e-01,  1.2094e-01],\n",
      "          [-3.0863e-02,  1.6241e-01,  8.5135e-01,  ..., -6.7436e-01,\n",
      "           -6.0130e-01, -3.5999e-01],\n",
      "          [-1.2170e-01,  1.8422e-01,  9.7000e-01,  ..., -7.1101e-01,\n",
      "           -7.0188e-01, -3.8067e-01],\n",
      "          ...,\n",
      "          [-8.0324e-01, -1.0866e+00, -1.3908e+00,  ..., -7.3524e-01,\n",
      "           -6.9085e-01, -3.4665e-01],\n",
      "          [-6.3818e-01, -1.3687e+00, -1.4996e+00,  ..., -7.4797e-01,\n",
      "           -3.6320e-01, -1.8449e-02],\n",
      "          [-6.5591e-01, -9.7366e-01, -9.2553e-01,  ..., -3.6591e-01,\n",
      "           -8.0058e-02, -2.0859e-01]],\n",
      "\n",
      "         [[-1.3083e+00, -1.2192e+00, -8.4378e-01,  ...,  2.0156e-02,\n",
      "           -5.5380e-01, -5.3996e-01],\n",
      "          [-9.4945e-01, -8.5648e-01, -9.3853e-01,  ...,  6.7964e-01,\n",
      "            2.8201e-01,  2.2474e-01],\n",
      "          [-2.8950e-01, -7.7165e-01, -5.5171e-01,  ...,  1.4182e+00,\n",
      "            1.0721e+00,  9.1003e-01],\n",
      "          ...,\n",
      "          [-8.1541e-01, -6.0288e-01, -2.6742e-01,  ...,  3.8149e-01,\n",
      "           -9.2380e-02, -5.0442e-01],\n",
      "          [-1.3405e+00, -1.7397e+00, -1.5704e+00,  ..., -1.0914e+00,\n",
      "           -1.1437e+00, -9.9686e-01],\n",
      "          [-1.0936e+00, -1.8982e+00, -1.9428e+00,  ..., -1.6013e+00,\n",
      "           -1.6928e+00, -1.3952e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4682e+00, -1.3840e+00, -1.0296e+00,  ..., -4.0898e-01,\n",
      "           -5.9583e-01, -8.8286e-01],\n",
      "          [-1.7595e+00, -5.4196e-01,  2.5523e-01,  ..., -8.0352e-01,\n",
      "           -9.0564e-01, -1.0863e+00],\n",
      "          [-1.4366e+00,  1.9278e-01,  1.0537e+00,  ..., -7.7142e-01,\n",
      "           -8.6313e-01, -7.2706e-01],\n",
      "          ...,\n",
      "          [-4.4322e-01, -7.9601e-01, -5.9472e-01,  ...,  2.5515e-01,\n",
      "           -4.8379e-03, -6.1340e-02],\n",
      "          [-3.8593e-01, -7.3028e-01, -7.2940e-01,  ..., -3.7632e-01,\n",
      "           -2.1738e-01, -6.2678e-02],\n",
      "          [ 1.2185e-01, -3.1369e-01, -2.7594e-01,  ..., -2.9539e-01,\n",
      "           -2.0427e-01,  1.2477e-02]],\n",
      "\n",
      "         [[ 1.1796e+00,  1.2195e+00,  1.9621e+00,  ...,  1.6218e+00,\n",
      "            1.5000e+00,  1.4124e+00],\n",
      "          [-1.1765e+00, -2.4905e+00, -2.7334e+00,  ...,  4.1648e-01,\n",
      "            3.8515e-01,  5.9004e-01],\n",
      "          [-1.3218e+00, -3.4372e+00, -3.5653e+00,  ..., -1.9093e-01,\n",
      "            9.1838e-03,  4.3529e-01],\n",
      "          ...,\n",
      "          [-1.8351e-01,  2.5694e-01,  1.4559e+00,  ..., -6.3632e-02,\n",
      "            4.9624e-01,  8.0288e-01],\n",
      "          [ 3.5639e-02,  6.6768e-01,  1.8047e+00,  ...,  9.2230e-01,\n",
      "            6.1452e-01,  4.7486e-01],\n",
      "          [ 1.2146e-01,  1.1197e+00,  1.7827e+00,  ...,  1.4593e+00,\n",
      "            1.0114e+00,  9.2673e-01]],\n",
      "\n",
      "         [[ 1.5466e-01, -1.3156e-02, -5.8111e-01,  ...,  5.7511e-01,\n",
      "            2.7343e-01, -2.5108e-02],\n",
      "          [-9.8450e-02, -3.8917e-01, -8.9016e-01,  ...,  2.4837e-01,\n",
      "           -1.3936e-01, -3.1865e-01],\n",
      "          [-5.4373e-01, -5.8730e-01, -8.2962e-01,  ...,  2.2779e-01,\n",
      "           -2.0691e-01, -6.8640e-01],\n",
      "          ...,\n",
      "          [-3.0467e-01, -4.9776e-01, -1.1068e+00,  ...,  2.2827e-02,\n",
      "           -3.5981e-01, -7.4791e-01],\n",
      "          [-2.9641e-01, -7.6603e-01, -1.4214e+00,  ..., -6.9082e-01,\n",
      "           -6.6333e-01, -7.7099e-01],\n",
      "          [-4.4739e-01, -1.0191e+00, -1.3960e+00,  ..., -9.6442e-01,\n",
      "           -9.9404e-01, -9.5376e-01]]]], device='cuda:0'), 'res5': tensor([[[[-1.3020e-01, -1.1547e-01,  2.0639e-01,  ..., -2.9890e-02,\n",
      "           -6.3417e-02, -1.3097e-01],\n",
      "          [-6.2891e-02, -4.8166e-01,  9.3399e-01,  ..., -1.9115e-01,\n",
      "           -3.7801e-01, -1.6985e-01],\n",
      "          [-4.3481e-02, -2.6824e-01,  5.8397e-01,  ..., -2.0761e-01,\n",
      "           -2.2095e-01, -1.0276e-01],\n",
      "          ...,\n",
      "          [-2.7942e-01,  8.2331e-02,  2.1189e-01,  ...,  1.0938e+00,\n",
      "            1.5579e+00,  6.1180e-01],\n",
      "          [-6.3335e-02,  2.3546e-01,  6.4032e-01,  ...,  8.1825e-01,\n",
      "            1.0655e+00,  2.5567e-01],\n",
      "          [-1.4611e-01,  1.0013e-01,  5.0975e-01,  ...,  1.2829e-01,\n",
      "            1.4078e-01, -4.1140e-02]],\n",
      "\n",
      "         [[-2.7557e-01, -1.0484e-01, -4.2092e-01,  ..., -2.2783e-01,\n",
      "           -2.7830e-01, -4.5442e-01],\n",
      "          [ 2.4626e-01,  3.2144e-01, -2.4703e-01,  ...,  1.8698e-01,\n",
      "            6.0159e-01, -3.6902e-01],\n",
      "          [ 2.1445e-01,  3.1934e-01, -1.7404e-01,  ...,  4.1281e-02,\n",
      "            3.7775e-01, -2.2605e-01],\n",
      "          ...,\n",
      "          [ 8.5489e-02, -1.2944e+00, -8.4769e-01,  ..., -6.9963e-01,\n",
      "            1.4174e-01,  9.3272e-01],\n",
      "          [-2.8956e-02,  1.1260e+00,  7.0134e-01,  ...,  8.2350e-01,\n",
      "            1.3879e+00,  2.1252e-01],\n",
      "          [-5.6558e-02,  4.4501e-01,  8.0401e-01,  ...,  5.5032e-01,\n",
      "            8.0590e-01,  3.2797e-02]],\n",
      "\n",
      "         [[ 1.4628e-01,  8.3949e-02,  3.8491e-01,  ...,  1.3519e-01,\n",
      "            1.3025e-01,  1.9370e-01],\n",
      "          [ 2.1554e-01, -2.1877e-01,  6.5360e-01,  ..., -5.4660e-01,\n",
      "           -2.4566e-01,  6.4054e-01],\n",
      "          [ 2.8886e-01, -2.2085e-01,  4.5114e-01,  ..., -5.8670e-01,\n",
      "           -9.1113e-02,  8.5946e-01],\n",
      "          ...,\n",
      "          [ 1.1066e+00,  2.9876e-01,  1.4380e+00,  ...,  8.7779e-01,\n",
      "            1.3425e+00,  9.3195e-01],\n",
      "          [ 4.6898e-01,  4.2164e-01, -4.1667e-02,  ...,  5.6327e-01,\n",
      "            4.9754e-01,  5.9475e-01],\n",
      "          [ 2.8160e-01,  9.4876e-02, -6.0574e-01,  ..., -3.1562e-01,\n",
      "           -4.4060e-01,  5.0145e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.0357e-02, -2.6333e-01,  9.0084e-02,  ...,  2.5615e-02,\n",
      "           -5.7209e-02, -1.7545e-01],\n",
      "          [-1.0859e-01, -9.0017e-01,  8.9348e-01,  ...,  1.1723e-01,\n",
      "           -1.3926e-01, -5.8269e-01],\n",
      "          [-8.3109e-02, -5.2839e-01,  8.2599e-01,  ...,  1.3529e-01,\n",
      "           -1.4572e-02, -4.8430e-01],\n",
      "          ...,\n",
      "          [-5.4828e-02,  5.4963e-01,  2.7295e-01,  ...,  1.6286e+00,\n",
      "            1.3994e+00,  9.0987e-01],\n",
      "          [-6.6735e-02,  1.9223e-01, -8.4493e-02,  ...,  1.5920e+00,\n",
      "            1.5853e+00,  4.3247e-01],\n",
      "          [-2.9729e-01, -8.8141e-02, -1.1591e-02,  ...,  1.0156e+00,\n",
      "            1.3839e+00,  2.1575e-01]],\n",
      "\n",
      "         [[-6.0431e-02, -1.0805e-01, -1.2425e-01,  ..., -7.5001e-02,\n",
      "           -8.6779e-02, -3.8647e-02],\n",
      "          [-1.1069e-01, -6.7788e-01, -4.2490e-01,  ..., -3.3281e-01,\n",
      "           -5.8954e-01, -1.6547e-01],\n",
      "          [-1.3260e-01, -4.6348e-01, -4.5730e-02,  ..., -3.1560e-01,\n",
      "           -3.6260e-01, -1.4182e-01],\n",
      "          ...,\n",
      "          [-4.8611e-01, -5.1555e-01, -9.0276e-02,  ...,  1.3788e+00,\n",
      "            1.2404e+00,  5.4130e-01],\n",
      "          [-2.3352e-01, -7.7964e-01, -5.8658e-01,  ...,  1.4006e+00,\n",
      "            1.2796e+00,  3.5998e-01],\n",
      "          [-4.9171e-01, -7.1924e-01, -5.4893e-01,  ...,  7.1865e-01,\n",
      "            5.8504e-01,  1.6650e-01]],\n",
      "\n",
      "         [[ 7.3454e-03, -1.2751e-02, -1.0608e-01,  ..., -9.3100e-02,\n",
      "           -7.1483e-02, -1.9104e-02],\n",
      "          [-1.4878e-02, -2.4124e-01, -5.0085e-01,  ..., -4.2734e-01,\n",
      "           -4.4751e-01, -4.2103e-03],\n",
      "          [-3.8238e-02, -2.6521e-01, -3.3527e-01,  ..., -4.2966e-01,\n",
      "           -2.7229e-01,  5.3395e-02],\n",
      "          ...,\n",
      "          [ 7.7607e-01,  6.6073e-01,  2.9552e-01,  ...,  4.9649e-01,\n",
      "            8.1526e-02, -1.7870e-01],\n",
      "          [ 1.3949e-01,  5.6672e-01,  1.9664e-01,  ...,  2.5289e-01,\n",
      "           -1.6143e-01, -1.7222e-01],\n",
      "          [ 1.1960e-01,  2.6531e-01,  1.5585e-01,  ...,  1.2784e-01,\n",
      "           -6.1909e-02, -9.6805e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.8693e-01, -4.5176e-02, -2.9217e-02,  ..., -2.8766e-01,\n",
      "           -2.3999e-01, -2.7140e-01],\n",
      "          [ 8.4349e-02,  1.2470e-01,  1.6913e-01,  ..., -5.9249e-01,\n",
      "           -8.5192e-01, -2.8956e-01],\n",
      "          [-2.8445e-01, -6.5592e-01, -7.6678e-01,  ..., -1.2096e-01,\n",
      "           -8.7425e-02, -1.4941e-01],\n",
      "          ...,\n",
      "          [-7.5600e-02, -4.9321e-02,  2.8005e-01,  ...,  1.9470e-01,\n",
      "            1.4278e-01,  2.7717e-02],\n",
      "          [-9.0900e-03,  1.7528e-01,  4.5267e-01,  ...,  2.0980e-01,\n",
      "            2.1877e-01,  4.7065e-02],\n",
      "          [-1.3165e-01,  1.3122e-02,  1.0826e-01,  ...,  5.1299e-02,\n",
      "            7.9542e-03, -7.6616e-02]],\n",
      "\n",
      "         [[-3.8238e-01, -5.6155e-02,  1.8961e-02,  ...,  8.3643e-02,\n",
      "            1.5876e-01, -4.0845e-01],\n",
      "          [-9.9579e-01, -1.4769e+00, -1.4779e+00,  ...,  7.5856e-02,\n",
      "            3.0867e-01, -4.5671e-01],\n",
      "          [-4.8450e-01, -5.0740e-01,  2.3490e-01,  ..., -1.2004e+00,\n",
      "           -1.1991e+00, -1.0384e+00],\n",
      "          ...,\n",
      "          [ 3.8787e-01,  1.2046e+00,  8.9248e-01,  ...,  5.4894e-01,\n",
      "            9.9900e-01,  3.4701e-01],\n",
      "          [ 2.0388e-01,  5.6982e-01,  5.5278e-01,  ...,  5.1991e-01,\n",
      "            1.2358e+00,  1.4476e-01],\n",
      "          [-3.7956e-01, -4.1308e-01, -3.5005e-01,  ..., -2.5244e-01,\n",
      "           -3.1425e-01, -3.2219e-01]],\n",
      "\n",
      "         [[ 3.9253e-01,  1.0373e+00,  7.6894e-01,  ..., -8.9671e-02,\n",
      "            8.2580e-02, -7.9887e-02],\n",
      "          [ 1.1311e+00,  1.3724e+00,  1.5008e+00,  ..., -1.0744e-02,\n",
      "            3.9702e-01,  2.1095e-01],\n",
      "          [ 1.2841e-01,  7.6395e-02, -5.3856e-01,  ..., -1.1503e+00,\n",
      "           -4.2287e-01,  5.9667e-02],\n",
      "          ...,\n",
      "          [ 5.9321e-02, -7.1522e-01, -4.9127e-01,  ..., -2.6155e-01,\n",
      "           -7.5772e-01,  2.6255e-02],\n",
      "          [ 3.0894e-01, -3.0444e-01, -4.9824e-01,  ..., -3.3265e-01,\n",
      "           -6.4384e-01,  2.2372e-01],\n",
      "          [ 1.3173e-01,  1.9531e-01, -1.2820e-02,  ..., -4.5566e-04,\n",
      "            7.7447e-02,  1.9943e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.4622e-02, -1.9028e-01, -1.4624e-01,  ..., -2.7675e-01,\n",
      "           -2.1461e-01, -2.1217e-01],\n",
      "          [-3.7129e-01, -6.0784e-01, -5.9629e-01,  ..., -2.6566e-01,\n",
      "           -4.4293e-01, -2.9902e-01],\n",
      "          [-3.6033e-01, -5.8673e-01, -3.5338e-01,  ..., -1.5765e-01,\n",
      "           -1.9813e-01, -3.6389e-01],\n",
      "          ...,\n",
      "          [-3.5345e-01, -5.9152e-01, -2.4934e-01,  ..., -3.0646e-01,\n",
      "           -7.2841e-01, -3.5513e-01],\n",
      "          [-1.8772e-01, -2.2795e-01, -1.5391e-01,  ..., -3.1701e-02,\n",
      "           -1.6458e-01, -1.2687e-01],\n",
      "          [-1.5677e-01, -7.1495e-02, -9.1873e-02,  ...,  3.8987e-03,\n",
      "           -2.8303e-03, -4.7427e-02]],\n",
      "\n",
      "         [[-1.4486e-02, -5.8774e-02, -5.4364e-02,  ..., -4.0858e-02,\n",
      "           -2.3723e-02,  2.1821e-02],\n",
      "          [-9.3689e-02, -1.2472e-02,  1.0037e-01,  ..., -7.7399e-02,\n",
      "           -1.5345e-01, -6.9851e-04],\n",
      "          [-1.2674e-01, -3.5562e-01, -3.3245e-02,  ..., -8.7251e-01,\n",
      "           -1.0082e+00, -7.0140e-01],\n",
      "          ...,\n",
      "          [ 2.4819e-02,  2.5498e-01,  1.9974e-01,  ...,  1.3609e-01,\n",
      "            2.1377e-02, -2.3465e-02],\n",
      "          [ 4.1128e-02,  3.6795e-01,  4.0631e-01,  ...,  2.7221e-01,\n",
      "            3.1932e-01,  3.7838e-02],\n",
      "          [ 2.7090e-02,  1.3776e-01,  2.2148e-01,  ...,  1.8674e-01,\n",
      "            1.6613e-01,  7.3846e-02]],\n",
      "\n",
      "         [[ 2.2999e-02,  1.6647e-01,  1.8744e-01,  ...,  1.9019e-02,\n",
      "            7.1074e-02,  1.0484e-02],\n",
      "          [ 2.7799e-01,  5.3866e-01,  5.5962e-01,  ...,  1.2346e-01,\n",
      "            3.0253e-01,  7.0036e-02],\n",
      "          [ 1.5851e-01,  6.1675e-01,  6.2766e-01,  ...,  1.7946e-01,\n",
      "            3.2626e-01,  1.2588e-01],\n",
      "          ...,\n",
      "          [ 1.8655e-02,  1.3968e-02,  1.5759e-02,  ...,  1.1736e-01,\n",
      "            2.5721e-01,  7.9912e-02],\n",
      "          [-3.4120e-02, -2.2614e-01, -1.3320e-01,  ..., -4.9029e-02,\n",
      "           -1.1677e-01, -2.1182e-03],\n",
      "          [-9.0410e-02, -5.6502e-02, -4.8990e-02,  ...,  3.9511e-05,\n",
      "           -2.3132e-02, -4.8591e-02]]]], device='cuda:0')}\n",
      "3\n",
      "tensor([[[[ 1.8427,  1.8256,  1.6886,  ..., -0.7774, -0.7603, -0.7603],\n",
      "          [ 1.8085,  1.7913,  1.6715,  ..., -0.7431, -0.7089, -0.7089],\n",
      "          [ 1.7571,  1.7400,  1.6201,  ..., -0.6746, -0.6233, -0.6061],\n",
      "          ...,\n",
      "          [-0.3321, -0.3150, -0.2636,  ...,  0.8152,  0.7810,  0.7810],\n",
      "          [-0.4863, -0.4863, -0.3835,  ...,  0.7638,  0.7125,  0.7125],\n",
      "          [-0.5719, -0.5548, -0.4349,  ...,  0.7467,  0.6782,  0.6782]],\n",
      "\n",
      "         [[ 1.2059,  1.1884,  1.1008,  ..., -0.6324, -0.6674, -0.6674],\n",
      "          [ 1.2059,  1.1884,  1.0833,  ..., -0.6148, -0.6324, -0.6324],\n",
      "          [ 1.1884,  1.1709,  1.0658,  ..., -0.5623, -0.5623, -0.5623],\n",
      "          ...,\n",
      "          [-0.8775, -0.8775, -0.8950,  ...,  0.0329, -0.0371, -0.0371],\n",
      "          [-0.9650, -0.9650, -0.9650,  ...,  0.0154, -0.0721, -0.0896],\n",
      "          [-1.0000, -1.0000, -1.0000,  ...,  0.0154, -0.0896, -0.1071]],\n",
      "\n",
      "         [[ 0.5861,  0.5686,  0.4466,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [ 0.5686,  0.5512,  0.4466,  ..., -0.9129, -0.8954, -0.8954],\n",
      "          [ 0.5512,  0.5338,  0.4466,  ..., -0.7211, -0.6688, -0.6688],\n",
      "          ...,\n",
      "          [-0.7908, -0.8083, -0.8606,  ..., -0.1111,  0.0283,  0.0458],\n",
      "          [-0.9129, -0.9129, -0.9303,  ..., -0.1634, -0.0065,  0.0109],\n",
      "          [-0.9651, -0.9651, -0.9651,  ..., -0.1808, -0.0240, -0.0065]]],\n",
      "\n",
      "\n",
      "        [[[-0.6404, -0.6746, -0.7260,  ...,  0.0617,  0.0104, -0.0410],\n",
      "          [-0.6575, -0.6746, -0.7089,  ..., -0.0068, -0.0410, -0.0753],\n",
      "          [-0.6746, -0.6918, -0.6746,  ..., -0.1438, -0.1609, -0.1780],\n",
      "          ...,\n",
      "          [ 2.6476,  2.6818,  2.7161,  ...,  3.1956,  3.1784,  3.1613],\n",
      "          [ 2.6476,  2.6647,  2.6989,  ...,  3.2127,  3.1956,  3.1956],\n",
      "          [ 2.6476,  2.6647,  2.6818,  ...,  3.2127,  3.2127,  3.2127]],\n",
      "\n",
      "         [[-0.6674, -0.7199, -0.7899,  ..., -0.0546, -0.1071, -0.1772],\n",
      "          [-0.6849, -0.7374, -0.7899,  ..., -0.1246, -0.1597, -0.2122],\n",
      "          [-0.7374, -0.7549, -0.7724,  ..., -0.2822, -0.2997, -0.3172],\n",
      "          ...,\n",
      "          [ 2.6415,  2.6765,  2.7115,  ...,  3.1842,  3.1667,  3.1317],\n",
      "          [ 2.6415,  2.6590,  2.6940,  ...,  3.2017,  3.1842,  3.1667],\n",
      "          [ 2.6415,  2.6590,  2.6765,  ...,  3.2017,  3.1842,  3.1842]],\n",
      "\n",
      "         [[-0.6166, -0.7037, -0.8083,  ..., -0.2505, -0.2854, -0.3377],\n",
      "          [-0.6340, -0.7037, -0.7908,  ..., -0.3028, -0.3377, -0.3725],\n",
      "          [-0.6688, -0.7211, -0.7734,  ..., -0.4423, -0.4597, -0.4771],\n",
      "          ...,\n",
      "          [ 2.2767,  2.3115,  2.3464,  ...,  2.7473,  2.7124,  2.6601],\n",
      "          [ 2.2767,  2.2941,  2.3290,  ...,  2.7647,  2.7298,  2.6950],\n",
      "          [ 2.2767,  2.2941,  2.3115,  ...,  2.7647,  2.7298,  2.7124]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/djjin/Mygit/X-Decoder/notebooks/09_hoi_model_evaluate_test.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/09_hoi_model_evaluate_test.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     opt[\u001b[39m'\u001b[39m\u001b[39mbase_path\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m absolute_user_dir\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/09_hoi_model_evaluate_test.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(opt)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/09_hoi_model_evaluate_test.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m trainer\u001b[39m.\u001b[39;49meval()\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/trainer/default_trainer.py:87\u001b[0m, in \u001b[0;36mDefaultTrainer.eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel directory not found: \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_eval_on_set(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_folder)\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/trainer/default_trainer.py:92\u001b[0m, in \u001b[0;36mDefaultTrainer._eval_on_set\u001b[0;34m(self, save_folder)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_eval_on_set\u001b[39m(\u001b[39mself\u001b[39m, save_folder):\n\u001b[1;32m     91\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEvaluation start ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpipeline\u001b[39m.\u001b[39;49mevaluate_model(\u001b[39mself\u001b[39;49m, save_folder)\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt[\u001b[39m'\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     94\u001b[0m         logger\u001b[39m.\u001b[39minfo(results)\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/pipeline/HDecoderPipeline.py:165\u001b[0m, in \u001b[0;36mHDecoderPipeline.evaluate_model\u001b[0;34m(self, trainer, save_folder)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_opt[\u001b[39m'\u001b[39m\u001b[39mFP16\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# in FP16 mode, DeepSpeed casts the model to FP16, so the input needs to be manually casted to FP16\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     batch \u001b[39m=\u001b[39m cast_batch_to_half(batch)\n\u001b[0;32m--> 165\u001b[0m outputs \u001b[39m=\u001b[39m model(batch, mode\u001b[39m=\u001b[39;49meval_type)\n\u001b[1;32m    166\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    167\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/BaseModel.py:25\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 25\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/architectures/hoi_model.py:128\u001b[0m, in \u001b[0;36mCDNHOI.forward\u001b[0;34m(self, batched_inputs, mode)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhoi\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_hoi(batched_inputs)\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/architectures/hoi_model.py:180\u001b[0m, in \u001b[0;36mCDNHOI.evaluate_hoi\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mprint\u001b[39m(images\u001b[39m.\u001b[39mtensor)\n\u001b[1;32m    179\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone(images\u001b[39m.\u001b[39mtensor)\n\u001b[0;32m--> 180\u001b[0m \u001b[39mprint\u001b[39;49m(features)\n\u001b[1;32m    181\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhoid_head(features, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    182\u001b[0m \u001b[39m# print(outputs)\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[39m# TODO\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/_tensor.py:427\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    424\u001b[0m         Tensor\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_tensor_str\u001b[39m.\u001b[39;49m_str(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/_tensor_str.py:637\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    636\u001b[0m     guard \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 637\u001b[0m     \u001b[39mreturn\u001b[39;00m _str_intern(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/_tensor_str.py:568\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    566\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    567\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 568\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39;49m, indent)\n\u001b[1;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mstrided:\n\u001b[1;32m    571\u001b[0m     suffixes\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mlayout=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/_tensor_str.py:328\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    325\u001b[0m         \u001b[39mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     formatter \u001b[39m=\u001b[39m _Formatter(get_summarized_data(\u001b[39mself\u001b[39;49m) \u001b[39mif\u001b[39;49;00m summarize \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    329\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/_tensor_str.py:115\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mlen\u001b[39m(value_str))\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     nonzero_finite_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmasked_select(\n\u001b[1;32m    116\u001b[0m         tensor_view, torch\u001b[39m.\u001b[39;49misfinite(tensor_view) \u001b[39m&\u001b[39;49m tensor_view\u001b[39m.\u001b[39;49mne(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m nonzero_finite_vals\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    120\u001b[0m         \u001b[39m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from trainer import HDecoder_Trainer as Trainer\n",
    "if cmdline_args.user_dir:\n",
    "    absolute_user_dir = os.path.abspath(cmdline_args.user_dir)\n",
    "    opt['base_path'] = absolute_user_dir\n",
    "trainer = Trainer(opt)\n",
    "trainer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_Decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
