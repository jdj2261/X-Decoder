{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjin/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  1.13 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "os.environ[\"DATASET\"] = \"../datasets\"\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures import Boxes, ImageList, Instances, BitMasks, BoxMode\n",
    "\n",
    "from hdecoder.BaseModel import BaseModel\n",
    "from hdecoder import build_model\n",
    "\n",
    "from utils.arguments import load_opt_command\n",
    "from utils.misc import hook_metadata, hook_switcher, hook_opt\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "# cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/svlp_focalt_lang.yaml\")]\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/hdecoder/vcoco.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', '../checkpoints/xdecoder_focalt_best_openseg.pt'] \n",
    "cmdline_args.overrides\n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "config_dict\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel(opt, build_model(opt)).train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (model): CDNHOI(\n",
       "    (backbone): D2FocalNet(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=96, out_features=196, bias=True)\n",
       "                (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=96, out_features=196, bias=True)\n",
       "                (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (drop_path): DropPath(drop_prob=0.027)\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=192, out_features=388, bias=True)\n",
       "                (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (drop_path): DropPath(drop_prob=0.055)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=192, out_features=388, bias=True)\n",
       "                (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (drop_path): DropPath(drop_prob=0.082)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.109)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.136)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.164)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.191)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.218)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.245)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=768, out_features=1540, bias=True)\n",
       "                (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (drop_path): DropPath(drop_prob=0.273)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=768, out_features=1540, bias=True)\n",
       "                (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (drop_path): DropPath(drop_prob=0.300)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (hoid_head): CDN(\n",
       "      (encoder): TransformerEncoderHOI(\n",
       "        (adapter_1): Conv2d(\n",
       "          96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_1): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (adapter_2): Conv2d(\n",
       "          192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_2): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (adapter_3): Conv2d(\n",
       "          384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_3): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (mask_features): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (input_proj): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer): TransformerEncoderOnly(\n",
       "          (encoder): TransformerEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (3): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (4): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (5): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pe_layer): Positional encoding PositionEmbeddingSine\n",
       "            num_pos_feats: 128\n",
       "            temperature: 10000\n",
       "            normalize: True\n",
       "            scale: 6.283185307179586\n",
       "      )\n",
       "      (hoi_decoder): HDecoder(\n",
       "        (hopd_decoder): TransformerDecoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (interaction_decoder): TransformerDecoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (query_embed): Embedding(100, 256)\n",
       "      (obj_class_embed): Linear(in_features=256, out_features=82, bias=True)\n",
       "      (verb_class_embed): Linear(in_features=256, out_features=29, bias=True)\n",
       "      (sub_bbox_embed): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (obj_bbox_embed): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (criterion): SetCriterionHOI(\n",
       "      (matcher): HungarianMatcherHOI()\n",
       "    )\n",
       "    (postprocessors): PostProcessHOI()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdecoder.modules.criterion import SetCriterionHOI\n",
    "from hdecoder.modules.matcher import HungarianMatcherHOI\n",
    "\n",
    "matcher = HungarianMatcherHOI(\n",
    "    cost_obj_class=1, \n",
    "    cost_verb_class=1,\n",
    "    cost_bbox=2.5, \n",
    "    cost_giou=1, \n",
    "    cost_matching=1).cuda()\n",
    "\n",
    "weight_dict = {}\n",
    "weight_dict['loss_obj_ce'] = 1\n",
    "weight_dict['loss_verb_ce'] = 2\n",
    "weight_dict['loss_sub_bbox'] = 2.5\n",
    "weight_dict['loss_obj_bbox'] = 2.5\n",
    "weight_dict['loss_sub_giou'] = 1\n",
    "weight_dict['loss_obj_giou'] = 1\n",
    "losses = ['obj_labels', 'verb_labels', 'sub_obj_boxes', 'obj_cardinality']\n",
    "\n",
    "num_obj_classes = 80\n",
    "num_queries = 100\n",
    "num_verb_classes = 29\n",
    "eos_coef = 0.1\n",
    "criterion = SetCriterionHOI(\n",
    "    num_obj_classes, num_queries, num_verb_classes, matcher=matcher,\n",
    "                            weight_dict=weight_dict, eos_coef=eos_coef, losses=losses).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 5400\n"
     ]
    }
   ],
   "source": [
    "from datasets.build import build_train_dataloader\n",
    "train_data_loader = build_train_dataloader(opt)\n",
    "dataset_names = opt['DATASETS']['TRAIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_obj_bbox': tensor(1.0448, device='cuda:0'),\n",
      " 'loss_obj_bbox_0': tensor(1.0340, device='cuda:0'),\n",
      " 'loss_obj_bbox_1': tensor(1.0387, device='cuda:0'),\n",
      " 'loss_obj_ce': tensor(4.9381, device='cuda:0'),\n",
      " 'loss_obj_ce_0': tensor(5.1041, device='cuda:0'),\n",
      " 'loss_obj_ce_1': tensor(5.0056, device='cuda:0'),\n",
      " 'loss_obj_giou': tensor(1.0426, device='cuda:0'),\n",
      " 'loss_obj_giou_0': tensor(1.0614, device='cuda:0'),\n",
      " 'loss_obj_giou_1': tensor(1.0327, device='cuda:0'),\n",
      " 'loss_sub_bbox': tensor(0.5180, device='cuda:0'),\n",
      " 'loss_sub_bbox_0': tensor(0.5116, device='cuda:0'),\n",
      " 'loss_sub_bbox_1': tensor(0.5163, device='cuda:0'),\n",
      " 'loss_sub_giou': tensor(0.7062, device='cuda:0'),\n",
      " 'loss_sub_giou_0': tensor(0.7009, device='cuda:0'),\n",
      " 'loss_sub_giou_1': tensor(0.7017, device='cuda:0'),\n",
      " 'loss_verb_ce': tensor(82.1124, device='cuda:0'),\n",
      " 'loss_verb_ce_0': tensor(74.2222, device='cuda:0'),\n",
      " 'loss_verb_ce_1': tensor(78.7995, device='cuda:0'),\n",
      " 'obj_cardinality_error': tensor(97., device='cuda:0'),\n",
      " 'obj_cardinality_error_0': tensor(97., device='cuda:0'),\n",
      " 'obj_cardinality_error_1': tensor(97., device='cuda:0'),\n",
      " 'obj_class_error': tensor(100., device='cuda:0')}\n",
      "{'loss_obj_bbox': tensor(1.1334, device='cuda:0'),\n",
      " 'loss_obj_bbox_0': tensor(1.1322, device='cuda:0'),\n",
      " 'loss_obj_bbox_1': tensor(1.1331, device='cuda:0'),\n",
      " 'loss_obj_ce': tensor(4.8954, device='cuda:0'),\n",
      " 'loss_obj_ce_0': tensor(5.0858, device='cuda:0'),\n",
      " 'loss_obj_ce_1': tensor(4.9702, device='cuda:0'),\n",
      " 'loss_obj_giou': tensor(1.0043, device='cuda:0'),\n",
      " 'loss_obj_giou_0': tensor(1.0227, device='cuda:0'),\n",
      " 'loss_obj_giou_1': tensor(1.0051, device='cuda:0'),\n",
      " 'loss_sub_bbox': tensor(0.7013, device='cuda:0'),\n",
      " 'loss_sub_bbox_0': tensor(0.7036, device='cuda:0'),\n",
      " 'loss_sub_bbox_1': tensor(0.7011, device='cuda:0'),\n",
      " 'loss_sub_giou': tensor(0.9972, device='cuda:0'),\n",
      " 'loss_sub_giou_0': tensor(1.0268, device='cuda:0'),\n",
      " 'loss_sub_giou_1': tensor(1.0134, device='cuda:0'),\n",
      " 'loss_verb_ce': tensor(52.5369, device='cuda:0'),\n",
      " 'loss_verb_ce_0': tensor(47.3992, device='cuda:0'),\n",
      " 'loss_verb_ce_1': tensor(50.5149, device='cuda:0'),\n",
      " 'obj_cardinality_error': tensor(95.5000, device='cuda:0'),\n",
      " 'obj_cardinality_error_0': tensor(95.5000, device='cuda:0'),\n",
      " 'obj_cardinality_error_1': tensor(95.5000, device='cuda:0'),\n",
      " 'obj_class_error': tensor(100., device='cuda:0')}\n",
      "{'loss_obj_bbox': tensor(1.1737, device='cuda:0'),\n",
      " 'loss_obj_bbox_0': tensor(1.1626, device='cuda:0'),\n",
      " 'loss_obj_bbox_1': tensor(1.1714, device='cuda:0'),\n",
      " 'loss_obj_ce': tensor(4.7989, device='cuda:0'),\n",
      " 'loss_obj_ce_0': tensor(5.0326, device='cuda:0'),\n",
      " 'loss_obj_ce_1': tensor(4.9351, device='cuda:0'),\n",
      " 'loss_obj_giou': tensor(1.0474, device='cuda:0'),\n",
      " 'loss_obj_giou_0': tensor(1.0622, device='cuda:0'),\n",
      " 'loss_obj_giou_1': tensor(1.0415, device='cuda:0'),\n",
      " 'loss_sub_bbox': tensor(0.6227, device='cuda:0'),\n",
      " 'loss_sub_bbox_0': tensor(0.6507, device='cuda:0'),\n",
      " 'loss_sub_bbox_1': tensor(0.6386, device='cuda:0'),\n",
      " 'loss_sub_giou': tensor(0.8225, device='cuda:0'),\n",
      " 'loss_sub_giou_0': tensor(0.8483, device='cuda:0'),\n",
      " 'loss_sub_giou_1': tensor(0.8347, device='cuda:0'),\n",
      " 'loss_verb_ce': tensor(43.2247, device='cuda:0'),\n",
      " 'loss_verb_ce_0': tensor(39.3311, device='cuda:0'),\n",
      " 'loss_verb_ce_1': tensor(41.8017, device='cuda:0'),\n",
      " 'obj_cardinality_error': tensor(95.5000, device='cuda:0'),\n",
      " 'obj_cardinality_error_0': tensor(95.5000, device='cuda:0'),\n",
      " 'obj_cardinality_error_1': tensor(95.5000, device='cuda:0'),\n",
      " 'obj_class_error': tensor(100., device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        if idx > 2:\n",
    "            break\n",
    "        # print(batch)\n",
    "        losses = model(batch)\n",
    "        pprint(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
