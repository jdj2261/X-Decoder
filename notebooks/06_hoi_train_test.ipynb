{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjin/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  1.13 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures import Boxes, ImageList, Instances, BitMasks, BoxMode\n",
    "\n",
    "from hdecoder.BaseModel import BaseModel\n",
    "from hdecoder import build_model\n",
    "\n",
    "from utils.arguments import load_opt_command\n",
    "from utils.misc import hook_metadata, hook_switcher, hook_opt\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "# cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/svlp_focalt_lang.yaml\")]\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/vcoco.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', '../checkpoints/xdecoder_focalt_best_openseg.pt'] \n",
    "cmdline_args.overrides\n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "config_dict\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel(opt, build_model(opt)).train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets.registration.register_vcoco_dataset import register_all_vcoco\n",
    "# _root = os.getenv(\"DATASET\", \"../datasets\")\n",
    "# register_all_vcoco(_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdecoder.modules.criterion import SetCriterionHOI\n",
    "from hdecoder.modules.matcher import HungarianMatcherHOI\n",
    "\n",
    "matcher = HungarianMatcherHOI(\n",
    "    cost_obj_class=1, \n",
    "    cost_verb_class=1,\n",
    "    cost_bbox=2.5, \n",
    "    cost_giou=1, \n",
    "    cost_matching=1).cuda()\n",
    "\n",
    "weight_dict = {}\n",
    "weight_dict['loss_obj_ce'] = 1\n",
    "weight_dict['loss_verb_ce'] = 2\n",
    "weight_dict['loss_sub_bbox'] = 2.5\n",
    "weight_dict['loss_obj_bbox'] = 2.5\n",
    "weight_dict['loss_sub_giou'] = 1\n",
    "weight_dict['loss_obj_giou'] = 1\n",
    "losses = ['obj_labels', 'verb_labels', 'sub_obj_boxes', 'obj_cardinality']\n",
    "\n",
    "num_obj_classes = 80\n",
    "num_queries = 100\n",
    "num_verb_classes = 29\n",
    "eos_coef = 0.1\n",
    "criterion = SetCriterionHOI(\n",
    "    num_obj_classes, num_queries, num_verb_classes, matcher=matcher,\n",
    "                            weight_dict=weight_dict, eos_coef=eos_coef, losses=losses).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n"
     ]
    }
   ],
   "source": [
    "from datasets.build import build_train_dataloader\n",
    "train_data_loader = build_train_dataloader(opt)\n",
    "dataset_names = opt['DATASETS']['TRAIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_obj_bbox': tensor(0.9382, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_obj_bbox_0': tensor(0.9680, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_obj_bbox_1': tensor(0.9725, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_obj_ce': tensor(4.5616, device='cuda:0', grad_fn=<NllLoss2DBackward0>),\n",
      " 'loss_obj_ce_0': tensor(4.4895, device='cuda:0', grad_fn=<NllLoss2DBackward0>),\n",
      " 'loss_obj_ce_1': tensor(4.5098, device='cuda:0', grad_fn=<NllLoss2DBackward0>),\n",
      " 'loss_obj_giou': tensor(0.9772, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_obj_giou_0': tensor(0.9765, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_obj_giou_1': tensor(0.9776, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_sub_bbox': tensor(0.9178, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_sub_bbox_0': tensor(0.9199, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_sub_bbox_1': tensor(0.9050, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_sub_giou': tensor(0.9729, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_sub_giou_0': tensor(0.9367, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_sub_giou_1': tensor(0.9673, device='cuda:0', grad_fn=<DivBackward0>),\n",
      " 'loss_verb_ce': tensor(146.3899, device='cuda:0', grad_fn=<RsubBackward1>),\n",
      " 'loss_verb_ce_0': tensor(141.0123, device='cuda:0', grad_fn=<RsubBackward1>),\n",
      " 'loss_verb_ce_1': tensor(139.9866, device='cuda:0', grad_fn=<RsubBackward1>),\n",
      " 'obj_cardinality_error': tensor(100., device='cuda:0'),\n",
      " 'obj_cardinality_error_0': tensor(100., device='cuda:0'),\n",
      " 'obj_cardinality_error_1': tensor(100., device='cuda:0'),\n",
      " 'obj_class_error': tensor(100., device='cuda:0')}\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 3.81 GiB total capacity; 2.60 GiB already allocated; 9.69 MiB free; 2.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/djjin/Mygit/X-Decoder/notebooks/06_hoi_train_test.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/06_hoi_train_test.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/06_hoi_train_test.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# print(batch)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/06_hoi_train_test.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m losses \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/06_hoi_train_test.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m pprint(losses)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/BaseModel.py:19\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 19\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/architectures/hoi_model.py:101\u001b[0m, in \u001b[0;36mCDNHOI.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, batched_inputs):\n\u001b[1;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 101\u001b[0m         losses_hoi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_hoi(batched_inputs[\u001b[39m\"\u001b[39;49m\u001b[39mvcoco\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m losses_hoi\n\u001b[1;32m    103\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/architectures/hoi_model.py:112\u001b[0m, in \u001b[0;36mCDNHOI.forward_hoi\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    109\u001b[0m images \u001b[39m=\u001b[39m [(x \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpixel_mean) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpixel_std \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    110\u001b[0m images \u001b[39m=\u001b[39m ImageList\u001b[39m.\u001b[39mfrom_tensors(images, \u001b[39m32\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensor)\n\u001b[1;32m    114\u001b[0m \u001b[39m# TODO not mask None\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m# src, mask = features[-1].decompose()\u001b[39;00m\n\u001b[1;32m    116\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhoid_head(features, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/backbone/focal_dw.py:878\u001b[0m, in \u001b[0;36mD2FocalNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    875\u001b[0m     x\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m    876\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSwinTransformer takes an input of shape (N, C, H, W). Got \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m instead!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    877\u001b[0m outputs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 878\u001b[0m y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m    879\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m y\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    880\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_out_features:\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/backbone/focal_dw.py:784\u001b[0m, in \u001b[0;36mFocalNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n\u001b[1;32m    783\u001b[0m     layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\n\u001b[0;32m--> 784\u001b[0m     x_out, H, W, x, Wh, Ww \u001b[39m=\u001b[39m layer(x, Wh, Ww)\n\u001b[1;32m    785\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_indices:\n\u001b[1;32m    786\u001b[0m         norm_layer \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnorm\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/backbone/focal_dw.py:349\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[0;34m(self, x, H, W)\u001b[0m\n\u001b[1;32m    347\u001b[0m         x \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39mcheckpoint(blk, x)\n\u001b[1;32m    348\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         x \u001b[39m=\u001b[39m blk(x)\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m     x_reshaped \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], H, W)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/backbone/focal_dw.py:242\u001b[0m, in \u001b[0;36mFocalModulationBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    239\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(B, H, W, C)\n\u001b[1;32m    241\u001b[0m \u001b[39m# FM\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodulation(x)\u001b[39m.\u001b[39mview(B, H \u001b[39m*\u001b[39m W, C)\n\u001b[1;32m    243\u001b[0m x \u001b[39m=\u001b[39m shortcut \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma_1 \u001b[39m*\u001b[39m x)\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_postln:\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/backbone/focal_dw.py:131\u001b[0m, in \u001b[0;36mFocalModulation.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfocal_level):\n\u001b[1;32m    130\u001b[0m     ctx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfocal_layers[l](ctx)\n\u001b[0;32m--> 131\u001b[0m     ctx_all \u001b[39m=\u001b[39m ctx_all \u001b[39m+\u001b[39;49m ctx \u001b[39m*\u001b[39;49m gates[:, l : l \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]\n\u001b[1;32m    132\u001b[0m ctx_global \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(ctx\u001b[39m.\u001b[39mmean(\u001b[39m2\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mmean(\u001b[39m3\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m    133\u001b[0m ctx_all \u001b[39m=\u001b[39m ctx_all \u001b[39m+\u001b[39m ctx_global \u001b[39m*\u001b[39m gates[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfocal_level :]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 3.81 GiB total capacity; 2.60 GiB already allocated; 9.69 MiB free; 2.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "for idx, batch in enumerate(train_data_loader):\n",
    "    if idx > 2:\n",
    "        break\n",
    "    # print(batch)\n",
    "    losses = model(batch)\n",
    "    pprint(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
