{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  2.0 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "os.environ[\"DATASET\"] = \"../datasets\"\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures import Boxes, ImageList, Instances, BitMasks, BoxMode\n",
    "\n",
    "from hdecoder.BaseModel import BaseModel\n",
    "from hdecoder import build_model\n",
    "\n",
    "from utils.arguments import load_opt_command\n",
    "from utils.misc import hook_metadata, hook_switcher, hook_opt\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "# cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/svlp_focalt_lang.yaml\")]\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/hdecoder/vcoco.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', 'true', 'RESUME_FROM', '../checkpoints/xdecoder_focalt_best_openseg.pt'] \n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "opt[\"base_path\"] = \"../\"\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'backbone': True}, {'encoder': True}]\n"
     ]
    }
   ],
   "source": [
    "pprint(opt[\"MODEL\"][\"FREEZE_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backbone': True, 'encoder': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_param = opt['SOLVER'].get('FIX_PARAM',{})\n",
    "fix_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trainer.distributed_trainer:Setting SAVE_DIR as ../data/output/test\n",
      "INFO:trainer.distributed_trainer:Using CUDA\n",
      "WARNING:trainer.utils.mpi_adapter:----------------\n",
      "WARNING:trainer.utils.mpi_adapter:MPI Adapter data\n",
      "WARNING:trainer.utils.mpi_adapter:----------------\n",
      "WARNING:trainer.utils.mpi_adapter:environment info: no MPI\n",
      "WARNING:trainer.utils.mpi_adapter:init method url: tcp://127.0.0.1:36873\n",
      "WARNING:trainer.utils.mpi_adapter:world size: 1\n",
      "WARNING:trainer.utils.mpi_adapter:local size: 1\n",
      "WARNING:trainer.utils.mpi_adapter:rank: 0\n",
      "WARNING:trainer.utils.mpi_adapter:local rank: 0\n",
      "WARNING:trainer.utils.mpi_adapter:master address: 127.0.0.1\n",
      "WARNING:trainer.utils.mpi_adapter:master port: 36873\n",
      "WARNING:trainer.utils.mpi_adapter:----------------\n",
      "INFO:trainer.distributed_trainer:Save config file to ../data/output/test/conf_copy.yaml\n",
      "INFO:trainer.distributed_trainer:Base learning rate: 0.0001\n",
      "INFO:trainer.distributed_trainer:Number of GPUs: 1\n",
      "INFO:trainer.distributed_trainer:Gradient accumulation steps: 1\n",
      "INFO:trainer.default_trainer:Imported base_dir at base_path ../\n",
      "INFO:trainer.default_trainer:Pipeline for training: HDecoderPipeline\n",
      "INFO:trainer.default_trainer:-------------------------------------------------------\n",
      "INFO:trainer.default_trainer:Training on rank: 0\n",
      "INFO:base_dir.pipeline.HDecoderPipeline:CDNHOI(\n",
      "  (backbone): D2FocalNet(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=96, out_features=196, bias=True)\n",
      "              (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=96, out_features=196, bias=True)\n",
      "              (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (drop_path): DropPath(drop_prob=0.027)\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchEmbed(\n",
      "          (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=192, out_features=388, bias=True)\n",
      "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (drop_path): DropPath(drop_prob=0.055)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=192, out_features=388, bias=True)\n",
      "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (drop_path): DropPath(drop_prob=0.082)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchEmbed(\n",
      "          (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.109)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.136)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.164)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.191)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.218)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.245)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchEmbed(\n",
      "          (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=768, out_features=1540, bias=True)\n",
      "              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (drop_path): DropPath(drop_prob=0.273)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=768, out_features=1540, bias=True)\n",
      "              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (drop_path): DropPath(drop_prob=0.300)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (hoid_head): CDN(\n",
      "    (encoder): TransformerEncoderHOI(\n",
      "      (adapter_1): Conv2d(\n",
      "        96, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (adapter_2): Conv2d(\n",
      "        192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_2): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (adapter_3): Conv2d(\n",
      "        384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_3): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (input_proj): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (transformer): TransformerEncoderOnly(\n",
      "        (encoder): TransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-5): 6 x TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 256\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (layer_4): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (hoi_decoder): HDecoder(\n",
      "      (hopd_decoder): TransformerDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-2): 3 x TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (interaction_decoder): TransformerDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-2): 3 x TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (query_embed): Embedding(101, 512)\n",
      "    (obj_class_embed): Linear(in_features=512, out_features=81, bias=True)\n",
      "    (verb_class_embed): Linear(in_features=512, out_features=29, bias=True)\n",
      "    (sub_bbox_embed): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=512, out_features=512, bias=True)\n",
      "        (2): Linear(in_features=512, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (obj_bbox_embed): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=512, out_features=512, bias=True)\n",
      "        (2): Linear(in_features=512, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): SetCriterionHOI(\n",
      "    (matcher): HungarianMatcherHOI()\n",
      "  )\n",
      "  (postprocessors): PostProcessHOI()\n",
      ")\n",
      "INFO:datasets.build:Using training sampler TrainingSampler\n",
      "INFO:detectron2.data.common:Serializing 5400 elements to byte tensors and concatenating them all ...\n",
      "INFO:detectron2.data.common:Serialized dataset takes 3.62 MiB\n",
      "INFO:base_dir.pipeline.HDecoderPipeline:num of train samples: 5400\n",
      "INFO:trainer.hdecoder_trainer:Calculate MAX_ITER @ 48600000 and STEPS @ [43200054, 46799856]\n",
      "INFO:trainer.hdecoder_trainer:Total number of parameters in default module (on each GPU): 87767654\n",
      "INFO:trainer.hdecoder_trainer:Number of trainable parameters in default module (on each GPU): 26389110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 5400\n",
      "num of train samples: 5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw1.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw2.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_1, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_2, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.bias, Model Shape: torch.Size([196]) <-> Ckpt Shape: torch.Size([196])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.weight, Model Shape: torch.Size([196, 96]) <-> Ckpt Shape: torch.Size([196, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([96, 1, 5, 5]) <-> Ckpt Shape: torch.Size([96, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([96, 1, 7, 7]) <-> Ckpt Shape: torch.Size([96, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.weight, Model Shape: torch.Size([96, 96, 1, 1]) <-> Ckpt Shape: torch.Size([96, 96, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw1.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw2.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_1, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_2, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.bias, Model Shape: torch.Size([196]) <-> Ckpt Shape: torch.Size([196])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.weight, Model Shape: torch.Size([196, 96]) <-> Ckpt Shape: torch.Size([196, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([96, 1, 5, 5]) <-> Ckpt Shape: torch.Size([96, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([96, 1, 7, 7]) <-> Ckpt Shape: torch.Size([96, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.weight, Model Shape: torch.Size([96, 96, 1, 1]) <-> Ckpt Shape: torch.Size([96, 96, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.weight, Model Shape: torch.Size([192, 96, 3, 3]) <-> Ckpt Shape: torch.Size([192, 96, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw1.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw2.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.bias, Model Shape: torch.Size([388]) <-> Ckpt Shape: torch.Size([388])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.weight, Model Shape: torch.Size([388, 192]) <-> Ckpt Shape: torch.Size([388, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw1.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw2.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.bias, Model Shape: torch.Size([388]) <-> Ckpt Shape: torch.Size([388])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.weight, Model Shape: torch.Size([388, 192]) <-> Ckpt Shape: torch.Size([388, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.weight, Model Shape: torch.Size([384, 192, 3, 3]) <-> Ckpt Shape: torch.Size([384, 192, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.weight, Model Shape: torch.Size([768, 384, 3, 3]) <-> Ckpt Shape: torch.Size([768, 384, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw1.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw2.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.bias, Model Shape: torch.Size([1540]) <-> Ckpt Shape: torch.Size([1540])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.weight, Model Shape: torch.Size([1540, 768]) <-> Ckpt Shape: torch.Size([1540, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw1.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw2.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.bias, Model Shape: torch.Size([1540]) <-> Ckpt Shape: torch.Size([1540])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.weight, Model Shape: torch.Size([1540, 768]) <-> Ckpt Shape: torch.Size([1540, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.norm0.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.norm0.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.norm3.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.norm3.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.norm.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.norm.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.proj.weight, Model Shape: torch.Size([96, 3, 7, 7]) <-> Ckpt Shape: torch.Size([96, 3, 7, 7])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.weight, Model Shape: torch.Size([512, 96, 1, 1]) <-> Ckpt Shape: torch.Size([512, 96, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.weight, Model Shape: torch.Size([512, 192, 1, 1]) <-> Ckpt Shape: torch.Size([512, 192, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.weight, Model Shape: torch.Size([512, 384, 1, 1]) <-> Ckpt Shape: torch.Size([512, 384, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.input_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.input_proj.weight, Model Shape: torch.Size([512, 768, 1, 1]) <-> Ckpt Shape: torch.Size([512, 768, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_4.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.mask_features.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.mask_features.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.norm.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.norm.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.norm.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.norm.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.0.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.0.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.1.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.2.bias, Model Shape: torch.Size([4])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.2.weight, Model Shape: torch.Size([4, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_class_embed.bias, Model Shape: torch.Size([81])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_class_embed.weight, Model Shape: torch.Size([81, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.query_embed.weight, Model Shape: torch.Size([101, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.0.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.0.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.1.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.2.bias, Model Shape: torch.Size([4])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.2.weight, Model Shape: torch.Size([4, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.verb_class_embed.bias, Model Shape: torch.Size([29])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.verb_class_embed.weight, Model Shape: torch.Size([29, 512])\n",
      "WARNING:utils.model:$UNUSED$ backbone_proj, Ckpt Shape: torch.Size([768, 512])\n",
      "WARNING:utils.model:$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.caping_embed, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.class_embed, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.decoder_norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.decoder_norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding, Ckpt Shape: torch.Size([77, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.token_embedding.weight, Ckpt Shape: torch.Size([49408, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_proj, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.logit_scale, Ckpt Shape: torch.Size([])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.level_embed.weight, Ckpt Shape: torch.Size([3, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.0.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.0.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.1.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.2.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.query_embed.weight, Ckpt Shape: torch.Size([101, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.query_feat.weight, Ckpt Shape: torch.Size([101, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.query_feat_caping.weight, Ckpt Shape: torch.Size([77, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.self_attn_mask, Ckpt Shape: torch.Size([1, 178, 178])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNMATCHED* criterion.empty_weight, Model Shape: torch.Size([81]) <-> Ckpt Shape: torch.Size([134])\n",
      "WARNING:trainer.utils_trainer:Load weights from ../checkpoints/xdecoder_focalt_best_openseg.pt...\n",
      "INFO:trainer.default_trainer:***** Running training *****\n",
      "INFO:trainer.default_trainer:  Num of GPUs = 1\n",
      "INFO:trainer.default_trainer:  Num Epochs = 90\n",
      "INFO:trainer.default_trainer:  Num of Mini Batches per Epoch = 5400\n",
      "INFO:trainer.default_trainer:  Total train batch size (w. parallel, distributed & accumulation) = 486000\n",
      "INFO:trainer.default_trainer:  Gradient Accumulation steps = 1\n",
      "INFO:trainer.default_trainer:  Total optimization steps = 486000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.backbone.patch_embed.proj.weight False\n",
      "model.backbone.patch_embed.proj.bias False\n",
      "model.backbone.patch_embed.norm.weight False\n",
      "model.backbone.patch_embed.norm.bias False\n",
      "model.backbone.layers.0.blocks.0.gamma_1 False\n",
      "model.backbone.layers.0.blocks.0.gamma_2 False\n",
      "model.backbone.layers.0.blocks.0.dw1.weight False\n",
      "model.backbone.layers.0.blocks.0.dw1.bias False\n",
      "model.backbone.layers.0.blocks.0.norm1.weight False\n",
      "model.backbone.layers.0.blocks.0.norm1.bias False\n",
      "model.backbone.layers.0.blocks.0.modulation.f.weight False\n",
      "model.backbone.layers.0.blocks.0.modulation.f.bias False\n",
      "model.backbone.layers.0.blocks.0.modulation.h.weight False\n",
      "model.backbone.layers.0.blocks.0.modulation.h.bias False\n",
      "model.backbone.layers.0.blocks.0.modulation.proj.weight False\n",
      "model.backbone.layers.0.blocks.0.modulation.proj.bias False\n",
      "model.backbone.layers.0.blocks.0.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.0.blocks.0.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.0.blocks.0.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.0.blocks.0.dw2.weight False\n",
      "model.backbone.layers.0.blocks.0.dw2.bias False\n",
      "model.backbone.layers.0.blocks.0.norm2.weight False\n",
      "model.backbone.layers.0.blocks.0.norm2.bias False\n",
      "model.backbone.layers.0.blocks.0.mlp.fc1.weight False\n",
      "model.backbone.layers.0.blocks.0.mlp.fc1.bias False\n",
      "model.backbone.layers.0.blocks.0.mlp.fc2.weight False\n",
      "model.backbone.layers.0.blocks.0.mlp.fc2.bias False\n",
      "model.backbone.layers.0.blocks.1.gamma_1 False\n",
      "model.backbone.layers.0.blocks.1.gamma_2 False\n",
      "model.backbone.layers.0.blocks.1.dw1.weight False\n",
      "model.backbone.layers.0.blocks.1.dw1.bias False\n",
      "model.backbone.layers.0.blocks.1.norm1.weight False\n",
      "model.backbone.layers.0.blocks.1.norm1.bias False\n",
      "model.backbone.layers.0.blocks.1.modulation.f.weight False\n",
      "model.backbone.layers.0.blocks.1.modulation.f.bias False\n",
      "model.backbone.layers.0.blocks.1.modulation.h.weight False\n",
      "model.backbone.layers.0.blocks.1.modulation.h.bias False\n",
      "model.backbone.layers.0.blocks.1.modulation.proj.weight False\n",
      "model.backbone.layers.0.blocks.1.modulation.proj.bias False\n",
      "model.backbone.layers.0.blocks.1.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.0.blocks.1.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.0.blocks.1.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.0.blocks.1.dw2.weight False\n",
      "model.backbone.layers.0.blocks.1.dw2.bias False\n",
      "model.backbone.layers.0.blocks.1.norm2.weight False\n",
      "model.backbone.layers.0.blocks.1.norm2.bias False\n",
      "model.backbone.layers.0.blocks.1.mlp.fc1.weight False\n",
      "model.backbone.layers.0.blocks.1.mlp.fc1.bias False\n",
      "model.backbone.layers.0.blocks.1.mlp.fc2.weight False\n",
      "model.backbone.layers.0.blocks.1.mlp.fc2.bias False\n",
      "model.backbone.layers.0.downsample.proj.weight False\n",
      "model.backbone.layers.0.downsample.proj.bias False\n",
      "model.backbone.layers.0.downsample.norm.weight False\n",
      "model.backbone.layers.0.downsample.norm.bias False\n",
      "model.backbone.layers.1.blocks.0.gamma_1 False\n",
      "model.backbone.layers.1.blocks.0.gamma_2 False\n",
      "model.backbone.layers.1.blocks.0.dw1.weight False\n",
      "model.backbone.layers.1.blocks.0.dw1.bias False\n",
      "model.backbone.layers.1.blocks.0.norm1.weight False\n",
      "model.backbone.layers.1.blocks.0.norm1.bias False\n",
      "model.backbone.layers.1.blocks.0.modulation.f.weight False\n",
      "model.backbone.layers.1.blocks.0.modulation.f.bias False\n",
      "model.backbone.layers.1.blocks.0.modulation.h.weight False\n",
      "model.backbone.layers.1.blocks.0.modulation.h.bias False\n",
      "model.backbone.layers.1.blocks.0.modulation.proj.weight False\n",
      "model.backbone.layers.1.blocks.0.modulation.proj.bias False\n",
      "model.backbone.layers.1.blocks.0.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.1.blocks.0.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.1.blocks.0.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.1.blocks.0.dw2.weight False\n",
      "model.backbone.layers.1.blocks.0.dw2.bias False\n",
      "model.backbone.layers.1.blocks.0.norm2.weight False\n",
      "model.backbone.layers.1.blocks.0.norm2.bias False\n",
      "model.backbone.layers.1.blocks.0.mlp.fc1.weight False\n",
      "model.backbone.layers.1.blocks.0.mlp.fc1.bias False\n",
      "model.backbone.layers.1.blocks.0.mlp.fc2.weight False\n",
      "model.backbone.layers.1.blocks.0.mlp.fc2.bias False\n",
      "model.backbone.layers.1.blocks.1.gamma_1 False\n",
      "model.backbone.layers.1.blocks.1.gamma_2 False\n",
      "model.backbone.layers.1.blocks.1.dw1.weight False\n",
      "model.backbone.layers.1.blocks.1.dw1.bias False\n",
      "model.backbone.layers.1.blocks.1.norm1.weight False\n",
      "model.backbone.layers.1.blocks.1.norm1.bias False\n",
      "model.backbone.layers.1.blocks.1.modulation.f.weight False\n",
      "model.backbone.layers.1.blocks.1.modulation.f.bias False\n",
      "model.backbone.layers.1.blocks.1.modulation.h.weight False\n",
      "model.backbone.layers.1.blocks.1.modulation.h.bias False\n",
      "model.backbone.layers.1.blocks.1.modulation.proj.weight False\n",
      "model.backbone.layers.1.blocks.1.modulation.proj.bias False\n",
      "model.backbone.layers.1.blocks.1.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.1.blocks.1.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.1.blocks.1.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.1.blocks.1.dw2.weight False\n",
      "model.backbone.layers.1.blocks.1.dw2.bias False\n",
      "model.backbone.layers.1.blocks.1.norm2.weight False\n",
      "model.backbone.layers.1.blocks.1.norm2.bias False\n",
      "model.backbone.layers.1.blocks.1.mlp.fc1.weight False\n",
      "model.backbone.layers.1.blocks.1.mlp.fc1.bias False\n",
      "model.backbone.layers.1.blocks.1.mlp.fc2.weight False\n",
      "model.backbone.layers.1.blocks.1.mlp.fc2.bias False\n",
      "model.backbone.layers.1.downsample.proj.weight False\n",
      "model.backbone.layers.1.downsample.proj.bias False\n",
      "model.backbone.layers.1.downsample.norm.weight False\n",
      "model.backbone.layers.1.downsample.norm.bias False\n",
      "model.backbone.layers.2.blocks.0.gamma_1 False\n",
      "model.backbone.layers.2.blocks.0.gamma_2 False\n",
      "model.backbone.layers.2.blocks.0.dw1.weight False\n",
      "model.backbone.layers.2.blocks.0.dw1.bias False\n",
      "model.backbone.layers.2.blocks.0.norm1.weight False\n",
      "model.backbone.layers.2.blocks.0.norm1.bias False\n",
      "model.backbone.layers.2.blocks.0.modulation.f.weight False\n",
      "model.backbone.layers.2.blocks.0.modulation.f.bias False\n",
      "model.backbone.layers.2.blocks.0.modulation.h.weight False\n",
      "model.backbone.layers.2.blocks.0.modulation.h.bias False\n",
      "model.backbone.layers.2.blocks.0.modulation.proj.weight False\n",
      "model.backbone.layers.2.blocks.0.modulation.proj.bias False\n",
      "model.backbone.layers.2.blocks.0.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.2.blocks.0.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.2.blocks.0.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.2.blocks.0.dw2.weight False\n",
      "model.backbone.layers.2.blocks.0.dw2.bias False\n",
      "model.backbone.layers.2.blocks.0.norm2.weight False\n",
      "model.backbone.layers.2.blocks.0.norm2.bias False\n",
      "model.backbone.layers.2.blocks.0.mlp.fc1.weight False\n",
      "model.backbone.layers.2.blocks.0.mlp.fc1.bias False\n",
      "model.backbone.layers.2.blocks.0.mlp.fc2.weight False\n",
      "model.backbone.layers.2.blocks.0.mlp.fc2.bias False\n",
      "model.backbone.layers.2.blocks.1.gamma_1 False\n",
      "model.backbone.layers.2.blocks.1.gamma_2 False\n",
      "model.backbone.layers.2.blocks.1.dw1.weight False\n",
      "model.backbone.layers.2.blocks.1.dw1.bias False\n",
      "model.backbone.layers.2.blocks.1.norm1.weight False\n",
      "model.backbone.layers.2.blocks.1.norm1.bias False\n",
      "model.backbone.layers.2.blocks.1.modulation.f.weight False\n",
      "model.backbone.layers.2.blocks.1.modulation.f.bias False\n",
      "model.backbone.layers.2.blocks.1.modulation.h.weight False\n",
      "model.backbone.layers.2.blocks.1.modulation.h.bias False\n",
      "model.backbone.layers.2.blocks.1.modulation.proj.weight False\n",
      "model.backbone.layers.2.blocks.1.modulation.proj.bias False\n",
      "model.backbone.layers.2.blocks.1.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.2.blocks.1.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.2.blocks.1.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.2.blocks.1.dw2.weight False\n",
      "model.backbone.layers.2.blocks.1.dw2.bias False\n",
      "model.backbone.layers.2.blocks.1.norm2.weight False\n",
      "model.backbone.layers.2.blocks.1.norm2.bias False\n",
      "model.backbone.layers.2.blocks.1.mlp.fc1.weight False\n",
      "model.backbone.layers.2.blocks.1.mlp.fc1.bias False\n",
      "model.backbone.layers.2.blocks.1.mlp.fc2.weight False\n",
      "model.backbone.layers.2.blocks.1.mlp.fc2.bias False\n",
      "model.backbone.layers.2.blocks.2.gamma_1 False\n",
      "model.backbone.layers.2.blocks.2.gamma_2 False\n",
      "model.backbone.layers.2.blocks.2.dw1.weight False\n",
      "model.backbone.layers.2.blocks.2.dw1.bias False\n",
      "model.backbone.layers.2.blocks.2.norm1.weight False\n",
      "model.backbone.layers.2.blocks.2.norm1.bias False\n",
      "model.backbone.layers.2.blocks.2.modulation.f.weight False\n",
      "model.backbone.layers.2.blocks.2.modulation.f.bias False\n",
      "model.backbone.layers.2.blocks.2.modulation.h.weight False\n",
      "model.backbone.layers.2.blocks.2.modulation.h.bias False\n",
      "model.backbone.layers.2.blocks.2.modulation.proj.weight False\n",
      "model.backbone.layers.2.blocks.2.modulation.proj.bias False\n",
      "model.backbone.layers.2.blocks.2.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.2.blocks.2.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.2.blocks.2.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.2.blocks.2.dw2.weight False\n",
      "model.backbone.layers.2.blocks.2.dw2.bias False\n",
      "model.backbone.layers.2.blocks.2.norm2.weight False\n",
      "model.backbone.layers.2.blocks.2.norm2.bias False\n",
      "model.backbone.layers.2.blocks.2.mlp.fc1.weight False\n",
      "model.backbone.layers.2.blocks.2.mlp.fc1.bias False\n",
      "model.backbone.layers.2.blocks.2.mlp.fc2.weight False\n",
      "model.backbone.layers.2.blocks.2.mlp.fc2.bias False\n",
      "model.backbone.layers.2.blocks.3.gamma_1 False\n",
      "model.backbone.layers.2.blocks.3.gamma_2 False\n",
      "model.backbone.layers.2.blocks.3.dw1.weight False\n",
      "model.backbone.layers.2.blocks.3.dw1.bias False\n",
      "model.backbone.layers.2.blocks.3.norm1.weight False\n",
      "model.backbone.layers.2.blocks.3.norm1.bias False\n",
      "model.backbone.layers.2.blocks.3.modulation.f.weight False\n",
      "model.backbone.layers.2.blocks.3.modulation.f.bias False\n",
      "model.backbone.layers.2.blocks.3.modulation.h.weight False\n",
      "model.backbone.layers.2.blocks.3.modulation.h.bias False\n",
      "model.backbone.layers.2.blocks.3.modulation.proj.weight False\n",
      "model.backbone.layers.2.blocks.3.modulation.proj.bias False\n",
      "model.backbone.layers.2.blocks.3.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.2.blocks.3.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.2.blocks.3.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.2.blocks.3.dw2.weight False\n",
      "model.backbone.layers.2.blocks.3.dw2.bias False\n",
      "model.backbone.layers.2.blocks.3.norm2.weight False\n",
      "model.backbone.layers.2.blocks.3.norm2.bias False\n",
      "model.backbone.layers.2.blocks.3.mlp.fc1.weight False\n",
      "model.backbone.layers.2.blocks.3.mlp.fc1.bias False\n",
      "model.backbone.layers.2.blocks.3.mlp.fc2.weight False\n",
      "model.backbone.layers.2.blocks.3.mlp.fc2.bias False\n",
      "model.backbone.layers.2.blocks.4.gamma_1 False\n",
      "model.backbone.layers.2.blocks.4.gamma_2 False\n",
      "model.backbone.layers.2.blocks.4.dw1.weight False\n",
      "model.backbone.layers.2.blocks.4.dw1.bias False\n",
      "model.backbone.layers.2.blocks.4.norm1.weight False\n",
      "model.backbone.layers.2.blocks.4.norm1.bias False\n",
      "model.backbone.layers.2.blocks.4.modulation.f.weight False\n",
      "model.backbone.layers.2.blocks.4.modulation.f.bias False\n",
      "model.backbone.layers.2.blocks.4.modulation.h.weight False\n",
      "model.backbone.layers.2.blocks.4.modulation.h.bias False\n",
      "model.backbone.layers.2.blocks.4.modulation.proj.weight False\n",
      "model.backbone.layers.2.blocks.4.modulation.proj.bias False\n",
      "model.backbone.layers.2.blocks.4.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.2.blocks.4.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.2.blocks.4.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.2.blocks.4.dw2.weight False\n",
      "model.backbone.layers.2.blocks.4.dw2.bias False\n",
      "model.backbone.layers.2.blocks.4.norm2.weight False\n",
      "model.backbone.layers.2.blocks.4.norm2.bias False\n",
      "model.backbone.layers.2.blocks.4.mlp.fc1.weight False\n",
      "model.backbone.layers.2.blocks.4.mlp.fc1.bias False\n",
      "model.backbone.layers.2.blocks.4.mlp.fc2.weight False\n",
      "model.backbone.layers.2.blocks.4.mlp.fc2.bias False\n",
      "model.backbone.layers.2.blocks.5.gamma_1 False\n",
      "model.backbone.layers.2.blocks.5.gamma_2 False\n",
      "model.backbone.layers.2.blocks.5.dw1.weight False\n",
      "model.backbone.layers.2.blocks.5.dw1.bias False\n",
      "model.backbone.layers.2.blocks.5.norm1.weight False\n",
      "model.backbone.layers.2.blocks.5.norm1.bias False\n",
      "model.backbone.layers.2.blocks.5.modulation.f.weight False\n",
      "model.backbone.layers.2.blocks.5.modulation.f.bias False\n",
      "model.backbone.layers.2.blocks.5.modulation.h.weight False\n",
      "model.backbone.layers.2.blocks.5.modulation.h.bias False\n",
      "model.backbone.layers.2.blocks.5.modulation.proj.weight False\n",
      "model.backbone.layers.2.blocks.5.modulation.proj.bias False\n",
      "model.backbone.layers.2.blocks.5.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.2.blocks.5.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.2.blocks.5.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.2.blocks.5.dw2.weight False\n",
      "model.backbone.layers.2.blocks.5.dw2.bias False\n",
      "model.backbone.layers.2.blocks.5.norm2.weight False\n",
      "model.backbone.layers.2.blocks.5.norm2.bias False\n",
      "model.backbone.layers.2.blocks.5.mlp.fc1.weight False\n",
      "model.backbone.layers.2.blocks.5.mlp.fc1.bias False\n",
      "model.backbone.layers.2.blocks.5.mlp.fc2.weight False\n",
      "model.backbone.layers.2.blocks.5.mlp.fc2.bias False\n",
      "model.backbone.layers.2.downsample.proj.weight False\n",
      "model.backbone.layers.2.downsample.proj.bias False\n",
      "model.backbone.layers.2.downsample.norm.weight False\n",
      "model.backbone.layers.2.downsample.norm.bias False\n",
      "model.backbone.layers.3.blocks.0.gamma_1 False\n",
      "model.backbone.layers.3.blocks.0.gamma_2 False\n",
      "model.backbone.layers.3.blocks.0.dw1.weight False\n",
      "model.backbone.layers.3.blocks.0.dw1.bias False\n",
      "model.backbone.layers.3.blocks.0.norm1.weight False\n",
      "model.backbone.layers.3.blocks.0.norm1.bias False\n",
      "model.backbone.layers.3.blocks.0.modulation.f.weight False\n",
      "model.backbone.layers.3.blocks.0.modulation.f.bias False\n",
      "model.backbone.layers.3.blocks.0.modulation.h.weight False\n",
      "model.backbone.layers.3.blocks.0.modulation.h.bias False\n",
      "model.backbone.layers.3.blocks.0.modulation.proj.weight False\n",
      "model.backbone.layers.3.blocks.0.modulation.proj.bias False\n",
      "model.backbone.layers.3.blocks.0.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.3.blocks.0.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.3.blocks.0.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.3.blocks.0.dw2.weight False\n",
      "model.backbone.layers.3.blocks.0.dw2.bias False\n",
      "model.backbone.layers.3.blocks.0.norm2.weight False\n",
      "model.backbone.layers.3.blocks.0.norm2.bias False\n",
      "model.backbone.layers.3.blocks.0.mlp.fc1.weight False\n",
      "model.backbone.layers.3.blocks.0.mlp.fc1.bias False\n",
      "model.backbone.layers.3.blocks.0.mlp.fc2.weight False\n",
      "model.backbone.layers.3.blocks.0.mlp.fc2.bias False\n",
      "model.backbone.layers.3.blocks.1.gamma_1 False\n",
      "model.backbone.layers.3.blocks.1.gamma_2 False\n",
      "model.backbone.layers.3.blocks.1.dw1.weight False\n",
      "model.backbone.layers.3.blocks.1.dw1.bias False\n",
      "model.backbone.layers.3.blocks.1.norm1.weight False\n",
      "model.backbone.layers.3.blocks.1.norm1.bias False\n",
      "model.backbone.layers.3.blocks.1.modulation.f.weight False\n",
      "model.backbone.layers.3.blocks.1.modulation.f.bias False\n",
      "model.backbone.layers.3.blocks.1.modulation.h.weight False\n",
      "model.backbone.layers.3.blocks.1.modulation.h.bias False\n",
      "model.backbone.layers.3.blocks.1.modulation.proj.weight False\n",
      "model.backbone.layers.3.blocks.1.modulation.proj.bias False\n",
      "model.backbone.layers.3.blocks.1.modulation.focal_layers.0.0.weight False\n",
      "model.backbone.layers.3.blocks.1.modulation.focal_layers.1.0.weight False\n",
      "model.backbone.layers.3.blocks.1.modulation.focal_layers.2.0.weight False\n",
      "model.backbone.layers.3.blocks.1.dw2.weight False\n",
      "model.backbone.layers.3.blocks.1.dw2.bias False\n",
      "model.backbone.layers.3.blocks.1.norm2.weight False\n",
      "model.backbone.layers.3.blocks.1.norm2.bias False\n",
      "model.backbone.layers.3.blocks.1.mlp.fc1.weight False\n",
      "model.backbone.layers.3.blocks.1.mlp.fc1.bias False\n",
      "model.backbone.layers.3.blocks.1.mlp.fc2.weight False\n",
      "model.backbone.layers.3.blocks.1.mlp.fc2.bias False\n",
      "model.backbone.norm0.weight False\n",
      "model.backbone.norm0.bias False\n",
      "model.backbone.norm1.weight False\n",
      "model.backbone.norm1.bias False\n",
      "model.backbone.norm2.weight False\n",
      "model.backbone.norm2.bias False\n",
      "model.backbone.norm3.weight False\n",
      "model.backbone.norm3.bias False\n",
      "model.hoid_head.encoder.adapter_1.weight False\n",
      "model.hoid_head.encoder.adapter_1.norm.weight False\n",
      "model.hoid_head.encoder.adapter_1.norm.bias False\n",
      "model.hoid_head.encoder.layer_1.weight False\n",
      "model.hoid_head.encoder.layer_1.norm.weight False\n",
      "model.hoid_head.encoder.layer_1.norm.bias False\n",
      "model.hoid_head.encoder.adapter_2.weight False\n",
      "model.hoid_head.encoder.adapter_2.norm.weight False\n",
      "model.hoid_head.encoder.adapter_2.norm.bias False\n",
      "model.hoid_head.encoder.layer_2.weight False\n",
      "model.hoid_head.encoder.layer_2.norm.weight False\n",
      "model.hoid_head.encoder.layer_2.norm.bias False\n",
      "model.hoid_head.encoder.adapter_3.weight False\n",
      "model.hoid_head.encoder.adapter_3.norm.weight False\n",
      "model.hoid_head.encoder.adapter_3.norm.bias False\n",
      "model.hoid_head.encoder.layer_3.weight False\n",
      "model.hoid_head.encoder.layer_3.norm.weight False\n",
      "model.hoid_head.encoder.layer_3.norm.bias False\n",
      "model.hoid_head.encoder.mask_features.weight False\n",
      "model.hoid_head.encoder.mask_features.bias False\n",
      "model.hoid_head.encoder.input_proj.weight False\n",
      "model.hoid_head.encoder.input_proj.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.linear1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.linear1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.linear2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.linear2.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.norm1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.norm1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.norm2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.0.norm2.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.linear1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.linear1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.linear2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.linear2.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.norm1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.norm1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.norm2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.1.norm2.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.linear1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.linear1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.linear2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.linear2.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.norm1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.norm1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.norm2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.2.norm2.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.linear1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.linear1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.linear2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.linear2.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.norm1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.norm1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.norm2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.3.norm2.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.linear1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.linear1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.linear2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.linear2.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.norm1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.norm1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.norm2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.4.norm2.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.linear1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.linear1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.linear2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.linear2.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.norm1.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.norm1.bias False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.norm2.weight False\n",
      "model.hoid_head.encoder.transformer.encoder.layers.5.norm2.bias False\n",
      "model.hoid_head.encoder.layer_4.weight False\n",
      "model.hoid_head.encoder.layer_4.norm.weight False\n",
      "model.hoid_head.encoder.layer_4.norm.bias False\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.linear1.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.linear1.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.linear2.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.linear2.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.norm1.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.norm1.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.norm2.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.norm2.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.norm3.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.0.norm3.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.linear1.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.linear1.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.linear2.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.linear2.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.norm1.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.norm1.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.norm2.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.norm2.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.norm3.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.1.norm3.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.linear1.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.linear1.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.linear2.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.linear2.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.norm1.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.norm1.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.norm2.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.norm2.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.norm3.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.layers.2.norm3.bias True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.norm.weight True\n",
      "model.hoid_head.hoi_decoder.hopd_decoder.norm.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.linear1.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.linear1.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.linear2.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.linear2.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.norm1.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.norm1.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.norm2.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.norm2.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.norm3.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.0.norm3.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.linear1.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.linear1.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.linear2.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.linear2.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.norm1.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.norm1.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.norm2.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.norm2.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.norm3.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.1.norm3.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.in_proj_weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.in_proj_bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.out_proj.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.out_proj.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.linear1.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.linear1.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.linear2.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.linear2.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.norm1.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.norm1.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.norm2.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.norm2.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.norm3.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.layers.2.norm3.bias True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.norm.weight True\n",
      "model.hoid_head.hoi_decoder.interaction_decoder.norm.bias True\n",
      "model.hoid_head.query_embed.weight True\n",
      "model.hoid_head.obj_class_embed.weight True\n",
      "model.hoid_head.obj_class_embed.bias True\n",
      "model.hoid_head.verb_class_embed.weight True\n",
      "model.hoid_head.verb_class_embed.bias True\n",
      "model.hoid_head.sub_bbox_embed.layers.0.weight True\n",
      "model.hoid_head.sub_bbox_embed.layers.0.bias True\n",
      "model.hoid_head.sub_bbox_embed.layers.1.weight True\n",
      "model.hoid_head.sub_bbox_embed.layers.1.bias True\n",
      "model.hoid_head.sub_bbox_embed.layers.2.weight True\n",
      "model.hoid_head.sub_bbox_embed.layers.2.bias True\n",
      "model.hoid_head.obj_bbox_embed.layers.0.weight True\n",
      "model.hoid_head.obj_bbox_embed.layers.0.bias True\n",
      "model.hoid_head.obj_bbox_embed.layers.1.weight True\n",
      "model.hoid_head.obj_bbox_embed.layers.1.bias True\n",
      "model.hoid_head.obj_bbox_embed.layers.2.weight True\n",
      "model.hoid_head.obj_bbox_embed.layers.2.bias True\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from trainer import HDecoder_Trainer as Trainer\n",
    "trainer = Trainer(opt)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_Decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
