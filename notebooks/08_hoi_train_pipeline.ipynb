{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjin/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  1.13 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures import Boxes, ImageList, Instances, BitMasks, BoxMode\n",
    "\n",
    "from hdecoder.BaseModel import BaseModel\n",
    "from hdecoder import build_model\n",
    "\n",
    "from utils.arguments import load_opt_command\n",
    "from utils.misc import hook_metadata, hook_switcher, hook_opt\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "# cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/svlp_focalt_lang.yaml\")]\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/vcoco.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', '../checkpoints/xdecoder_focalt_best_openseg.pt'] \n",
    "cmdline_args.overrides\n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "config_dict\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel(opt, build_model(opt)).train().cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piplene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "# base_name = 'base_dir'\n",
    "# base_path =  os.path.join(opt['base_path'], '__init__.py')\n",
    "# spec = importlib.util.spec_from_file_location(base_name, base_path)\n",
    "# module = importlib.util.module_from_spec(spec)\n",
    "# sys.modules[base_name] = module\n",
    "# spec.loader.exec_module(module)\n",
    "# logger.info(f\"Imported {base_name} at base_path {opt['base_path']}\")\n",
    "\n",
    "# pipeline_module = importlib.import_module(f\"base_dir.pipeline.{opt['PIPELINE']}\")\n",
    "# pipeline_class = getattr(pipeline_module, opt['PIPELINE'])\n",
    "# pipeline = pipeline_class(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PIPELINE': 'HDecoderPipeline',\n",
       " 'TRAINER': 'hdecoder',\n",
       " 'SAVE_DIR': '../../data/output/test',\n",
       " 'base_path': '../',\n",
       " 'RESUME': False,\n",
       " 'WEIGHT': True,\n",
       " 'RESET_DATA_LOADER': False,\n",
       " 'RESUME_FROM': '',\n",
       " 'PYLEARN_MODEL': '',\n",
       " 'DONT_LOAD_MODEL': True,\n",
       " 'LOG_EVERY': 500,\n",
       " 'FIND_UNUSED_PARAMETERS': False,\n",
       " 'FP16': False,\n",
       " 'PORT': '36873',\n",
       " 'LOADER': {'JOINT': True, 'KEY_DATASET': 'coco'},\n",
       " 'VERBOSE': True,\n",
       " 'MODEL': {'NAME': 'hoi_model',\n",
       "  'HEAD': 'hoi_head',\n",
       "  'MASK_ON': False,\n",
       "  'KEYPOINT_ON': False,\n",
       "  'LOAD_PROPOSALS': False,\n",
       "  'DIM_PROJ': 512,\n",
       "  'BACKBONE_DIM': 768,\n",
       "  'BACKBONE': {'NAME': 'focal_dw',\n",
       "   'PRETRAINED': '',\n",
       "   'LOAD_PRETRAINED': False,\n",
       "   'FOCAL': {'PRETRAIN_IMG_SIZE': 224,\n",
       "    'PATCH_SIZE': 4,\n",
       "    'EMBED_DIM': 96,\n",
       "    'DEPTHS': [2, 2, 6, 2],\n",
       "    'FOCAL_LEVELS': [3, 3, 3, 3],\n",
       "    'FOCAL_WINDOWS': [3, 3, 3, 3],\n",
       "    'DROP_PATH_RATE': 0.3,\n",
       "    'MLP_RATIO': 4.0,\n",
       "    'DROP_RATE': 0.0,\n",
       "    'PATCH_NORM': True,\n",
       "    'USE_CONV_EMBED': True,\n",
       "    'SCALING_MODULATOR': True,\n",
       "    'USE_CHECKPOINT': False,\n",
       "    'USE_POSTLN': True,\n",
       "    'USE_POSTLN_IN_MODULATION': False,\n",
       "    'USE_LAYERSCALE': True,\n",
       "    'OUT_FEATURES': ['res2', 'res3', 'res4', 'res5'],\n",
       "    'OUT_INDICES': [0, 1, 2, 3]}},\n",
       "  'ENCODER': {'NAME': 'transformer_encoder_hoi',\n",
       "   'IGNORE_VALUE': 255,\n",
       "   'NUM_CLASSES': 133,\n",
       "   'LOSS_WEIGHT': 1.0,\n",
       "   'CONVS_DIM': 512,\n",
       "   'MASK_DIM': 512,\n",
       "   'NORM': 'GN',\n",
       "   'IN_FEATURES': ['res2', 'res3', 'res4', 'res5'],\n",
       "   'DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES': ['res3', 'res4', 'res5'],\n",
       "   'COMMON_STRIDE': 4,\n",
       "   'TRANSFORMER_ENC_LAYERS': 6},\n",
       "  'DECODER': {'NAME': 'hdecoder',\n",
       "   'MASK': False,\n",
       "   'HIDDEN_DIM': 512,\n",
       "   'NUM_OBJECT_QUERIES': 101,\n",
       "   'NUM_OBJECT_CLASSES': 80,\n",
       "   'NUM_VERB_CLASSES': 29,\n",
       "   'NHEADS': 8,\n",
       "   'DROPOUT': 0.1,\n",
       "   'DIM_FEEDFORWARD': 2048,\n",
       "   'PRE_NORM': False,\n",
       "   'HOPD_DEC_LAYERS': 3,\n",
       "   'INTERACTION_DEC_LAYERS': 3,\n",
       "   'RETURN_INTERMEDIATE_DEC': True,\n",
       "   'COST_OBJECT_CLASS': 1,\n",
       "   'COST_VERB_CLASS': 1,\n",
       "   'COST_BBOX': 1,\n",
       "   'COST_GIOU': 1,\n",
       "   'COST_MATCHING': 1,\n",
       "   'OBJ_LOSS_COEF': 1,\n",
       "   'VERB_LOSS_COEF': 2,\n",
       "   'BBOX_LOSS_COEF': 2.5,\n",
       "   'GIOU_LOSS_COEF': 1,\n",
       "   'MATCHING_LOSS_COEF': 1,\n",
       "   'EOS_COEF': 0.1}},\n",
       " 'INPUT': {'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
       "  'PIXEL_STD': [58.395, 57.12, 57.375]},\n",
       " 'DATASETS': {'TRAIN': ['vcoco_train'],\n",
       "  'TEST': ['vcoco_val'],\n",
       "  'CLASS_CONCAT': False,\n",
       "  'SIZE_DIVISIBILITY': 32,\n",
       "  'PROPOSAL_FILES_TRAIN': []},\n",
       " 'DATALOADER': {'FILTER_EMPTY_ANNOTATIONS': False,\n",
       "  'NUM_WORKERS': 16,\n",
       "  'LOAD_PROPOSALS': False,\n",
       "  'SAMPLER_TRAIN': 'TrainingSampler',\n",
       "  'ASPECT_RATIO_GROUPING': True},\n",
       " 'SOLVER': {'BASE_LR': 0.0001,\n",
       "  'STEPS': [0.88889, 0.96296],\n",
       "  'MAX_ITER': 1,\n",
       "  'GAMMA': 0.1,\n",
       "  'WARMUP_FACTOR': 1.0,\n",
       "  'WARMUP_ITERS': 10,\n",
       "  'WARMUP_METHOD': 'linear',\n",
       "  'WEIGHT_DECAY': 0.05,\n",
       "  'OPTIMIZER': 'ADAMW',\n",
       "  'LR_SCHEDULER_NAME': 'WarmupMultiStepLR',\n",
       "  'LR_MULTIPLIER': {'backbone': 0.1, 'lang_encoder': 0.1},\n",
       "  'WEIGHT_DECAY_NORM': 0.0,\n",
       "  'WEIGHT_DECAY_EMBED': 0.0,\n",
       "  'CLIP_GRADIENTS': {'ENABLED': True,\n",
       "   'CLIP_TYPE': 'full_model',\n",
       "   'CLIP_VALUE': 5.0,\n",
       "   'NORM_TYPE': 2.0},\n",
       "  'AMP': {'ENABLED': True},\n",
       "  'MAX_NUM_EPOCHS': 50},\n",
       " 'VCOCO': {'INPUT': {'MIN_SIZE_TEST': 512,\n",
       "   'MAX_SIZE_TEST': 1024,\n",
       "   'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
       "   'PIXEL_STD': [58.395, 57.12, 57.375],\n",
       "   'NUM_QUERIES': 100,\n",
       "   'DATASET_MAPPER_NAME': 'vcoco'},\n",
       "  'DATALOADER': {'FILTER_EMPTY_ANNOTATIONS': False,\n",
       "   'NUM_WORKERS': 0,\n",
       "   'LOAD_PROPOSALS': False,\n",
       "   'SAMPLER_TRAIN': 'TrainingSampler',\n",
       "   'ASPECT_RATIO_GROUPING': False},\n",
       "  'TEST': {'BATCH_SIZE_TOTAL': 2},\n",
       "  'TRAIN': {'BATCH_SIZE_TOTAL': 2, 'BATCH_SIZE_PER_GPU': 2}},\n",
       " 'command': 'evaluate',\n",
       " 'conf_files': ['/home/djjin/Mygit/X-Decoder/configs/xdecoder/vcoco.yaml'],\n",
       " 'overrides': ['WEIGHT', '../checkpoints/xdecoder_focalt_best_openseg.pt'],\n",
       " 'CUDA': True,\n",
       " 'env_info': 'no MPI',\n",
       " 'world_size': 1,\n",
       " 'local_size': 1,\n",
       " 'rank': 0,\n",
       " 'local_rank': 0,\n",
       " 'master_address': '127.0.0.1',\n",
       " 'master_port': '8673',\n",
       " 'device': device(type='cuda', index=0)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "CUDA_VISIBLE_DEVICES=0 python entry.py train \\\n",
    "            --conf_files configs/xdecoder/segvlp_focalt_lang.yaml \\\n",
    "            --overrides \\\n",
    "            COCO.INPUT.IMAGE_SIZE 1024 \\\n",
    "            MODEL.DECODER.CAPTIONING.ENABLED True \\\n",
    "            MODEL.DECODER.RETRIEVAL.ENABLED True \\\n",
    "            MODEL.DECODER.GROUNDING.ENABLED True \\\n",
    "            MODEL.DECODER.CAPTIONING_WEIGHT 8 \\\n",
    "            MODEL.DECODER.RETRIEVAL_WEIGHT 8 \\\n",
    "            MODEL.DECODER.TOP_CAPTIONING_LAYERS 3 \\\n",
    "            MODEL.DECODER.TOP_RETRIEVAL_LAYERS 3 \\\n",
    "            MODEL.DECODER.TOP_GROUNDING_LAYERS 6 \\\n",
    "            COCO.TEST.BATCH_SIZE_TOTAL 1 \\\n",
    "            COCO.TRAIN.BATCH_SIZE_TOTAL 1 \\\n",
    "            COCO.TRAIN.BATCH_SIZE_PER_GPU 1 \\\n",
    "            VLP.TEST.BATCH_SIZE_TOTAL 32 \\\n",
    "            VLP.TRAIN.BATCH_SIZE_TOTAL 32 \\\n",
    "            VLP.TRAIN.BATCH_SIZE_PER_GPU 32 \\\n",
    "            MODEL.DECODER.HIDDEN_DIM 512 \\\n",
    "            MODEL.ENCODER.CONVS_DIM 512 \\\n",
    "            MODEL.ENCODER.MASK_DIM 512 \\\n",
    "            FP16 True \\\n",
    "            WEIGHT True \\\n",
    "            RESUME_FROM \\pth\\to\\focalt_unicl_pretrain.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "MPI Adapter data\n",
      "----------------\n",
      "environment info: no MPI\n",
      "init method url: tcp://127.0.0.1:36873\n",
      "world size: 1\n",
      "local size: 1\n",
      "rank: 0\n",
      "local rank: 0\n",
      "master address: 127.0.0.1\n",
      "master port: 36873\n",
      "----------------\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/djjin/Mygit/X-Decoder/notebooks/08_hoi_train_pipeline.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/08_hoi_train_pipeline.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m HDecoder_Trainer \u001b[39mas\u001b[39;00m Trainer\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/08_hoi_train_pipeline.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(opt)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/08_hoi_train_pipeline.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/trainer/default_trainer.py:224\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    221\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m    Training\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_train()\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/trainer/default_trainer.py:203\u001b[0m, in \u001b[0;36mDefaultTrainer.init_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_ddp()\n\u001b[1;32m    202\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mWEIGHT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_weight(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt[\u001b[39m'\u001b[39;49m\u001b[39mRESUME_FROM\u001b[39;49m\u001b[39m'\u001b[39;49m], must_exist\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mRESUME\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_checkpoint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt[\u001b[39m'\u001b[39m\u001b[39mRESUME_FROM\u001b[39m\u001b[39m'\u001b[39m], must_exist\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/trainer/utils_trainer.py:159\u001b[0m, in \u001b[0;36mUtilsTrainer.load_weight\u001b[0;34m(self, checkpoint_path, must_exist)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_weight\u001b[39m(\u001b[39mself\u001b[39m, checkpoint_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, must_exist\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_model(checkpoint_path)\n\u001b[1;32m    160\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoad weights from \u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_path\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/trainer/utils_trainer.py:73\u001b[0m, in \u001b[0;36mUtilsTrainer.load_model\u001b[0;34m(self, load_path)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(\u001b[39mself\u001b[39m, load_path):\n\u001b[1;32m     72\u001b[0m     \u001b[39mfor\u001b[39;00m module_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_names:\n\u001b[0;32m---> 73\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_models[module_name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_models[module_name]\u001b[39m.\u001b[39;49mfrom_pretrained(load_path)\n\u001b[1;32m     74\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_models[module_name]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/BaseModel.py:26\u001b[0m, in \u001b[0;36mBaseModel.from_pretrained\u001b[0;34m(self, load_dir)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mself\u001b[39m, load_dir):\n\u001b[0;32m---> 26\u001b[0m     state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(load_dir, map_location\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt[\u001b[39m\"\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     27\u001b[0m     state_dict \u001b[39m=\u001b[39m align_and_update_state_dicts(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstate_dict(), state_dict)\n\u001b[1;32m     28\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(state_dict, strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "from trainer import HDecoder_Trainer as Trainer\n",
    "trainer = Trainer(opt)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_Decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
