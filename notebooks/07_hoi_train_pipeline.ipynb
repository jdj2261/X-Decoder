{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjin/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  1.13 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "os.environ[\"DATASET\"] = \"../datasets\"\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures import Boxes, ImageList, Instances, BitMasks, BoxMode\n",
    "\n",
    "from hdecoder.BaseModel import BaseModel\n",
    "from hdecoder import build_model\n",
    "\n",
    "from utils.arguments import load_opt_command\n",
    "from utils.misc import hook_metadata, hook_switcher, hook_opt\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "# cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/svlp_focalt_lang.yaml\")]\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/hdecoder/vcoco.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', 'true', 'RESUME_FROM', '../checkpoints/xdecoder_focalt_last.pt'] \n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "opt[\"base_path\"] = \"../\"\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backbone': False, 'encoder': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt[\"WANDB\"] = False\n",
    "fix_param = opt['SOLVER'].get('FIX_PARAM',{})\n",
    "fix_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt[\"FP16\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trainer.distributed_trainer:Setting SAVE_DIR as data/output/test\n",
      "INFO:trainer.distributed_trainer:Using CUDA\n",
      "WARNING:trainer.utils.mpi_adapter:----------------\n",
      "WARNING:trainer.utils.mpi_adapter:MPI Adapter data\n",
      "WARNING:trainer.utils.mpi_adapter:----------------\n",
      "WARNING:trainer.utils.mpi_adapter:environment info: no MPI\n",
      "WARNING:trainer.utils.mpi_adapter:init method url: tcp://127.0.0.1:2222\n",
      "WARNING:trainer.utils.mpi_adapter:world size: 1\n",
      "WARNING:trainer.utils.mpi_adapter:local size: 1\n",
      "WARNING:trainer.utils.mpi_adapter:rank: 0\n",
      "WARNING:trainer.utils.mpi_adapter:local rank: 0\n",
      "WARNING:trainer.utils.mpi_adapter:master address: 127.0.0.1\n",
      "WARNING:trainer.utils.mpi_adapter:master port: 2222\n",
      "WARNING:trainer.utils.mpi_adapter:----------------\n",
      "INFO:trainer.distributed_trainer:Save config file to data/output/test/conf_copy.yaml\n",
      "INFO:trainer.distributed_trainer:Base learning rate: 0.0001\n",
      "INFO:trainer.distributed_trainer:Number of GPUs: 1\n",
      "INFO:trainer.distributed_trainer:Gradient accumulation steps: 1\n",
      "INFO:trainer.default_trainer:Imported base_dir at base_path ../\n",
      "INFO:trainer.default_trainer:Pipeline for training: HDecoderPipeline\n",
      "INFO:trainer.default_trainer:-------------------------------------------------------\n",
      "INFO:trainer.default_trainer:Training on rank: 0\n",
      "INFO:base_dir.pipeline.HDecoderPipeline:CDNHOI(\n",
      "  (backbone): D2FocalNet(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=96, out_features=196, bias=True)\n",
      "              (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=96, out_features=196, bias=True)\n",
      "              (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
      "            (drop_path): DropPath(drop_prob=0.027)\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchEmbed(\n",
      "          (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=192, out_features=388, bias=True)\n",
      "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (drop_path): DropPath(drop_prob=0.055)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=192, out_features=388, bias=True)\n",
      "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (drop_path): DropPath(drop_prob=0.082)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchEmbed(\n",
      "          (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.109)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.136)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.164)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.191)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.218)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): FocalModulationBlock(\n",
      "            (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=384, out_features=772, bias=True)\n",
      "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (drop_path): DropPath(drop_prob=0.245)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchEmbed(\n",
      "          (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): FocalModulationBlock(\n",
      "            (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=768, out_features=1540, bias=True)\n",
      "              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (drop_path): DropPath(drop_prob=0.273)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): FocalModulationBlock(\n",
      "            (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (modulation): FocalModulation(\n",
      "              (f): Linear(in_features=768, out_features=1540, bias=True)\n",
      "              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (act): GELU(approximate='none')\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (focal_layers): ModuleList(\n",
      "                (0): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (1): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "                (2): Sequential(\n",
      "                  (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
      "                  (1): GELU(approximate='none')\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
      "            (drop_path): DropPath(drop_prob=0.300)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (hoid_head): CDN(\n",
      "    (encoder): TransformerEncoderHOI(\n",
      "      (adapter_1): Conv2d(\n",
      "        96, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (adapter_2): Conv2d(\n",
      "        192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_2): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (adapter_3): Conv2d(\n",
      "        384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_3): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (input_proj): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (transformer): TransformerEncoderOnly(\n",
      "        (encoder): TransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (3): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (4): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (5): TransformerEncoderLayer(\n",
      "              (self_attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 256\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "    )\n",
      "    (hoi_decoder): HDecoder(\n",
      "      (hopd_decoder): TransformerDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (interaction_decoder): TransformerDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): TransformerDecoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (multihead_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (dropout3): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (query_embed): Embedding(100, 512)\n",
      "    (obj_class_embed): Linear(in_features=512, out_features=82, bias=True)\n",
      "    (verb_class_embed): Linear(in_features=512, out_features=29, bias=True)\n",
      "    (sub_bbox_embed): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (2): Linear(in_features=512, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (obj_bbox_embed): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (2): Linear(in_features=512, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): SetCriterionHOI(\n",
      "    (matcher): HungarianMatcherHOI()\n",
      "  )\n",
      "  (postprocessors): PostProcessHOI()\n",
      ")\n",
      "INFO:datasets.build:Using training sampler TrainingSampler\n",
      "INFO:detectron2.data.common:Serializing 5400 elements to byte tensors and concatenating them all ...\n",
      "INFO:detectron2.data.common:Serialized dataset takes 4.09 MiB\n",
      "INFO:base_dir.pipeline.HDecoderPipeline:num of train samples: 2700\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.patch_embed.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.patch_embed.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.patch_embed.norm.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.patch_embed.norm.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.0.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.blocks.1.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.downsample.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.downsample.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.downsample.norm.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.0.downsample.norm.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.0.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.blocks.1.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.downsample.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.downsample.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.downsample.norm.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.1.downsample.norm.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.0.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.1.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.2.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.3.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.4.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.blocks.5.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.downsample.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.downsample.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.downsample.norm.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.2.downsample.norm.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.0.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.gamma_1: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.gamma_2: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.dw1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.dw1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.modulation.f.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.modulation.f.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.modulation.h.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.modulation.h.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.modulation.proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.modulation.proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.modulation.focal_layers.0.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.modulation.focal_layers.1.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.modulation.focal_layers.2.0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.dw2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.dw2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.mlp.fc1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.mlp.fc1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.mlp.fc2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.layers.3.blocks.1.mlp.fc2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.norm0.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.norm0.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.norm3.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.backbone.norm3.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.adapter_1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.adapter_1.norm.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.adapter_1.norm.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.layer_1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.layer_1.norm.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.layer_1.norm.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.adapter_2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.adapter_2.norm.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.adapter_2.norm.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.layer_2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.layer_2.norm.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.layer_2.norm.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.adapter_3.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.adapter_3.norm.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.adapter_3.norm.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.layer_3.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.layer_3.norm.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.layer_3.norm.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.mask_features.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.mask_features.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.input_proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.input_proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.linear1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.linear1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.linear2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.linear2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.0.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.linear1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.linear1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.linear2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.linear2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.1.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.linear1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.linear1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.linear2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.linear2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.2.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.linear1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.linear1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.linear2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.linear2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.3.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.linear1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.linear1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.linear2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.linear2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.4.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.linear1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.linear1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.linear2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.linear2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.norm1.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.norm1.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.norm2.weight: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Modify Learning rate of model.hoid_head.encoder.transformer.encoder.layers.5.norm2.bias: 0.1\n",
      "INFO:trainer.hdecoder_trainer:Calculate MAX_ITER @ 24300000 and STEPS @ [21600027, 23399928]\n",
      "INFO:trainer.hdecoder_trainer:Total number of parameters in default module (on each GPU): 85407335\n",
      "INFO:trainer.hdecoder_trainer:Number of trainable parameters in default module (on each GPU): 85407335\n",
      "WARNING:trainer.utils_trainer:PyTorch AMP GradScaler initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train samples: 2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw1.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw2.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_1, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_2, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.bias, Model Shape: torch.Size([196]) <-> Ckpt Shape: torch.Size([196])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.weight, Model Shape: torch.Size([196, 96]) <-> Ckpt Shape: torch.Size([196, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([96, 1, 5, 5]) <-> Ckpt Shape: torch.Size([96, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([96, 1, 7, 7]) <-> Ckpt Shape: torch.Size([96, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.weight, Model Shape: torch.Size([96, 96, 1, 1]) <-> Ckpt Shape: torch.Size([96, 96, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw1.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw2.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_1, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_2, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.bias, Model Shape: torch.Size([196]) <-> Ckpt Shape: torch.Size([196])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.weight, Model Shape: torch.Size([196, 96]) <-> Ckpt Shape: torch.Size([196, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([96, 1, 5, 5]) <-> Ckpt Shape: torch.Size([96, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([96, 1, 7, 7]) <-> Ckpt Shape: torch.Size([96, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.weight, Model Shape: torch.Size([96, 96, 1, 1]) <-> Ckpt Shape: torch.Size([96, 96, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.weight, Model Shape: torch.Size([192, 96, 3, 3]) <-> Ckpt Shape: torch.Size([192, 96, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw1.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw2.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.bias, Model Shape: torch.Size([388]) <-> Ckpt Shape: torch.Size([388])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.weight, Model Shape: torch.Size([388, 192]) <-> Ckpt Shape: torch.Size([388, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw1.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw2.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.bias, Model Shape: torch.Size([388]) <-> Ckpt Shape: torch.Size([388])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.weight, Model Shape: torch.Size([388, 192]) <-> Ckpt Shape: torch.Size([388, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.weight, Model Shape: torch.Size([384, 192, 3, 3]) <-> Ckpt Shape: torch.Size([384, 192, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.weight, Model Shape: torch.Size([768, 384, 3, 3]) <-> Ckpt Shape: torch.Size([768, 384, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw1.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw2.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.bias, Model Shape: torch.Size([1540]) <-> Ckpt Shape: torch.Size([1540])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.weight, Model Shape: torch.Size([1540, 768]) <-> Ckpt Shape: torch.Size([1540, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw1.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw2.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.bias, Model Shape: torch.Size([1540]) <-> Ckpt Shape: torch.Size([1540])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.weight, Model Shape: torch.Size([1540, 768]) <-> Ckpt Shape: torch.Size([1540, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.norm0.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.norm0.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.norm3.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.norm3.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.norm.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.norm.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.proj.weight, Model Shape: torch.Size([96, 3, 7, 7]) <-> Ckpt Shape: torch.Size([96, 3, 7, 7])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.weight, Model Shape: torch.Size([512, 96, 1, 1]) <-> Ckpt Shape: torch.Size([512, 96, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.weight, Model Shape: torch.Size([512, 192, 1, 1]) <-> Ckpt Shape: torch.Size([512, 192, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.weight, Model Shape: torch.Size([512, 384, 1, 1]) <-> Ckpt Shape: torch.Size([512, 384, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.input_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.input_proj.weight, Model Shape: torch.Size([512, 768, 1, 1]) <-> Ckpt Shape: torch.Size([512, 768, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.mask_features.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.mask_features.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.norm.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.norm.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.norm.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.norm.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.0.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.0.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.1.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.2.bias, Model Shape: torch.Size([4])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.2.weight, Model Shape: torch.Size([4, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_class_embed.bias, Model Shape: torch.Size([82])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_class_embed.weight, Model Shape: torch.Size([82, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.query_embed.weight, Model Shape: torch.Size([100, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.0.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.0.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.1.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.2.bias, Model Shape: torch.Size([4])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.2.weight, Model Shape: torch.Size([4, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.verb_class_embed.bias, Model Shape: torch.Size([29])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.verb_class_embed.weight, Model Shape: torch.Size([29, 512])\n",
      "WARNING:utils.model:$UNUSED$ backbone_proj, Ckpt Shape: torch.Size([768, 512])\n",
      "WARNING:utils.model:$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.pixel_decoder.layer_4.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.pixel_decoder.layer_4.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.pixel_decoder.layer_4.weight, Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.caping_embed, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.class_embed, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.decoder_norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.decoder_norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding, Ckpt Shape: torch.Size([77, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.token_embedding.weight, Ckpt Shape: torch.Size([49408, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_proj, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.logit_scale, Ckpt Shape: torch.Size([])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.level_embed.weight, Ckpt Shape: torch.Size([3, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.0.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.0.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.1.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.2.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.query_embed.weight, Ckpt Shape: torch.Size([101, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.query_feat.weight, Ckpt Shape: torch.Size([101, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.query_feat_caping.weight, Ckpt Shape: torch.Size([77, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.self_attn_mask, Ckpt Shape: torch.Size([1, 178, 178])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNMATCHED* criterion.empty_weight, Model Shape: torch.Size([82]) <-> Ckpt Shape: torch.Size([134])\n",
      "WARNING:trainer.utils_trainer:Load weights from ../checkpoints/xdecoder_focalt_last.pt...\n",
      "INFO:trainer.default_trainer:***** Running training *****\n",
      "INFO:trainer.default_trainer:  Num of GPUs = 1\n",
      "INFO:trainer.default_trainer:  Num Epochs = 90\n",
      "INFO:trainer.default_trainer:  Num of Mini Batches per Epoch = 2700\n",
      "INFO:trainer.default_trainer:  Total train batch size (w. parallel, distributed & accumulation) = 243000\n",
      "INFO:trainer.default_trainer:  Gradient Accumulation steps = 1\n",
      "INFO:trainer.default_trainer:  Total optimization steps = 243000\n",
      "INFO:trainer.default_trainer:Start epoch: 0 training.\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[1/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 418.43018/418.43018, loss_obj_ce: 4.95622/4.95622, obj_class_error: 100.00000/100.00000, loss_verb_ce: 60.72153/60.72153, loss_sub_bbox: 1.00187/1.00187, loss_obj_bbox: 1.25111/1.25111, loss_sub_giou: 0.97965/0.97965, loss_obj_giou: 1.21688/1.21688, obj_cardinality_error: 97.00000/97.00000, loss_obj_ce_0: 4.87503/4.87503, loss_verb_ce_0: 67.61061/67.61061, loss_sub_bbox_0: 0.99874/0.99874, loss_obj_bbox_0: 1.23676/1.23676, loss_sub_giou_0: 0.99031/0.99031, loss_obj_giou_0: 1.22460/1.22460, obj_cardinality_error_0: 97.00000/97.00000, loss_obj_ce_1: 4.95480/4.95480, loss_verb_ce_1: 61.76704/61.76704, loss_sub_bbox_1: 0.99886/0.99886, loss_obj_bbox_1: 1.24725/1.24725, loss_sub_giou_1: 0.98543/0.98543, loss_obj_giou_1: 1.21241/1.21241, obj_cardinality_error_1: 97.00000/97.00000] items per batch[1] items per second[0.62] total items[1] mini batches[     1] memory[5290] epoch remaining[1:12:25]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[2/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 344.08646/381.25832, loss_obj_ce: 4.84358/4.89990, obj_class_error: 100.00000/100.00000, loss_verb_ce: 52.08464/56.40309, loss_sub_bbox: 0.64620/0.82404, loss_obj_bbox: 0.78974/1.02043, loss_sub_giou: 0.72423/0.85194, loss_obj_giou: 0.99671/1.10680, obj_cardinality_error: 97.00000/97.00000, loss_obj_ce_0: 4.71998/4.79751, loss_verb_ce_0: 54.20091/60.90576, loss_sub_bbox_0: 0.66943/0.83409, loss_obj_bbox_0: 0.80183/1.01929, loss_sub_giou_0: 0.73643/0.86337, loss_obj_giou_0: 0.99954/1.11207, obj_cardinality_error_0: 97.00000/97.00000, loss_obj_ce_1: 4.81104/4.88292, loss_verb_ce_1: 50.52295/56.14499, loss_sub_bbox_1: 0.65474/0.82680, loss_obj_bbox_1: 0.80030/1.02378, loss_sub_giou_1: 0.73219/0.85881, loss_obj_giou_1: 1.00008/1.10624, obj_cardinality_error_1: 97.00000/97.00000] items per batch[1] items per second[4.00] total items[2] mini batches[     2] memory[5594] epoch remaining[0:41:49]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[3/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 569.46863/443.99509, loss_obj_ce: 5.02911/4.94297, obj_class_error: 100.00000/100.00000, loss_verb_ce: 85.99493/66.26704, loss_sub_bbox: 1.35380/1.00062, loss_obj_bbox: 1.38751/1.14279, loss_sub_giou: 1.18323/0.96237, loss_obj_giou: 1.21498/1.14286, obj_cardinality_error: 97.50000/97.16667, loss_obj_ce_0: 4.86993/4.82165, loss_verb_ce_0: 91.51163/71.10772, loss_sub_bbox_0: 1.33134/0.99984, loss_obj_bbox_0: 1.41265/1.15041, loss_sub_giou_0: 1.17616/0.96763, loss_obj_giou_0: 1.25025/1.15813, obj_cardinality_error_0: 97.50000/97.16667, loss_obj_ce_1: 5.02152/4.92912, loss_verb_ce_1: 85.87231/66.05410, loss_sub_bbox_1: 1.34081/0.99814, loss_obj_bbox_1: 1.39711/1.14822, loss_sub_giou_1: 1.18267/0.96676, loss_obj_giou_1: 1.22502/1.14584, obj_cardinality_error_1: 97.50000/97.16667] items per batch[1] items per second[4.09] total items[3] mini batches[     3] memory[5594] epoch remaining[0:31:32]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[4/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 522.64435/463.65740, loss_obj_ce: 4.80066/4.90739, obj_class_error: 100.00000/100.00000, loss_verb_ce: 78.91856/69.42992, loss_sub_bbox: 0.96490/0.99169, loss_obj_bbox: 0.96547/1.09846, loss_sub_giou: 1.13343/1.00513, loss_obj_giou: 0.98526/1.10346, obj_cardinality_error: 98.50000/97.50000, loss_obj_ce_0: 4.78313/4.81202, loss_verb_ce_0: 84.77718/74.52508, loss_sub_bbox_0: 0.95131/0.98771, loss_obj_bbox_0: 0.94386/1.09877, loss_sub_giou_0: 1.15435/1.01431, loss_obj_giou_0: 0.96141/1.10895, obj_cardinality_error_0: 98.50000/97.50000, loss_obj_ce_1: 4.85871/4.91152, loss_verb_ce_1: 80.05624/69.55464, loss_sub_bbox_1: 0.95807/0.98812, loss_obj_bbox_1: 0.95241/1.09927, loss_sub_giou_1: 1.14843/1.01218, loss_obj_giou_1: 0.97495/1.10311, obj_cardinality_error_1: 98.50000/97.50000] items per batch[1] items per second[4.06] total items[4] mini batches[     4] memory[5594] epoch remaining[0:26:24]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[5/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 806.78143/532.28221, loss_obj_ce: 4.59255/4.84442, obj_class_error: 100.00000/100.00000, loss_verb_ce: 118.50045/79.24402, loss_sub_bbox: 1.06842/1.00704, loss_obj_bbox: 1.49607/1.17798, loss_sub_giou: 1.11991/1.02809, loss_obj_giou: 1.52310/1.18739, obj_cardinality_error: 99.00000/97.80000, loss_obj_ce_0: 4.51590/4.75280, loss_verb_ce_0: 139.73328/87.56672, loss_sub_bbox_0: 1.04510/0.99919, loss_obj_bbox_0: 1.46384/1.17179, loss_sub_giou_0: 1.15847/1.04314, loss_obj_giou_0: 1.53273/1.19371, obj_cardinality_error_0: 99.00000/97.80000, loss_obj_ce_1: 4.60009/4.84923, loss_verb_ce_1: 124.76810/80.59733, loss_sub_bbox_1: 1.04620/0.99974, loss_obj_bbox_1: 1.49802/1.17902, loss_sub_giou_1: 1.15481/1.04071, loss_obj_giou_1: 1.53612/1.18972, obj_cardinality_error_1: 99.00000/97.80000] items per batch[1] items per second[4.02] total items[5] mini batches[     5] memory[5594] epoch remaining[0:23:21]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[6/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 601.49292/543.81733, loss_obj_ce: 4.78867/4.83513, obj_class_error: 75.00000/95.83333, loss_verb_ce: 87.93248/80.69210, loss_sub_bbox: 0.82507/0.97671, loss_obj_bbox: 1.02802/1.15299, loss_sub_giou: 0.78535/0.98763, loss_obj_giou: 0.83414/1.12851, obj_cardinality_error: 98.00000/97.83333, loss_obj_ce_0: 4.52086/4.71414, loss_verb_ce_0: 103.70451/90.25635, loss_sub_bbox_0: 0.82123/0.96953, loss_obj_bbox_0: 1.01765/1.14610, loss_sub_giou_0: 0.79771/1.00224, loss_obj_giou_0: 0.83335/1.13365, obj_cardinality_error_0: 98.00000/97.83333, loss_obj_ce_1: 4.70571/4.82531, loss_verb_ce_1: 92.73095/82.61960, loss_sub_bbox_1: 0.82709/0.97096, loss_obj_bbox_1: 1.02143/1.15275, loss_sub_giou_1: 0.80670/1.00170, loss_obj_giou_1: 0.83333/1.13032, obj_cardinality_error_1: 98.00000/97.83333] items per batch[1] items per second[4.04] total items[6] mini batches[     6] memory[5594] epoch remaining[0:21:18]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[7/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 641.39056/557.75636, loss_obj_ce: 4.90255/4.84476, obj_class_error: 100.00000/96.42857, loss_verb_ce: 101.92509/83.72538, loss_sub_bbox: 0.51546/0.91082, loss_obj_bbox: 0.71932/1.09103, loss_sub_giou: 0.64131/0.93816, loss_obj_giou: 0.92473/1.09940, obj_cardinality_error: 98.50000/97.92857, loss_obj_ce_0: 4.82078/4.72937, loss_verb_ce_0: 103.27353/92.11595, loss_sub_bbox_0: 0.54785/0.90929, loss_obj_bbox_0: 0.71481/1.08449, loss_sub_giou_0: 0.65345/0.95241, loss_obj_giou_0: 0.93026/1.10459, obj_cardinality_error_0: 98.50000/97.92857, loss_obj_ce_1: 4.89918/4.83586, loss_verb_ce_1: 101.12318/85.26297, loss_sub_bbox_1: 0.53955/0.90933, loss_obj_bbox_1: 0.72030/1.09097, loss_sub_giou_1: 0.65021/0.95149, loss_obj_giou_1: 0.93125/1.10188, obj_cardinality_error_1: 98.50000/97.92857] items per batch[1] items per second[4.08] total items[7] mini batches[     7] memory[5594] epoch remaining[0:19:50]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[8/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 500.75528/550.63123, loss_obj_ce: 4.98699/4.86254, obj_class_error: 100.00000/96.87500, loss_verb_ce: 75.31762/82.67441, loss_sub_bbox: 1.28874/0.95806, loss_obj_bbox: 1.37661/1.12673, loss_sub_giou: 1.31405/0.98514, loss_obj_giou: 1.28465/1.12256, obj_cardinality_error: 98.50000/98.00000, loss_obj_ce_0: 4.64916/4.71935, loss_verb_ce_0: 78.18519/90.37460, loss_sub_bbox_0: 1.26847/0.95418, loss_obj_bbox_0: 1.37429/1.12071, loss_sub_giou_0: 1.32144/0.99854, loss_obj_giou_0: 1.31959/1.13147, obj_cardinality_error_0: 98.50000/98.00000, loss_obj_ce_1: 4.92745/4.84731, loss_verb_ce_1: 75.73714/84.07224, loss_sub_bbox_1: 1.27962/0.95562, loss_obj_bbox_1: 1.36123/1.12476, loss_sub_giou_1: 1.31788/0.99729, loss_obj_giou_1: 1.28178/1.12437, obj_cardinality_error_1: 98.50000/98.00000] items per batch[1] items per second[4.14] total items[8] mini batches[     8] memory[5594] epoch remaining[0:18:42]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[9/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 351.49127/528.50456, loss_obj_ce: 4.82713/4.85861, obj_class_error: 100.00000/97.22222, loss_verb_ce: 52.37919/79.30828, loss_sub_bbox: 0.73508/0.93328, loss_obj_bbox: 0.75530/1.08546, loss_sub_giou: 1.00212/0.98703, loss_obj_giou: 0.88835/1.09653, obj_cardinality_error: 98.00000/98.00000, loss_obj_ce_0: 4.77259/4.72526, loss_verb_ce_0: 55.60209/86.51099, loss_sub_bbox_0: 0.72531/0.92875, loss_obj_bbox_0: 0.71013/1.07509, loss_sub_giou_0: 1.01595/1.00047, loss_obj_giou_0: 0.88453/1.10403, obj_cardinality_error_0: 98.00000/98.00000, loss_obj_ce_1: 4.81058/4.84323, loss_verb_ce_1: 52.22464/80.53362, loss_sub_bbox_1: 0.72964/0.93051, loss_obj_bbox_1: 0.74187/1.08221, loss_sub_giou_1: 1.00133/0.99774, loss_obj_giou_1: 0.88353/1.09761, obj_cardinality_error_1: 98.00000/98.00000] items per batch[1] items per second[3.82] total items[9] mini batches[     9] memory[5594] epoch remaining[0:17:55]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[10/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 511.00266/526.75437, loss_obj_ce: 4.93784/4.86653, obj_class_error: 100.00000/97.50000, loss_verb_ce: 74.34896/78.81235, loss_sub_bbox: 0.80172/0.92013, loss_obj_bbox: 1.04694/1.08161, loss_sub_giou: 0.80521/0.96885, loss_obj_giou: 1.15331/1.10221, obj_cardinality_error: 98.50000/98.05000, loss_obj_ce_0: 4.89653/4.74239, loss_verb_ce_0: 86.19478/86.47937, loss_sub_bbox_0: 0.79716/0.91560, loss_obj_bbox_0: 1.03388/1.07097, loss_sub_giou_0: 0.82468/0.98289, loss_obj_giou_0: 1.18127/1.11175, obj_cardinality_error_0: 98.50000/98.05000, loss_obj_ce_1: 4.96837/4.85575, loss_verb_ce_1: 77.69972/80.25023, loss_sub_bbox_1: 0.79765/0.91722, loss_obj_bbox_1: 1.03071/1.07706, loss_sub_giou_1: 0.81559/0.97952, loss_obj_giou_1: 1.16278/1.10413, obj_cardinality_error_1: 98.50000/98.05000] items per batch[1] items per second[4.07] total items[10] mini batches[    10] memory[5594] epoch remaining[0:17:13]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[20/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 37.14392/399.75184, loss_obj_ce: 1.09589/4.22504, obj_class_error: 100.00000/98.75000, loss_verb_ce: 2.73052/59.12459, loss_sub_bbox: 0.67883/0.90674, loss_obj_bbox: 0.85091/0.97492, loss_sub_giou: 0.84348/0.97374, loss_obj_giou: 1.01775/1.03761, obj_cardinality_error: 2.00000/78.85000, loss_obj_ce_0: 1.12251/4.14549, loss_verb_ce_0: 3.04712/64.38484, loss_sub_bbox_0: 0.69025/0.90130, loss_obj_bbox_0: 0.83821/0.96901, loss_sub_giou_0: 0.84836/0.98125, loss_obj_giou_0: 1.00856/1.04212, obj_cardinality_error_0: 2.00000/78.65000, loss_obj_ce_1: 1.11979/4.20045, loss_verb_ce_1: 2.59470/60.02183, loss_sub_bbox_1: 0.69067/0.90391, loss_obj_bbox_1: 0.84639/0.96998, loss_sub_giou_1: 0.84191/0.97994, loss_obj_giou_1: 1.01280/1.03902, obj_cardinality_error_1: 2.00000/78.82500] items per batch[1] items per second[0.38] total items[20] mini batches[    20] memory[6208] epoch remaining[0:14:23]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[30/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 38.16548/277.15744, loss_obj_ce: 1.20049/3.27587, obj_class_error: 100.00000/99.16667, loss_verb_ce: 1.38490/39.83538, loss_sub_bbox: 1.39990/0.87858, loss_obj_bbox: 1.20195/0.97012, loss_sub_giou: 0.83293/0.98209, loss_obj_giou: 1.34175/1.12208, obj_cardinality_error: 2.00000/53.58333, loss_obj_ce_0: 1.29150/3.23311, loss_verb_ce_0: 1.31924/43.34204, loss_sub_bbox_0: 1.42938/0.87659, loss_obj_bbox_0: 1.21667/0.97015, loss_sub_giou_0: 0.83805/0.98475, loss_obj_giou_0: 1.34565/1.12147, obj_cardinality_error_0: 2.00000/53.45000, loss_obj_ce_1: 1.25413/3.26680, loss_verb_ce_1: 1.36054/40.42796, loss_sub_bbox_1: 1.42194/0.87736, loss_obj_bbox_1: 1.22685/0.96807, loss_sub_giou_1: 0.83721/0.98661, loss_obj_giou_1: 1.35271/1.12173, obj_cardinality_error_1: 2.00000/53.56667] items per batch[1] items per second[0.37] total items[30] mini batches[    30] memory[6208] epoch remaining[0:13:33]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[40/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 22.64571/215.26301, loss_obj_ce: 0.90818/2.72780, obj_class_error: 100.00000/99.37500, loss_verb_ce: 1.21028/30.17518, loss_sub_bbox: 0.57788/0.86035, loss_obj_bbox: 0.34709/0.90729, loss_sub_giou: 0.97083/1.03358, loss_obj_giou: 1.00702/1.19717, obj_cardinality_error: 2.00000/40.80000, loss_obj_ce_0: 0.89739/2.69668, loss_verb_ce_0: 1.18894/32.79787, loss_sub_bbox_0: 0.55115/0.85773, loss_obj_bbox_0: 0.35264/0.90654, loss_sub_giou_0: 0.94235/1.03413, loss_obj_giou_0: 1.02372/1.19835, obj_cardinality_error_0: 2.00000/40.70000, loss_obj_ce_1: 0.91020/2.72067, loss_verb_ce_1: 1.16708/30.61958, loss_sub_bbox_1: 0.55902/0.85742, loss_obj_bbox_1: 0.35411/0.90400, loss_sub_giou_1: 0.94988/1.03655, loss_obj_giou_1: 1.04882/1.19946, obj_cardinality_error_1: 2.00000/40.78750] items per batch[1] items per second[0.37] total items[40] mini batches[    40] memory[6208] epoch remaining[0:13:05]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[50/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 31.52422/177.81782, loss_obj_ce: 1.53352/2.41348, obj_class_error: 100.00000/99.50000, loss_verb_ce: 1.20551/24.37680, loss_sub_bbox: 0.69520/0.81550, loss_obj_bbox: 0.81185/0.85904, loss_sub_giou: 1.26011/1.04279, loss_obj_giou: 1.70058/1.25158, obj_cardinality_error: 4.50000/33.25000, loss_obj_ce_0: 1.41791/2.38396, loss_verb_ce_0: 1.10168/26.47240, loss_sub_bbox_0: 0.70396/0.81918, loss_obj_bbox_0: 0.78170/0.85565, loss_sub_giou_0: 1.33093/1.05297, loss_obj_giou_0: 1.73000/1.25861, obj_cardinality_error_0: 4.50000/33.17000, loss_obj_ce_1: 1.49004/2.40669, loss_verb_ce_1: 1.12929/24.73045, loss_sub_bbox_1: 0.69334/0.81444, loss_obj_bbox_1: 0.79281/0.85338, loss_sub_giou_1: 1.27047/1.04962, loss_obj_giou_1: 1.72054/1.25579, obj_cardinality_error_1: 4.50000/33.24000] items per batch[1] items per second[0.37] total items[50] mini batches[    50] memory[6208] epoch remaining[0:12:50]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[60/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 25.30767/152.44835, loss_obj_ce: 1.00802/2.22120, obj_class_error: 100.00000/99.58333, loss_verb_ce: 1.19753/20.52511, loss_sub_bbox: 0.51028/0.76256, loss_obj_bbox: 0.64851/0.80776, loss_sub_giou: 0.91394/1.01555, loss_obj_giou: 1.06323/1.25407, obj_cardinality_error: 2.00000/28.21667, loss_obj_ce_0: 1.04347/2.18861, loss_verb_ce_0: 1.20437/22.26816, loss_sub_bbox_0: 0.54410/0.76429, loss_obj_bbox_0: 0.69181/0.80511, loss_sub_giou_0: 0.97336/1.02616, loss_obj_giou_0: 1.10672/1.26369, obj_cardinality_error_0: 2.00000/28.15000, loss_obj_ce_1: 1.04634/2.21342, loss_verb_ce_1: 1.20545/20.81959, loss_sub_bbox_1: 0.52234/0.76141, loss_obj_bbox_1: 0.65858/0.80275, loss_sub_giou_1: 0.92379/1.02124, loss_obj_giou_1: 1.07501/1.25896, obj_cardinality_error_1: 2.00000/28.20833] items per batch[1] items per second[0.38] total items[60] mini batches[    60] memory[6208] epoch remaining[0:12:36]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[70/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 29.42776/134.43032, loss_obj_ce: 1.47842/2.06061, obj_class_error: 100.00000/99.64286, loss_verb_ce: 1.18734/17.76789, loss_sub_bbox: 0.72359/0.74914, loss_obj_bbox: 0.87608/0.78555, loss_sub_giou: 0.56161/0.97984, loss_obj_giou: 1.41592/1.25059, obj_cardinality_error: 3.00000/24.53571, loss_obj_ce_0: 1.42807/2.03037, loss_verb_ce_0: 1.16903/19.25631, loss_sub_bbox_0: 0.75370/0.74725, loss_obj_bbox_0: 0.86772/0.78125, loss_sub_giou_0: 0.57499/0.98928, loss_obj_giou_0: 1.41679/1.26106, obj_cardinality_error_0: 3.00000/24.47857, loss_obj_ce_1: 1.48771/2.05535, loss_verb_ce_1: 1.19053/18.01942, loss_sub_bbox_1: 0.71822/0.74739, loss_obj_bbox_1: 0.86356/0.78073, loss_sub_giou_1: 0.55614/0.98411, loss_obj_giou_1: 1.40710/1.25360, obj_cardinality_error_1: 3.00000/24.52857] items per batch[1] items per second[0.36] total items[70] mini batches[    70] memory[6208] epoch remaining[0:12:31]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[80/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 23.98088/120.90156, loss_obj_ce: 1.32735/1.94104, obj_class_error: 100.00000/99.68750, loss_verb_ce: 1.22742/15.69947, loss_sub_bbox: 0.42811/0.73006, loss_obj_bbox: 0.33634/0.77351, loss_sub_giou: 1.12893/0.96561, loss_obj_giou: 1.31665/1.23955, obj_cardinality_error: 3.00000/21.78125, loss_obj_ce_0: 1.28325/1.91325, loss_verb_ce_0: 1.21409/17.00082, loss_sub_bbox_0: 0.37280/0.72694, loss_obj_bbox_0: 0.34412/0.76832, loss_sub_giou_0: 1.10195/0.96893, loss_obj_giou_0: 1.22306/1.24841, obj_cardinality_error_0: 3.00000/21.73125, loss_obj_ce_1: 1.33087/1.93640, loss_verb_ce_1: 1.22589/15.91966, loss_sub_bbox_1: 0.40766/0.72848, loss_obj_bbox_1: 0.33736/0.76902, loss_sub_giou_1: 1.12666/0.96820, loss_obj_giou_1: 1.24135/1.23942, obj_cardinality_error_1: 3.00000/21.77500] items per batch[1] items per second[0.37] total items[80] mini batches[    80] memory[6208] epoch remaining[0:12:23]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[90/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 34.37859/110.17679, loss_obj_ce: 1.78217/1.83087, obj_class_error: 100.00000/99.72222, loss_verb_ce: 1.02151/14.08811, loss_sub_bbox: 1.18927/0.71397, loss_obj_bbox: 0.87647/0.75016, loss_sub_giou: 1.06511/0.95183, loss_obj_giou: 1.40556/1.22423, obj_cardinality_error: 6.50000/19.65000, loss_obj_ce_0: 1.66892/1.80648, loss_verb_ce_0: 1.02248/15.24465, loss_sub_bbox_0: 1.20085/0.71048, loss_obj_bbox_0: 0.88773/0.74507, loss_sub_giou_0: 1.06187/0.95390, loss_obj_giou_0: 1.38794/1.22937, obj_cardinality_error_0: 6.50000/19.60556, loss_obj_ce_1: 1.76567/1.82643, loss_verb_ce_1: 1.02137/14.28194, loss_sub_bbox_1: 1.21710/0.71244, loss_obj_bbox_1: 0.90301/0.74668, loss_sub_giou_1: 1.05768/0.95453, loss_obj_giou_1: 1.36688/1.22276, obj_cardinality_error_1: 6.50000/19.64444] items per batch[1] items per second[0.37] total items[90] mini batches[    90] memory[6208] epoch remaining[0:12:16]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[100/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 22.92677/101.81252, loss_obj_ce: 0.50719/1.76358, obj_class_error: 100.00000/99.75000, loss_verb_ce: 1.33037/12.79971, loss_sub_bbox: 0.31049/0.70149, loss_obj_bbox: 0.45815/0.73979, loss_sub_giou: 0.98220/0.95200, loss_obj_giou: 1.72911/1.23524, obj_cardinality_error: 1.00000/17.94000, loss_obj_ce_0: 0.48836/1.73981, loss_verb_ce_0: 1.29805/13.84072, loss_sub_bbox_0: 0.30573/0.69566, loss_obj_bbox_0: 0.43471/0.73324, loss_sub_giou_0: 0.96862/0.95129, loss_obj_giou_0: 1.65182/1.23525, obj_cardinality_error_0: 1.00000/17.90000, loss_obj_ce_1: 0.49082/1.75782, loss_verb_ce_1: 1.23692/12.97413, loss_sub_bbox_1: 0.30750/0.69961, loss_obj_bbox_1: 0.45595/0.73546, loss_sub_giou_1: 0.98810/0.95348, loss_obj_giou_1: 1.70859/1.23182, obj_cardinality_error_1: 1.00000/17.93500] items per batch[1] items per second[0.37] total items[100] mini batches[   100] memory[6208] epoch remaining[0:12:10]\n",
      "INFO:trainer.default_trainer:epochs[     0] optim steps[110/2700] learning rate[default: 1.00000e-05] train loss[total_loss: 28.98773/94.72420, loss_obj_ce: 1.04745/1.70474, obj_class_error: 100.00000/99.77273, loss_verb_ce: 1.31745/11.74115, loss_sub_bbox: 0.53830/0.68095, loss_obj_bbox: 0.73210/0.71813, loss_sub_giou: 1.26281/0.95297, loss_obj_giou: 1.61678/1.23280, obj_cardinality_error: 2.00000/16.57273, loss_obj_ce_0: 1.04684/1.68327, loss_verb_ce_0: 1.26483/12.68654, loss_sub_bbox_0: 0.51910/0.67252, loss_obj_bbox_0: 0.75829/0.71215, loss_sub_giou_0: 1.21118/0.95078, loss_obj_giou_0: 1.59319/1.22787, obj_cardinality_error_0: 2.00000/16.53636, loss_obj_ce_1: 1.05279/1.70015, loss_verb_ce_1: 1.27515/11.89842, loss_sub_bbox_1: 0.52931/0.67715, loss_obj_bbox_1: 0.75606/0.71433, loss_sub_giou_1: 1.24790/0.95417, loss_obj_giou_1: 1.61102/1.22716, obj_cardinality_error_1: 2.00000/16.56818] items per batch[1] items per second[0.37] total items[110] mini batches[   110] memory[6208] epoch remaining[0:12:04]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/djjin/Mygit/X-Decoder/notebooks/07_hoi_train_pipeline.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/07_hoi_train_pipeline.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m HDecoder_Trainer \u001b[39mas\u001b[39;00m Trainer\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/07_hoi_train_pipeline.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(opt)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/07_hoi_train_pipeline.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/trainer/default_trainer.py:255\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m prev_total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_params[\u001b[39m'\u001b[39m\u001b[39mtotal_batch_size\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    254\u001b[0m \u001b[39m# update\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(batch)\n\u001b[1;32m    257\u001b[0m \u001b[39m# for i, g in enumerate(self.lr_schedulers['default'].optimizer.param_groups):\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m#     is_close = math.isclose(g['initial_lr'], 1e-05, rel_tol=1e-9, abs_tol=1e-12)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m#     if is_close:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39m# logging\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# if prev_optim_steps != current_optim_steps:  # an optimizer update was made\u001b[39;00m\n\u001b[1;32m    266\u001b[0m log_first \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mLOG_FIRST\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10\u001b[39m)\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/trainer/default_trainer.py:148\u001b[0m, in \u001b[0;36mDefaultTrainer.train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    144\u001b[0m total_batch_sample \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[39mfor\u001b[39;00m batch_index, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_acc_batches):\n\u001b[1;32m    147\u001b[0m     loss_info, sample_size_info, extra_info \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 148\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpipeline\u001b[39m.\u001b[39;49mforward_step(\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    149\u001b[0m                                 batch,\n\u001b[1;32m    150\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad_acc_batches,\n\u001b[1;32m    151\u001b[0m                                 batch_index,\n\u001b[1;32m    152\u001b[0m                                 is_distributed\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt[\u001b[39m'\u001b[39;49m\u001b[39mworld_size\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m>\u001b[39;49m \u001b[39m1\u001b[39;49m))\n\u001b[1;32m    153\u001b[0m     \u001b[39m# print(loss_info)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loss\u001b[39m.\u001b[39mupdate_iter(loss_info)\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/notebooks/../pipeline/HDecoderPipeline.py:103\u001b[0m, in \u001b[0;36mHDecoderPipeline.forward_step\u001b[0;34m(self, trainer, batch, grad_acc_batches, grad_acc_index, is_distributed)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_opt[\u001b[39m'\u001b[39m\u001b[39mFP16\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    101\u001b[0m     \u001b[39m# in FP16 mode, DeepSpeed casts the model to FP16, so the input needs to be manually casted to FP16\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     batch \u001b[39m=\u001b[39m cast_batch_to_half(batch)\n\u001b[0;32m--> 103\u001b[0m loss_dict \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mcompute_loss(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_func, batch)\n\u001b[1;32m    104\u001b[0m weight_dict \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mmodels[\u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcriterion\u001b[39m.\u001b[39mweight_dict\n\u001b[1;32m    106\u001b[0m loss_dict_reduced \u001b[39m=\u001b[39m reduce_dict(loss_dict)\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/trainer/default_trainer.py:107\u001b[0m, in \u001b[0;36mDefaultTrainer.compute_loss\u001b[0;34m(self, forward_func, batch)\u001b[0m\n\u001b[1;32m    105\u001b[0m         loss \u001b[39m=\u001b[39m func(trainer, batch)\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n\u001b[0;32m--> 107\u001b[0m loss \u001b[39m=\u001b[39m forward(forward_func, \u001b[39mself\u001b[39;49m, batch)\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/trainer/default_trainer.py:103\u001b[0m, in \u001b[0;36mDefaultTrainer.compute_loss.<locals>.forward\u001b[0;34m(func, trainer, batch)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcuda\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mamp\u001b[39;00m \u001b[39mimport\u001b[39;00m autocast\n\u001b[1;32m    102\u001b[0m     \u001b[39mwith\u001b[39;00m autocast():\n\u001b[0;32m--> 103\u001b[0m         loss \u001b[39m=\u001b[39m func(trainer, batch)\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     loss \u001b[39m=\u001b[39m func(trainer, batch)\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/notebooks/../pipeline/HDecoderPipeline.py:87\u001b[0m, in \u001b[0;36mHDecoderPipeline.forward_func\u001b[0;34m(trainer, batch)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_func\u001b[39m(trainer, batch):\n\u001b[1;32m     86\u001b[0m     \u001b[39m# trainer.models['default'].train()\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     loss \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mmodels[\u001b[39m'\u001b[39;49m\u001b[39mdefault\u001b[39;49m\u001b[39m'\u001b[39;49m](batch)\n\u001b[1;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_X_decoder/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/BaseModel.py:25\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 25\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_X_decoder/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/architectures/hoi_model.py:124\u001b[0m, in \u001b[0;36mCDNHOI.forward\u001b[0;34m(self, batched_inputs, mode)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, batched_inputs, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhoi\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    123\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 124\u001b[0m         losses_hoi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_hoi(batched_inputs[\u001b[39m\"\u001b[39;49m\u001b[39mvcoco\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    125\u001b[0m         \u001b[39mreturn\u001b[39;00m losses_hoi\n\u001b[1;32m    126\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/architectures/hoi_model.py:142\u001b[0m, in \u001b[0;36mCDNHOI.forward_hoi\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    138\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone(images\u001b[39m.\u001b[39mtensor)\n\u001b[1;32m    140\u001b[0m \u001b[39m# TODO not mask None\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m# src, mask = features[-1].decompose()\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhoid_head(features, mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    143\u001b[0m losses_hoi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(out, targets)\n\u001b[1;32m    145\u001b[0m \u001b[39mdel\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_X_decoder/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/body/hoi_head.py:72\u001b[0m, in \u001b[0;36mCDN.forward\u001b[0;34m(self, features, mask, task)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     66\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     67\u001b[0m     features,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m ):\n\u001b[1;32m     71\u001b[0m     \u001b[39m# Encoder\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     encoder_features, pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(features)\n\u001b[1;32m     74\u001b[0m     bs, _, _, _ \u001b[39m=\u001b[39m pos\u001b[39m.\u001b[39mshape\n\u001b[1;32m     75\u001b[0m     pos_embed \u001b[39m=\u001b[39m pos\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_X_decoder/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/body/encoder/transformer_encoder_hoi.py:258\u001b[0m, in \u001b[0;36mTransformerEncoderHOI.forward\u001b[0;34m(self, features, targets)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, features, targets\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(features)\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/body/encoder/transformer_encoder_hoi.py:253\u001b[0m, in \u001b[0;36mTransformerEncoderHOI.forward_features\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    251\u001b[0m x \u001b[39m=\u001b[39m features[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n\u001b[1;32m    252\u001b[0m transformer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_proj(x)\n\u001b[0;32m--> 253\u001b[0m pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpe_layer(x)\n\u001b[1;32m    254\u001b[0m transformer_encoder_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(transformer, \u001b[39mNone\u001b[39;00m, pos)\n\u001b[1;32m    255\u001b[0m \u001b[39mreturn\u001b[39;00m transformer_encoder_features, pos\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_X_decoder/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/modules/position_encoding.py:45\u001b[0m, in \u001b[0;36mPositionEmbeddingSine.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     42\u001b[0m     x_embed \u001b[39m=\u001b[39m x_embed \u001b[39m/\u001b[39m (x_embed[:, :, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:] \u001b[39m+\u001b[39m eps) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[1;32m     44\u001b[0m dim_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_pos_feats, dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 45\u001b[0m dim_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtemperature \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m (\u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m (dim_t \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m2\u001b[39;49m) \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_pos_feats)\n\u001b[1;32m     47\u001b[0m pos_x \u001b[39m=\u001b[39m x_embed[:, :, :, \u001b[39mNone\u001b[39;00m] \u001b[39m/\u001b[39m dim_t\n\u001b[1;32m     48\u001b[0m pos_y \u001b[39m=\u001b[39m y_embed[:, :, :, \u001b[39mNone\u001b[39;00m] \u001b[39m/\u001b[39m dim_t\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_X_decoder/lib/python3.9/site-packages/torch/_tensor.py:39\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     38\u001b[0m         \u001b[39mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_X_decoder/lib/python3.9/site-packages/torch/_tensor.py:864\u001b[0m, in \u001b[0;36mTensor.__rpow__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[39m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    862\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__rpow__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m    863\u001b[0m     dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mresult_type(other, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 864\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(other, dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from trainer import HDecoder_Trainer as Trainer\n",
    "trainer = Trainer(opt)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_Decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
