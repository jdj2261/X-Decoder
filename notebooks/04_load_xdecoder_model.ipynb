{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  2.0 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid MIT-MAGIC-COOKIE-1 keyWARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "from utils.arguments import load_opt_command\n",
    "from utils.misc import hook_metadata, hook_switcher, hook_opt\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures import Boxes, ImageList, Instances, BitMasks, BoxMode\n",
    "from xdecoder.BaseModel import BaseModel\n",
    "from xdecoder import build_model\n",
    "from utils.visualizer import Visualizer\n",
    "from utils.distributed import init_distributed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt\n",
    "from datasets import build_evaluator, build_eval_dataloader\n",
    "from xdecoder.utils import get_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/segvlp_focalt_lang.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', '../checkpoints/xdecoder_focalt_best_openseg.pt'] \n",
    "cmdline_args.overrides\n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "config_dict\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../checkpoints/xdecoder_focalt_best_openseg.pt\n"
     ]
    }
   ],
   "source": [
    "pretrained_pth = '../checkpoints/xdecoder_focalt_best_openseg.pt'\n",
    "\n",
    "output_root = './output'\n",
    "image_pth = '../images/animals.png'\n",
    "print(pretrained_pth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*UNLOADED* sem_seg_head.predictor.pos_embed_caping.weight, Model Shape: torch.Size([77, 512])\n",
      "$UNUSED$ sem_seg_head.predictor.query_feat_caping.weight, Ckpt Shape: torch.Size([77, 512])\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained_pth).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (model): GeneralizedXdecoder(\n",
       "    (backbone): D2FocalNet(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=96, out_features=196, bias=True)\n",
       "                (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=96, out_features=196, bias=True)\n",
       "                (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x FocalModulationBlock(\n",
       "              (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=192, out_features=388, bias=True)\n",
       "                (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0-5): 6 x FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x FocalModulationBlock(\n",
       "              (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=768, out_features=1540, bias=True)\n",
       "                (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (sem_seg_head): XdecoderHead(\n",
       "      (pixel_decoder): TransformerEncoderPixelDecoder(\n",
       "        (adapter_1): Conv2d(\n",
       "          96, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_1): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (adapter_2): Conv2d(\n",
       "          192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_2): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (adapter_3): Conv2d(\n",
       "          384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_3): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (input_proj): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer): TransformerEncoderOnly(\n",
       "          (encoder): TransformerEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-5): 6 x TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.0, inplace=False)\n",
       "                (dropout2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pe_layer): Positional encoding PositionEmbeddingSine\n",
       "            num_pos_feats: 256\n",
       "            temperature: 10000\n",
       "            normalize: True\n",
       "            scale: 6.283185307179586\n",
       "        (layer_4): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (predictor): XDecoder(\n",
       "        (pe_layer): Positional encoding PositionEmbeddingSine\n",
       "            num_pos_feats: 256\n",
       "            temperature: 10000\n",
       "            normalize: True\n",
       "            scale: 6.283185307179586\n",
       "        (transformer_self_attention_layers): ModuleList(\n",
       "          (0-8): 9 x SelfAttentionLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (transformer_cross_attention_layers): ModuleList(\n",
       "          (0-8): 9 x CrossAttentionLayer(\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (transformer_ffn_layers): ModuleList(\n",
       "          (0-8): 9 x FFNLayer(\n",
       "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (query_feat): Embedding(101, 512)\n",
       "        (query_embed): Embedding(101, 512)\n",
       "        (level_embed): Embedding(3, 512)\n",
       "        (input_proj): ModuleList(\n",
       "          (0-2): 3 x Sequential()\n",
       "        )\n",
       "        (lang_encoder): LanguageEncoder(\n",
       "          (lang_encoder): Transformer(\n",
       "            (token_embedding): Embedding(49408, 512)\n",
       "            (resblocks): ModuleList(\n",
       "              (0-11): 12 x ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm()\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm()\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ln_final): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (mask_embed): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (pos_embed_caping): Embedding(77, 512)\n",
       "      )\n",
       "    )\n",
       "    (criterion): Criterion SetCriterion\n",
       "        matcher: Matcher HungarianMatcher\n",
       "            cost_class: 2.0\n",
       "            cost_mask: 5.0\n",
       "            cost_dice: 5.0\n",
       "        losses: []\n",
       "        weight_dict: {'loss_mask_ce_0': 2.0, 'loss_mask_dice_0': 5.0, 'loss_mask_bce_0': 5.0, 'loss_caption_0': 1.0, 'loss_captioning_0': 2.0, 'loss_retrieval_decoder_0': 2.0, 'loss_retrieval_backbone_0': 8.0, 'loss_grounding_ce_0': 0.4, 'loss_grounding_dice_0': 1.0, 'loss_grounding_bce_0': 1.0, 'loss_mask_ce_1': 2.0, 'loss_mask_dice_1': 5.0, 'loss_mask_bce_1': 5.0, 'loss_caption_1': 1.0, 'loss_captioning_1': 2.0, 'loss_retrieval_decoder_1': 2.0, 'loss_retrieval_backbone_1': 8.0, 'loss_grounding_ce_1': 0.4, 'loss_grounding_dice_1': 1.0, 'loss_grounding_bce_1': 1.0, 'loss_mask_ce_2': 2.0, 'loss_mask_dice_2': 5.0, 'loss_mask_bce_2': 5.0, 'loss_caption_2': 1.0, 'loss_captioning_2': 2.0, 'loss_retrieval_decoder_2': 2.0, 'loss_retrieval_backbone_2': 8.0, 'loss_grounding_ce_2': 0.4, 'loss_grounding_dice_2': 1.0, 'loss_grounding_bce_2': 1.0, 'loss_mask_ce_3': 2.0, 'loss_mask_dice_3': 5.0, 'loss_mask_bce_3': 5.0, 'loss_mask_ce_4': 2.0, 'loss_mask_dice_4': 5.0, 'loss_mask_bce_4': 5.0, 'loss_mask_ce_5': 2.0, 'loss_mask_dice_5': 5.0, 'loss_mask_bce_5': 5.0, 'loss_mask_ce_6': 2.0, 'loss_mask_dice_6': 5.0, 'loss_mask_bce_6': 5.0, 'loss_mask_ce_7': 2.0, 'loss_mask_dice_7': 5.0, 'loss_mask_bce_7': 5.0, 'loss_mask_ce_8': 2.0, 'loss_mask_dice_8': 5.0, 'loss_mask_bce_8': 5.0, 'loss_mask_ce_9': 2.0, 'loss_mask_dice_9': 5.0, 'loss_mask_bce_9': 5.0}\n",
       "        num_classes: 133\n",
       "        eos_coef: 0.1\n",
       "        num_points: 12544\n",
       "        oversample_ratio: 3.0\n",
       "        importance_sample_ratio: 0.75\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = []\n",
    "t.append(transforms.Resize(800, interpolation=Image.BICUBIC))\n",
    "transform = transforms.Compose(t)\n",
    "\n",
    "thing_classes = ['zebra','antelope','giraffe','ostrich','sky','water','grass','sand','tree']\n",
    "thing_colors = [random_color(rgb=True, maximum=255).astype(np.int64).tolist() for _ in range(len(thing_classes))]\n",
    "thing_dataset_id_to_contiguous_id = {x:x for x in range(len(thing_classes))}\n",
    "\n",
    "MetadataCatalog.get(\"demo\").set(\n",
    "    thing_colors=thing_colors,\n",
    "    thing_classes=thing_classes,\n",
    "    thing_dataset_id_to_contiguous_id=thing_dataset_id_to_contiguous_id,\n",
    ")\n",
    "\n",
    "model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(thing_classes + [\"background\"], is_eval=False)\n",
    "metadata = MetadataCatalog.get('demo')\n",
    "model.model.metadata = metadata\n",
    "model.model.sem_seg_head.num_classes = len(thing_classes)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 2, 6, 2], [0, 1, 2, 3], [3, 3, 3, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers = opt['MODEL']['BACKBONE']['FOCAL']['DEPTHS']\n",
    "out_indices = opt['MODEL']['BACKBONE']['FOCAL']['OUT_INDICES']\n",
    "focal_levels = opt['MODEL']['BACKBONE']['FOCAL']['FOCAL_LEVELS']\n",
    "num_layers, out_indices, focal_levels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 3, 800, 1344])\n",
      "patch_embed shape: torch.Size([1, 96, 200, 336])\n",
      "flatten shape: torch.Size([1, 67200, 96])\n",
      "\n",
      "Backbone output\n",
      "res2: torch.Size([1, 96, 200, 336])\n",
      "res3: torch.Size([1, 192, 100, 168])\n",
      "res4: torch.Size([1, 384, 50, 84])\n",
      "res5: torch.Size([1, 768, 25, 42])\n"
     ]
    }
   ],
   "source": [
    "pixel_mean = torch.Tensor([123.675, 116.280, 103.530]).view(-1, 1, 1).cuda()\n",
    "pixel_std = torch.Tensor([58.395, 57.120, 57.375]).view(-1, 1, 1).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_ori = Image.open(image_pth).convert('RGB')\n",
    "    width = image_ori.size[0]\n",
    "    height = image_ori.size[1]\n",
    "    image = transform(image_ori)\n",
    "    image = np.asarray(image)\n",
    "    image_ori = np.asarray(image_ori)\n",
    "    images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "    batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "    # outputs = model.forward(batch_inputs)\n",
    "\n",
    "    images = [x[\"image\"].to(\"cuda\") for x in batch_inputs]\n",
    "    images = [(x - pixel_mean) / pixel_std for x in images]\n",
    "    \n",
    "    images = ImageList.from_tensors(images, 32)\n",
    "    print(f\"Image shape: {images.tensor.shape}\")\n",
    "\n",
    "    ######################################################\n",
    "    ## Backbone\n",
    "    # output = model.model.backbone(images.tensor)\n",
    "    # print(output.shape)\n",
    "    ######################################################\n",
    "    ## Backbone Inner Code\n",
    "\n",
    "    # 1. Patch Embedding\n",
    "    patch_embed = deepcopy(model.model.backbone.patch_embed)\n",
    "    x = patch_embed(images.tensor)\n",
    "    print(f\"patch_embed shape: {x.shape}\")\n",
    "    Wh, Ww = x.size(2), x.size(3)\n",
    "    x = x.flatten(2).transpose(1, 2)\n",
    "    print(f\"flatten shape: {x.shape}\")\n",
    "\n",
    "    # 2. Dropout\n",
    "    pos_drop = deepcopy(model.model.backbone.pos_drop)\n",
    "    x = pos_drop(x)\n",
    "\n",
    "    # 3. Layers\n",
    "    outs = {}\n",
    "    for i in range(len(num_layers)):\n",
    "        layer = model.model.backbone.layers[i]\n",
    "        x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "        if i in out_indices:\n",
    "            norm_layer = getattr(model.model.backbone, f'norm{i}')\n",
    "            x_out = norm_layer(x_out)\n",
    "\n",
    "            out = x_out.view(-1, H, W, model.model.backbone.num_features[i]).permute(0, 3, 1, 2).contiguous()\n",
    "            outs[\"res{}\".format(i + 2)] = out\n",
    "            \n",
    "    if len(model.model.backbone.out_indices) == 0:\n",
    "        outs[\"res5\"] = x_out.view(-1, H, W, model.model.backbone.num_features[i]).permute(0, 3, 1, 2).contiguous()\n",
    "    \n",
    "    print(\"\\nBackbone output\")\n",
    "    outputs = {}\n",
    "    for k in outs.keys():\n",
    "        if k in model.model.backbone._out_features:\n",
    "            outputs[k] = outs[k]\n",
    "            print(f\"{k}: {outs[k].shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEMANTIC SEGMENTATION HEAD (xdecoder_head.py)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIXEL DECODER (transformer_encoder_fpn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 768, 25, 42])\n",
      "transformer: torch.Size([1, 512, 25, 42])\n",
      "transformer conv: torch.Size([1, 512, 25, 42])\n",
      "transformer: torch.Size([1, 512, 25, 42])\n",
      "x: torch.Size([1, 384, 50, 84])\n",
      "cur_fpn: torch.Size([1, 512, 50, 84])\n",
      "y: torch.Size([1, 512, 25, 42])\n",
      "y: torch.Size([1, 512, 50, 84])\n",
      "ouput_y: torch.Size([1, 512, 50, 84])\n",
      "x: torch.Size([1, 192, 100, 168])\n",
      "cur_fpn: torch.Size([1, 512, 100, 168])\n",
      "y: torch.Size([1, 512, 50, 84])\n",
      "y: torch.Size([1, 512, 100, 168])\n",
      "ouput_y: torch.Size([1, 512, 100, 168])\n",
      "x: torch.Size([1, 96, 200, 336])\n",
      "cur_fpn: torch.Size([1, 512, 200, 336])\n",
      "y: torch.Size([1, 512, 100, 168])\n",
      "y: torch.Size([1, 512, 200, 336])\n",
      "ouput_y: torch.Size([1, 512, 200, 336])\n",
      "\n",
      "Pixel Decoder output\n",
      "mask features: torch.Size([1, 512, 200, 336])\n",
      "transformer enoder features: torch.Size([1, 512, 25, 42])\n",
      "\n",
      "multi_scale_feature: torch.Size([1, 512, 25, 42])\n",
      "multi_scale_feature: torch.Size([1, 512, 50, 84])\n",
      "multi_scale_feature: torch.Size([1, 512, 100, 168])\n"
     ]
    }
   ],
   "source": [
    "## XDecoder Head\n",
    "## 1. Pixel Decoder\n",
    "\n",
    "with torch.no_grad():\n",
    "    ## Pixel Decoder Inner Code\n",
    "    multi_scale_features = []\n",
    "    num_cur_levels = 0\n",
    "    \n",
    "    # Reverse feature maps into top-down order (from low to high resolution)\n",
    "    for idx, f in enumerate(model.model.sem_seg_head.pixel_decoder.in_features[::-1]):\n",
    "        x = outputs[f]\n",
    "        lateral_conv = model.model.sem_seg_head.pixel_decoder.lateral_convs[idx]\n",
    "        output_conv = model.model.sem_seg_head.pixel_decoder.output_convs[idx]\n",
    "        if lateral_conv is None:\n",
    "            print(f\"x: {x.shape}\")\n",
    "            transformer = model.model.sem_seg_head.pixel_decoder.input_proj(x)\n",
    "            print(f\"transformer: {transformer.shape}\")\n",
    "            pos = model.model.sem_seg_head.pixel_decoder.pe_layer(x)\n",
    "            # print(transformer.shape)\n",
    "            transformer = model.model.sem_seg_head.pixel_decoder.transformer(transformer, None, pos)\n",
    "            y = output_conv(transformer)\n",
    "            print(f\"transformer conv: {y.shape}\")\n",
    "            # save intermediate feature as input to Transformer decoder\n",
    "            print(f\"transformer: {transformer.shape}\")\n",
    "            transformer_encoder_features = transformer\n",
    "        else:\n",
    "            print(f\"x: {x.shape}\")\n",
    "            cur_fpn = lateral_conv(x)\n",
    "            print(f\"cur_fpn: {cur_fpn.shape}\")\n",
    "            # Following FPN implementation, we use nearest upsampling here\n",
    "            print(f\"y: {y.shape}\")\n",
    "            y = cur_fpn + F.interpolate(y, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "            print(f\"y: {y.shape}\")\n",
    "            y = output_conv(y)\n",
    "            print(f\"ouput_y: {y.shape}\")\n",
    "            # print()\n",
    "        if num_cur_levels < model.model.sem_seg_head.pixel_decoder.maskformer_num_feature_levels:\n",
    "            multi_scale_features.append(y)\n",
    "            num_cur_levels += 1\n",
    "\n",
    "    mask_features = model.model.sem_seg_head.pixel_decoder.mask_features(y) if model.model.sem_seg_head.pixel_decoder.mask_on else None\n",
    "    \n",
    "    print(\"\\nPixel Decoder output\")\n",
    "    print(f\"mask features: {mask_features.shape}\")\n",
    "    print(f\"transformer enoder features: {transformer_encoder_features.shape}\\n\")\n",
    "    for multi_scale_feature in multi_scale_features:\n",
    "        print(f\"multi_scale_feature: {multi_scale_feature.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTOR (xdecoder.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.model.sem_seg_head.predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: torch.Size([1, 512, 25, 42]) -> torch.Size([1050, 1, 512])\n",
      "0: torch.Size([1, 512, 25, 42]) -> torch.Size([1050, 1, 512])\n",
      "1: torch.Size([1, 512, 50, 84]) -> torch.Size([4200, 1, 512])\n",
      "1: torch.Size([1, 512, 50, 84]) -> torch.Size([4200, 1, 512])\n",
      "2: torch.Size([1, 512, 100, 168]) -> torch.Size([16800, 1, 512])\n",
      "2: torch.Size([1, 512, 100, 168]) -> torch.Size([16800, 1, 512])\n",
      "query_pos: torch.Size([101, 1, 512])\n",
      "query_feature: torch.Size([101, 1, 512])\n",
      "self_tgt_mask: torch.Size([1, 178, 178])\n",
      "\n",
      "decoder_output: torch.Size([1, 101, 512])\n",
      "norm decoder_output: torch.Size([1, 101, 512])\n",
      "obj_token: torch.Size([1, 100, 512])\n",
      "cls_token: torch.Size([1, 1, 512])\n",
      "sim: torch.Size([1, 100, 1])\n",
      "cls_token: torch.Size([1, 1, 512])\n",
      "decoder_output: torch.Size([1, 101, 512])\n",
      "class_embed torch.Size([1, 101, 512])\n",
      "outputs_class torch.Size([1, 101, 10])\n",
      "attn_mask: torch.Size([8, 101, 1050])\n",
      "\n",
      "outputs_class: torch.Size([1, 101, 10])\n",
      "outputs_mask: torch.Size([1, 101, 200, 336])\n",
      "attn_mask: torch.Size([8, 101, 1050])\n",
      "outputs_caption: torch.Size([1, 101, 512])\n",
      "\n",
      "9 3\n",
      "Query: torch.Size([101, 1, 512]), Key: torch.Size([1050, 1, 512])\n",
      "Query: torch.Size([101, 1, 512])\n",
      "torch.Size([101, 1, 512]) torch.Size([1050, 1, 512]) torch.Size([8, 101, 1050])\n",
      "Query: torch.Size([101, 1, 512]), Key: torch.Size([4200, 1, 512])\n",
      "Query: torch.Size([101, 1, 512])\n",
      "torch.Size([101, 1, 512]) torch.Size([4200, 1, 512]) torch.Size([8, 101, 4200])\n",
      "Query: torch.Size([101, 1, 512]), Key: torch.Size([16800, 1, 512])\n",
      "Query: torch.Size([101, 1, 512])\n",
      "torch.Size([101, 1, 512]) torch.Size([16800, 1, 512]) torch.Size([8, 101, 16800])\n",
      "Query: torch.Size([101, 1, 512]), Key: torch.Size([1050, 1, 512])\n",
      "Query: torch.Size([101, 1, 512])\n",
      "torch.Size([101, 1, 512]) torch.Size([1050, 1, 512]) torch.Size([8, 101, 1050])\n",
      "Query: torch.Size([101, 1, 512]), Key: torch.Size([4200, 1, 512])\n",
      "Query: torch.Size([101, 1, 512])\n",
      "torch.Size([101, 1, 512]) torch.Size([4200, 1, 512]) torch.Size([8, 101, 4200])\n",
      "Query: torch.Size([101, 1, 512]), Key: torch.Size([16800, 1, 512])\n",
      "Query: torch.Size([101, 1, 512])\n",
      "torch.Size([101, 1, 512]) torch.Size([16800, 1, 512]) torch.Size([8, 101, 16800])\n",
      "Query: torch.Size([101, 1, 512]), Key: torch.Size([1050, 1, 512])\n",
      "Query: torch.Size([101, 1, 512])\n",
      "torch.Size([101, 1, 512]) torch.Size([1050, 1, 512]) torch.Size([8, 101, 1050])\n",
      "Query: torch.Size([101, 1, 512]), Key: torch.Size([4200, 1, 512])\n",
      "Query: torch.Size([101, 1, 512])\n",
      "torch.Size([101, 1, 512]) torch.Size([4200, 1, 512]) torch.Size([8, 101, 4200])\n",
      "Query: torch.Size([101, 1, 512]), Key: torch.Size([16800, 1, 512])\n",
      "Query: torch.Size([101, 1, 512])\n",
      "torch.Size([101, 1, 512]) torch.Size([16800, 1, 512]) torch.Size([8, 101, 16800])\n",
      "pred_logits torch.Size([1, 101, 10])\n",
      "pred_masks torch.Size([1, 101, 200, 336])\n",
      "pred_captions torch.Size([1, 101, 512])\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################################################\n",
    "###### XDecoder Head ######\n",
    "###### 2. Predictor ######\n",
    "num_feature_levels = 3\n",
    "with torch.no_grad():\n",
    "    # predictions = model.model.sem_seg_head.predictor(multi_scale_features, mask_features, mask=None, target_queries=None, target_vlp=None, task='seg', extra={})\n",
    "    # print(\"\\npredictor output\")\n",
    "    # print(predictions.keys())\n",
    "    x = multi_scale_features\n",
    "    src = []\n",
    "    pos = []\n",
    "    size_list = []\n",
    "    for i in range(num_feature_levels):\n",
    "        size_list.append(x[i].shape[-2:])\n",
    "        pos.append(predictor.pe_layer(x[i], None).flatten(2))\n",
    "        # print(predictor.level_embed.weight[i][None, :, None].shape)\n",
    "        # print(predictor.level_embed.weight[i])\n",
    "        src.append(predictor.input_proj[i](x[i]).flatten(2) + predictor.level_embed.weight[i][None, :, None])\n",
    "\n",
    "        # flatten NxCxHxW to HWxNxC\n",
    "        pos[-1] = pos[-1].permute(2, 0, 1)\n",
    "        src[-1] = src[-1].permute(2, 0, 1)\n",
    "    \n",
    "        print(f\"{i}: {x[i].shape} -> {pos[-1].shape}\")\n",
    "        print(f\"{i}: {x[i].shape} -> {pos[-1].shape}\")\n",
    "\n",
    "    _, bs, _ = src[0].shape\n",
    "\n",
    "    query_embed = predictor.query_embed.weight.unsqueeze(1).repeat(1, bs, 1)\n",
    "    output = predictor.query_feat.weight.unsqueeze(1).repeat(1, bs, 1)\n",
    "\n",
    "    print(f\"query_pos: {query_embed.shape}\")\n",
    "    print(f\"query_feature: {output.shape}\")\n",
    "\n",
    "    predictions_class = []\n",
    "    predictions_mask = []\n",
    "    predictions_bbox = []\n",
    "    predictions_caption = []\n",
    "    predictions_captioning = []\n",
    "\n",
    "    # refer forward_prediction_heads func\n",
    "    # seg\n",
    "    self_tgt_mask = predictor.self_attn_mask[:,:predictor.num_queries,:predictor.num_queries].repeat(output.shape[1]*predictor.num_heads, 1, 1)\n",
    "    print(f\"self_tgt_mask: {predictor.self_attn_mask.shape}\\n\")\n",
    "    \n",
    "    decoder_output = predictor.decoder_norm(output)\n",
    "    decoder_output = decoder_output.transpose(0, 1)\n",
    "    print(f\"decoder_output: {decoder_output.shape}\")\n",
    "\n",
    "    norm_decoder_output = decoder_output / (decoder_output.norm(dim=-1, keepdim=True) + 1e-7)\n",
    "    print(f\"norm decoder_output: {norm_decoder_output.shape}\")\n",
    "    obj_token = norm_decoder_output[:,:predictor.num_queries-1]\n",
    "    cls_token = norm_decoder_output[:,predictor.num_queries-1:predictor.num_queries]\n",
    "\n",
    "    print(f\"obj_token: {obj_token.shape}\")\n",
    "    print(f\"cls_token: {cls_token.shape}\")\n",
    "\n",
    "    sim = (cls_token @ obj_token.transpose(1,2)).softmax(-1)[:,0,:,None] # TODO include class token.\n",
    "    cls_token = (sim * decoder_output[:,:predictor.num_queries-1]).sum(dim=1, keepdim=True)\n",
    "\n",
    "    print(f\"sim: {sim.shape}\")\n",
    "    print(f\"cls_token: {cls_token.shape}\")\n",
    "\n",
    "    decoder_output = torch.cat((decoder_output[:,:predictor.num_queries-1], cls_token), dim=1)\n",
    "    print(f\"decoder_output: {decoder_output.shape}\")\n",
    "\n",
    "    class_embed = decoder_output @ predictor.class_embed\n",
    "    outputs_class = predictor.lang_encoder.compute_similarity(class_embed, fake=(((not predictor.task_switch['mask']) and predictor.training)))\n",
    "    print(f\"class_embed {class_embed.shape}\")\n",
    "    print(f\"outputs_class {outputs_class.shape}\")\n",
    "\n",
    "    attn_mask = torch.zeros((list(decoder_output.shape[:2]) + [size_list[0][0]*size_list[0][1]]), device=decoder_output.device).repeat(predictor.num_heads, 1, 1).bool()\n",
    "    print(f\"attn_mask: {attn_mask.shape}\\n\")\n",
    "    \n",
    "    results = predictor.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[0], task='seg')\n",
    "\n",
    "    for key, val in results.items():\n",
    "        if key == \"outputs_bbox\" or key ==\"outputs_captionting\":\n",
    "            continue\n",
    "        print(f\"{key}: {val.shape}\")\n",
    "    \n",
    "    attn_mask = results[\"attn_mask\"]\n",
    "    predictions_class.append(results[\"outputs_class\"])\n",
    "    predictions_mask.append(results[\"outputs_mask\"])\n",
    "    predictions_bbox.append(results[\"outputs_bbox\"])\n",
    "    predictions_caption.append(results[\"outputs_caption\"])\n",
    "    predictions_captioning.append(results[\"outputs_captionting\"])\n",
    "\n",
    "    print()\n",
    "    print(predictor.num_layers, num_feature_levels)\n",
    "    for i in range(predictor.num_layers):\n",
    "        level_index = i % predictor.num_feature_levels\n",
    "        attn_mask[torch.where(attn_mask.sum(-1) == attn_mask.shape[-1])] = False\n",
    "        \n",
    "        print(f\"Query: {output.shape}, Key: {src[level_index].shape}\")\n",
    "        output, avg_attn = predictor.transformer_cross_attention_layers[i](\n",
    "            output, src[level_index],\n",
    "            memory_mask=attn_mask,\n",
    "            memory_key_padding_mask=None,  # here we do not apply masking on padded region\n",
    "            pos=pos[level_index], query_pos=query_embed\n",
    "        )\n",
    "\n",
    "        print(f\"Query: {output.shape}\")\n",
    "        output = predictor.transformer_self_attention_layers[i](\n",
    "            output, tgt_mask=self_tgt_mask,\n",
    "            tgt_key_padding_mask=None,\n",
    "            query_pos=query_embed\n",
    "        )\n",
    "\n",
    "        output = predictor.transformer_ffn_layers[i](\n",
    "            output\n",
    "        )\n",
    "\n",
    "        print(output.shape, src[level_index].shape, attn_mask.shape)\n",
    "        results = predictor.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[(i + 1) % predictor.num_feature_levels], layer_id=i, task=\"seg\")\n",
    "        attn_mask = results[\"attn_mask\"]\n",
    "        predictions_class.append(results[\"outputs_class\"])\n",
    "        predictions_mask.append(results[\"outputs_mask\"])\n",
    "        predictions_bbox.append(results[\"outputs_bbox\"])\n",
    "        predictions_caption.append(results[\"outputs_caption\"])\n",
    "        predictions_captioning.append(results[\"outputs_captionting\"])\n",
    "\n",
    "    out = {\n",
    "        'pred_logits': predictions_class[-1],\n",
    "        'pred_masks': predictions_mask[-1],\n",
    "        'pred_boxes': predictions_bbox[-1],\n",
    "        'pred_captions': predictions_caption[-1],\n",
    "    }\n",
    "    \n",
    "    for key, val in out.items():\n",
    "        if key == \"pred_boxes\":\n",
    "            continue\n",
    "        print(key, val.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
