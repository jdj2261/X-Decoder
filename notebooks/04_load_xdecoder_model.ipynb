{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjin/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  1.13 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n",
      "['xdecoder', 'body', 'decoder', 'xdecoder']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid MIT-MAGIC-COOKIE-1 keyWARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/ADE20K_2021_17_01/images_detectron2/training\n",
      "datasets/ADE20K_2021_17_01/images_detectron2/validation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils.arguments import load_opt_command\n",
    "from utils.misc import hook_metadata, hook_switcher, hook_opt\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures import Boxes, ImageList, Instances, BitMasks, BoxMode\n",
    "from xdecoder.BaseModel import BaseModel\n",
    "from xdecoder import build_model\n",
    "from utils.visualizer import Visualizer\n",
    "from utils.distributed import init_distributed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt\n",
    "from datasets import build_evaluator, build_eval_dataloader\n",
    "from xdecoder.utils import get_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/svlp_focalt_lang.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', '../checkpoints/xdecoder_focalt_best_openseg.pt'] \n",
    "cmdline_args.overrides\n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "config_dict\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../checkpoints/xdecoder_focalt_best_openseg.pt\n"
     ]
    }
   ],
   "source": [
    "pretrained_pth = os.path.join(opt['WEIGHT'])\n",
    "output_root = './output'\n",
    "image_pth = '../images/animals.png'\n",
    "print(pretrained_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ADE20K': {'DATASET': {'DATASET': 'ade'},\n",
      "            'INPUT': {'COLOR_AUG_SSD': True,\n",
      "                      'CROP': {'ENABLED': True,\n",
      "                               'SINGLE_CATEGORY_MAX_AREA': 1.0,\n",
      "                               'SIZE': '(640, 640)',\n",
      "                               'TYPE': 'absolute'},\n",
      "                      'DATASET_MAPPER_NAME': 'mask_former_panoptic',\n",
      "                      'FORMAT': 'RGB',\n",
      "                      'MASK_FORMAT': 'polygon',\n",
      "                      'MAX_SIZE_TEST': 200,\n",
      "                      'MAX_SIZE_TRAIN': 200,\n",
      "                      'MIN_SIZE_TEST': 640,\n",
      "                      'MIN_SIZE_TRAIN': 640,\n",
      "                      'MIN_SIZE_TRAIN_SAMPLING': 'choice',\n",
      "                      'SIZE_DIVISIBILITY': 640},\n",
      "            'TEST': {'BATCH_SIZE_TOTAL': 8}},\n",
      " 'BDD': {'DATALOADER': {'ASPECT_RATIO_GROUPING': False,\n",
      "                        'FILTER_EMPTY_ANNOTATIONS': False,\n",
      "                        'LOAD_PROPOSALS': False,\n",
      "                        'NUM_WORKERS': 0,\n",
      "                        'SAMPLER_TRAIN': 'TrainingSampler'},\n",
      "         'INPUT': {'MAX_SIZE_TEST': 1333,\n",
      "                   'MIN_SIZE_TEST': 800,\n",
      "                   'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
      "                   'PIXEL_STD': [58.395, 57.12, 57.375]},\n",
      "         'TEST': {'BATCH_SIZE_TOTAL': 8}},\n",
      " 'CITY': {'DATALOADER': {'FILTER_EMPTY_ANNOTATIONS': True, 'NUM_WORKERS': 4},\n",
      "          'INPUT': {'COLOR_AUG_SSD': True,\n",
      "                    'CROP': {'ENABLED': True,\n",
      "                             'SINGLE_CATEGORY_MAX_AREA': 1.0,\n",
      "                             'SIZE': '(512, 1024)',\n",
      "                             'TYPE': 'absolute'},\n",
      "                    'DATASET_MAPPER_NAME': 'mask_former_panoptic',\n",
      "                    'FORMAT': 'RGB',\n",
      "                    'MASK_FORMAT': 'polygon',\n",
      "                    'MAX_SIZE_TEST': 2048,\n",
      "                    'MAX_SIZE_TRAIN': 4096,\n",
      "                    'MIN_SIZE_TEST': 1024,\n",
      "                    'MIN_SIZE_TRAIN': 1024,\n",
      "                    'MIN_SIZE_TRAIN_SAMPLING': 'choice',\n",
      "                    'SIZE_DIVISIBILITY': -1},\n",
      "          'TEST': {'AUG': {'ENABLED': False,\n",
      "                           'FLIP': True,\n",
      "                           'MAX_SIZE': 4096,\n",
      "                           'MIN_SIZES': [512, 768, 1024, 1280, 1536, 1792]},\n",
      "                   'BATCH_SIZE_TOTAL': 8,\n",
      "                   'EVAL_PERIOD': 5000}},\n",
      " 'COCO': {'DATALOADER': {'ASPECT_RATIO_GROUPING': True,\n",
      "                         'FILTER_EMPTY_ANNOTATIONS': False,\n",
      "                         'LOAD_PROPOSALS': False,\n",
      "                         'NUM_WORKERS': 2,\n",
      "                         'SAMPLER_TRAIN': 'TrainingSampler'},\n",
      "          'DATASET': {'DATASET': 'coco'},\n",
      "          'INPUT': {'COLOR_AUG_SSD': False,\n",
      "                    'CROP': {'ENABLED': True},\n",
      "                    'DATASET_MAPPER_NAME': 'coco_panoptic_lsj',\n",
      "                    'FORMAT': 'RGB',\n",
      "                    'IGNORE_VALUE': 255,\n",
      "                    'IMAGE_SIZE': 1024,\n",
      "                    'MASK_FORMAT': 'polygon',\n",
      "                    'MAX_SCALE': 2.0,\n",
      "                    'MAX_SIZE_TEST': 1333,\n",
      "                    'MIN_SCALE': 0.1,\n",
      "                    'MIN_SIZE_TEST': 800,\n",
      "                    'RANDOM_FLIP': 'horizontal',\n",
      "                    'SIZE_DIVISIBILITY': 32},\n",
      "          'TEST': {'AUG': {'ENABLED': False},\n",
      "                   'BATCH_SIZE_TOTAL': 8,\n",
      "                   'DETECTIONS_PER_IMAGE': 100,\n",
      "                   'IOU_TYPE': ['bbox', 'segm'],\n",
      "                   'MODEL_FILE': '',\n",
      "                   'NAME': 'coco_eval',\n",
      "                   'USE_MULTISCALE': False}},\n",
      " 'CUDA': True,\n",
      " 'DATALOADER': {'ASPECT_RATIO_GROUPING': True,\n",
      "                'FILTER_EMPTY_ANNOTATIONS': False,\n",
      "                'LOAD_PROPOSALS': False,\n",
      "                'NUM_WORKERS': 16,\n",
      "                'SAMPLER_TRAIN': 'TrainingSampler'},\n",
      " 'DATASETS': {'CLASS_CONCAT': False,\n",
      "              'PROPOSAL_FILES_TRAIN': [],\n",
      "              'SIZE_DIVISIBILITY': 32,\n",
      "              'TEST': ['refcocog_val_umd']},\n",
      " 'INPUT': {'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
      "           'PIXEL_STD': [58.395, 57.12, 57.375]},\n",
      " 'MODEL': {'BACKBONE': {'FOCAL': {'DEPTHS': [2, 2, 6, 2],\n",
      "                                  'DROP_PATH_RATE': 0.3,\n",
      "                                  'DROP_RATE': 0.0,\n",
      "                                  'EMBED_DIM': 96,\n",
      "                                  'FOCAL_LEVELS': [3, 3, 3, 3],\n",
      "                                  'FOCAL_WINDOWS': [3, 3, 3, 3],\n",
      "                                  'MLP_RATIO': 4.0,\n",
      "                                  'OUT_FEATURES': ['res2',\n",
      "                                                   'res3',\n",
      "                                                   'res4',\n",
      "                                                   'res5'],\n",
      "                                  'OUT_INDICES': [0, 1, 2, 3],\n",
      "                                  'PATCH_NORM': True,\n",
      "                                  'PATCH_SIZE': 4,\n",
      "                                  'PRETRAIN_IMG_SIZE': 224,\n",
      "                                  'SCALING_MODULATOR': True,\n",
      "                                  'USE_CHECKPOINT': False,\n",
      "                                  'USE_CONV_EMBED': True,\n",
      "                                  'USE_LAYERSCALE': True,\n",
      "                                  'USE_POSTLN': True,\n",
      "                                  'USE_POSTLN_IN_MODULATION': False},\n",
      "                        'LOAD_PRETRAINED': False,\n",
      "                        'NAME': 'focal_dw',\n",
      "                        'PRETRAINED': ''},\n",
      "           'BACKBONE_DIM': 768,\n",
      "           'DECODER': {'CAPTION': {'ENABLED': True,\n",
      "                                   'PHRASE_PROB': 0.0,\n",
      "                                   'SIM_THRES': 0.95},\n",
      "                       'CAPTIONING': {'ENABLED': True, 'STEP': 50},\n",
      "                       'DEC_LAYERS': 10,\n",
      "                       'DETECTION': False,\n",
      "                       'DIM_FEEDFORWARD': 2048,\n",
      "                       'DROPOUT': 0.0,\n",
      "                       'ENFORCE_INPUT_PROJ': False,\n",
      "                       'GROUNDING': {'CLASS_WEIGHT': 0.5,\n",
      "                                     'ENABLED': True,\n",
      "                                     'MAX_LEN': 5,\n",
      "                                     'TEXT_WEIGHT': 2.0},\n",
      "                       'HIDDEN_DIM': 512,\n",
      "                       'IMPORTANCE_SAMPLE_RATIO': 0.75,\n",
      "                       'MASK': True,\n",
      "                       'NAME': 'xdecoder',\n",
      "                       'NHEADS': 8,\n",
      "                       'NUM_OBJECT_QUERIES': 101,\n",
      "                       'OVERSAMPLE_RATIO': 3.0,\n",
      "                       'PRE_NORM': False,\n",
      "                       'RETRIEVAL': {'DIM_IMG': 768,\n",
      "                                     'ENABLED': True,\n",
      "                                     'ENSEMBLE': True},\n",
      "                       'SIZE_DIVISIBILITY': 32,\n",
      "                       'TEST': {'INSTANCE_ON': True,\n",
      "                                'OBJECT_MASK_THRESHOLD': 0.8,\n",
      "                                'OVERLAP_THRESHOLD': 0.8,\n",
      "                                'PANOPTIC_ON': True,\n",
      "                                'SEMANTIC_ON': True,\n",
      "                                'SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE': False},\n",
      "                       'TOP_CAPTIONING_LAYERS': 3,\n",
      "                       'TOP_CAPTION_LAYERS': 3,\n",
      "                       'TOP_GROUNDING_LAYERS': 3,\n",
      "                       'TOP_RETRIEVAL_LAYERS': 3,\n",
      "                       'TRAIN_NUM_POINTS': 12544,\n",
      "                       'TRANSFORMER_IN_FEATURE': 'multi_scale_pixel_decoder'},\n",
      "           'DIM_PROJ': 512,\n",
      "           'ENCODER': {'COMMON_STRIDE': 4,\n",
      "                       'CONVS_DIM': 512,\n",
      "                       'DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES': ['res3',\n",
      "                                                                      'res4',\n",
      "                                                                      'res5'],\n",
      "                       'IGNORE_VALUE': 255,\n",
      "                       'IN_FEATURES': ['res2', 'res3', 'res4', 'res5'],\n",
      "                       'LOSS_WEIGHT': 1.0,\n",
      "                       'MASK_DIM': 512,\n",
      "                       'NAME': 'transformer_encoder_fpn',\n",
      "                       'NORM': 'GN',\n",
      "                       'NUM_CLASSES': 133,\n",
      "                       'TRANSFORMER_ENC_LAYERS': 6},\n",
      "           'HEAD': 'xdecoder_head',\n",
      "           'KEYPOINT_ON': False,\n",
      "           'LOAD_PROPOSALS': False,\n",
      "           'MASK_ON': False,\n",
      "           'NAME': 'xdecoder_model',\n",
      "           'TEXT': {'ARCH': 'vlpencoder',\n",
      "                    'AUTOGRESSIVE': True,\n",
      "                    'CONTEXT_LENGTH': 77,\n",
      "                    'HEADS': 8,\n",
      "                    'LAYERS': 12,\n",
      "                    'NAME': 'transformer',\n",
      "                    'TOKENIZER': 'clip',\n",
      "                    'WIDTH': 512}},\n",
      " 'PORT': 53711,\n",
      " 'REF': {'DATALOADER': {'ASPECT_RATIO_GROUPING': False,\n",
      "                        'FILTER_EMPTY_ANNOTATIONS': False,\n",
      "                        'LOAD_PROPOSALS': False,\n",
      "                        'NUM_WORKERS': 0,\n",
      "                        'SAMPLER_TRAIN': 'TrainingSampler'},\n",
      "         'INPUT': {'FORMAT': 'RGB',\n",
      "                   'MAX_SIZE_TEST': 1024,\n",
      "                   'MIN_SIZE_TEST': 512,\n",
      "                   'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
      "                   'PIXEL_STD': [58.395, 57.12, 57.375]},\n",
      "         'TEST': {'BATCH_SIZE_TOTAL': 8}},\n",
      " 'SAVE_DIR': '../../data/output/test',\n",
      " 'SCAN': {'DATALOADER': {'ASPECT_RATIO_GROUPING': False,\n",
      "                         'FILTER_EMPTY_ANNOTATIONS': False,\n",
      "                         'LOAD_PROPOSALS': False,\n",
      "                         'NUM_WORKERS': 0,\n",
      "                         'SAMPLER_TRAIN': 'TrainingSampler'},\n",
      "          'INPUT': {'MAX_SIZE_TEST': 1024,\n",
      "                    'MIN_SIZE_TEST': 512,\n",
      "                    'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
      "                    'PIXEL_STD': [58.395, 57.12, 57.375]},\n",
      "          'TEST': {'BATCH_SIZE_TOTAL': 8}},\n",
      " 'SUN': {'DATALOADER': {'ASPECT_RATIO_GROUPING': False,\n",
      "                        'FILTER_EMPTY_ANNOTATIONS': False,\n",
      "                        'LOAD_PROPOSALS': False,\n",
      "                        'NUM_WORKERS': 0,\n",
      "                        'SAMPLER_TRAIN': 'TrainingSampler'},\n",
      "         'INPUT': {'MAX_SIZE_TEST': 1024,\n",
      "                   'MIN_SIZE_TEST': 512,\n",
      "                   'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
      "                   'PIXEL_STD': [58.395, 57.12, 57.375]},\n",
      "         'TEST': {'BATCH_SIZE_TOTAL': 8}},\n",
      " 'VCOCO': {'DATALOADER': {'ASPECT_RATIO_GROUPING': False,\n",
      "                          'FILTER_EMPTY_ANNOTATIONS': False,\n",
      "                          'LOAD_PROPOSALS': False,\n",
      "                          'NUM_WORKERS': 0,\n",
      "                          'SAMPLER_TRAIN': 'TrainingSampler'},\n",
      "           'INPUT': {'FORMAT': 'RGB',\n",
      "                     'MAX_SIZE_TEST': 1024,\n",
      "                     'MIN_SIZE_TEST': 512,\n",
      "                     'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
      "                     'PIXEL_STD': [58.395, 57.12, 57.375]},\n",
      "           'TEST': {'BATCH_SIZE_TOTAL': 8}},\n",
      " 'VERBOSE': True,\n",
      " 'VLP': {'DATALOADER': {'ASPECT_RATIO_GROUPING': True,\n",
      "                        'FILTER_EMPTY_ANNOTATIONS': False,\n",
      "                        'LOAD_PROPOSALS': False,\n",
      "                        'NUM_WORKERS': 16,\n",
      "                        'SAMPLER_TRAIN': 'TrainingSampler'},\n",
      "         'INPUT': {'COLOR_AUG_SSD': False,\n",
      "                   'CROP': {'ENABLED': True},\n",
      "                   'DATASET_MAPPER_NAME': 'vlpretrain',\n",
      "                   'FORMAT': 'RGB',\n",
      "                   'IGNORE_VALUE': 255,\n",
      "                   'IMAGE_SIZE': 224,\n",
      "                   'MASK_FORMAT': 'polygon',\n",
      "                   'SIZE_DIVISIBILITY': 32},\n",
      "         'TEST': {'BATCH_SIZE_TOTAL': 64},\n",
      "         'TRAIN': {'BATCH_SIZE_PER_GPU': 2, 'BATCH_SIZE_TOTAL': 2}},\n",
      " 'WEIGHT': '../checkpoints/xdecoder_focalt_best_openseg.pt',\n",
      " 'command': 'evaluate',\n",
      " 'conf_files': ['/home/djjin/Mygit/X-Decoder/configs/xdecoder/svlp_focalt_lang.yaml'],\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'env_info': 'no MPI',\n",
      " 'local_rank': 0,\n",
      " 'local_size': 1,\n",
      " 'master_address': '127.0.0.1',\n",
      " 'master_port': '8673',\n",
      " 'overrides': ['WEIGHT', '../checkpoints/xdecoder_focalt_best_openseg.pt'],\n",
      " 'rank': 0,\n",
      " 'world_size': 1}\n"
     ]
    }
   ],
   "source": [
    "pprint(opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*UNLOADED* sem_seg_head.predictor.pos_embed_caping.weight, Model Shape: torch.Size([77, 512])\n",
      "$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])\n",
      "$UNUSED$ sem_seg_head.predictor.query_feat_caping.weight, Ckpt Shape: torch.Size([77, 512])\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained_pth).eval().cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D2FocalNet(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): FocalModulationBlock(\n",
       "          (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=96, out_features=196, bias=True)\n",
       "            (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): FocalModulationBlock(\n",
       "          (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=96, out_features=196, bias=True)\n",
       "            (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchEmbed(\n",
       "        (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): FocalModulationBlock(\n",
       "          (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=192, out_features=388, bias=True)\n",
       "            (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): FocalModulationBlock(\n",
       "          (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=192, out_features=388, bias=True)\n",
       "            (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchEmbed(\n",
       "        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): FocalModulationBlock(\n",
       "          (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "            (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): FocalModulationBlock(\n",
       "          (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "            (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): FocalModulationBlock(\n",
       "          (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "            (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): FocalModulationBlock(\n",
       "          (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "            (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): FocalModulationBlock(\n",
       "          (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "            (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): FocalModulationBlock(\n",
       "          (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "            (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchEmbed(\n",
       "        (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): FocalModulationBlock(\n",
       "          (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=768, out_features=1540, bias=True)\n",
       "            (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): FocalModulationBlock(\n",
       "          (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (modulation): FocalModulation(\n",
       "            (f): Linear(in_features=768, out_features=1540, bias=True)\n",
       "            (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (focal_layers): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = []\n",
    "t.append(transforms.Resize(800, interpolation=Image.BICUBIC))\n",
    "transform = transforms.Compose(t)\n",
    "\n",
    "thing_classes = ['zebra','antelope','giraffe','ostrich','sky','water','grass','sand','tree']\n",
    "thing_colors = [random_color(rgb=True, maximum=255).astype(np.int).tolist() for _ in range(len(thing_classes))]\n",
    "thing_dataset_id_to_contiguous_id = {x:x for x in range(len(thing_classes))}\n",
    "\n",
    "MetadataCatalog.get(\"demo\").set(\n",
    "    thing_colors=thing_colors,\n",
    "    thing_classes=thing_classes,\n",
    "    thing_dataset_id_to_contiguous_id=thing_dataset_id_to_contiguous_id,\n",
    ")\n",
    "\n",
    "model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(thing_classes + [\"background\"], is_eval=False)\n",
    "metadata = MetadataCatalog.get('demo')\n",
    "model.model.metadata = metadata\n",
    "model.model.sem_seg_head.num_classes = len(thing_classes)\n",
    "model.eval()\n",
    "model.model.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 2, 6, 2], [0, 1, 2, 3], [3, 3, 3, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers = opt['MODEL']['BACKBONE']['FOCAL']['DEPTHS']\n",
    "out_indices = opt['MODEL']['BACKBONE']['FOCAL']['OUT_INDICES']\n",
    "focal_levels = opt['MODEL']['BACKBONE']['FOCAL']['FOCAL_LEVELS']\n",
    "num_layers, out_indices, focal_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 800, 1344])\n",
      "torch.Size([1, 96, 200, 336])\n",
      "torch.Size([1, 67200, 96])\n",
      "torch.Size([1, 67200, 96])\n",
      "res2 torch.Size([1, 96, 200, 336])\n",
      "res3 torch.Size([1, 192, 100, 168])\n",
      "res4 torch.Size([1, 384, 50, 84])\n",
      "res5 torch.Size([1, 768, 25, 42])\n"
     ]
    }
   ],
   "source": [
    "pixel_mean = torch.Tensor([123.675, 116.280, 103.530]).view(-1, 1, 1).cuda()\n",
    "pixel_std = torch.Tensor([58.395, 57.120, 57.375]).view(-1, 1, 1).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_ori = Image.open(image_pth).convert('RGB')\n",
    "    width = image_ori.size[0]\n",
    "    height = image_ori.size[1]\n",
    "    image = transform(image_ori)\n",
    "    image = np.asarray(image)\n",
    "    image_ori = np.asarray(image_ori)\n",
    "    images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "    batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "    # outputs = model.forward(batch_inputs)\n",
    "\n",
    "    images = [x[\"image\"].to(\"cuda\") for x in batch_inputs]\n",
    "    images = [(x - pixel_mean) / pixel_std for x in images]\n",
    "    \n",
    "    images = ImageList.from_tensors(images, 32)\n",
    "    print(images.tensor.shape)\n",
    "\n",
    "    #####################\n",
    "    # Backbone Stard\n",
    "    output = model.model.backbone(images.tensor)\n",
    "    # print(output)\n",
    "    # Patch Embedding\n",
    "    patch_embed = deepcopy(model.model.backbone.patch_embed)\n",
    "    x = patch_embed(images.tensor)\n",
    "    print(x.shape)\n",
    "    Wh, Ww = x.size(2), x.size(3)\n",
    "    x = x.flatten(2).transpose(1, 2)\n",
    "    print(x.shape)\n",
    "\n",
    "    # Dropout\n",
    "    pos_drop = deepcopy(model.model.backbone.pos_drop)\n",
    "    x = pos_drop(x)\n",
    "    print(x.shape)\n",
    "\n",
    "    # Layers\n",
    "    outs = {}\n",
    "    for i in range(len(num_layers)):\n",
    "        layer = model.model.backbone.layers[i]\n",
    "        x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "        if i in out_indices:\n",
    "            norm_layer = getattr(model.model.backbone, f'norm{i}')\n",
    "            x_out = norm_layer(x_out)\n",
    "\n",
    "            out = x_out.view(-1, H, W, model.model.backbone.num_features[i]).permute(0, 3, 1, 2).contiguous()\n",
    "            outs[\"res{}\".format(i + 2)] = out\n",
    "            \n",
    "    if len(model.model.backbone.out_indices) == 0:\n",
    "        outs[\"res5\"] = x_out.view(-1, H, W, model.model.backbone.num_features[i]).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    for out, val in outs.items():\n",
    "        print(out, val.shape)\n",
    "\n",
    "    outputs = {}\n",
    "    for k in outs.keys():\n",
    "        if k in model.model.backbone._out_features:\n",
    "            outputs[k] = outs[k]\n",
    "    # Backbone End\n",
    "    #####################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     image_ori = Image.open(image_pth).convert('RGB')\n",
    "#     width = image_ori.size[0]\n",
    "#     height = image_ori.size[1]\n",
    "#     image = transform(image_ori)\n",
    "#     image = np.asarray(image)\n",
    "#     image_ori = np.asarray(image_ori)\n",
    "#     images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "#     batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "#     outputs = model.forward(batch_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.sem_seg_head.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.sem_seg_head.pixel_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.sem_seg_head.predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "\n",
    "# # from xdecoder.backbone.focal import D2FocalNet\n",
    "# focal_backbone = deepcopy(model.model.backbone)\n",
    "\n",
    "# # from xdecoder.body.encoder.transformer_encoder_fpn import TransformerEncoderPixelDecoder\n",
    "# pixel_decoder = deepcopy(model.model.sem_seg_head.pixel_decoder)\n",
    "\n",
    "# # from xdecoder.body.decoder import TransformerEncoderPixelDecoder\n",
    "# predictor = model.model.sem_seg_head.predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.sem_seg_head.predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.sem_seg_head.transformer_in_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel_mean = torch.Tensor([123.675, 116.280, 103.530]).view(-1, 1, 1).cuda()\n",
    "# pixel_std = torch.Tensor([58.395, 57.120, 57.375]).view(-1, 1, 1).cuda()\n",
    "\n",
    "# # focal_backbone.eval()\n",
    "# # evaluate() function in xdecoder_model.py\n",
    "# with torch.no_grad():\n",
    "#     image_ori = Image.open(image_pth).convert('RGB')\n",
    "#     width = image_ori.size[0]\n",
    "#     height = image_ori.size[1]\n",
    "#     image = transform(image_ori)\n",
    "#     image = np.asarray(image)\n",
    "#     image_ori = np.asarray(image_ori)\n",
    "#     images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "#     batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "#     images = [x[\"image\"].to('cuda') for x in batch_inputs]\n",
    "#     images = [(x - pixel_mean) / pixel_std for x in images]\n",
    "    \n",
    "#     images = ImageList.from_tensors(images, 32)\n",
    "#     img_bs = images.tensor.shape[0]\n",
    "\n",
    "#     # Process\n",
    "#     # 1. image -> focal_backbone -> features\n",
    "#     # 2. features -> TransformerEncoderPixelDecoder -> mask_features, transformer_encoder_features, multi_scale_features\n",
    "\n",
    "#     features = focal_backbone(images.tensor)\n",
    "#     print(features['res5'].shape)\n",
    "#     mask_features, transformer_encoder_features, multi_scale_features = pixel_decoder(features)\n",
    "    \n",
    "#     print(f\"mask_features {mask_features.shape}\")\n",
    "#     print(f\"transformer_encoder_features {transformer_encoder_features.shape}\")\n",
    "#     print(f\"multi_scale_features {multi_scale_features[0].shape}\")\n",
    "\n",
    "#     # predictor를 수정하면 될듯\n",
    "#     predictions = predictor(multi_scale_features, mask_features, None, None, None, \"seg\", {})\n",
    "#     for key, value in predictions.items():\n",
    "#         if not isinstance(predictions[key], list):\n",
    "#             print(key, predictions[key].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = []\n",
    "# t.append(transforms.Resize(800, interpolation=Image.BICUBIC))\n",
    "# transform = transforms.Compose(t)\n",
    "\n",
    "# thing_classes = ['zebra','antelope','giraffe','ostrich','sky','water','grass','sand','tree']\n",
    "# thing_colors = [random_color(rgb=True, maximum=255).astype(np.int).tolist() for _ in range(len(thing_classes))]\n",
    "# thing_dataset_id_to_contiguous_id = {x:x for x in range(len(thing_classes))}\n",
    "\n",
    "# MetadataCatalog.get(\"demo\").set(\n",
    "#     thing_colors=thing_colors,\n",
    "#     thing_classes=thing_classes,\n",
    "#     thing_dataset_id_to_contiguous_id=thing_dataset_id_to_contiguous_id,\n",
    "# )\n",
    "# model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(thing_classes + [\"background\"], is_eval=False)\n",
    "# metadata = MetadataCatalog.get('demo')\n",
    "# model.model.metadata = metadata\n",
    "# model.model.sem_seg_head.num_classes = len(thing_classes)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     image_ori = Image.open(image_pth).convert('RGB')\n",
    "#     width = image_ori.size[0]\n",
    "#     height = image_ori.size[1]\n",
    "#     image = transform(image_ori)\n",
    "#     image = np.asarray(image)\n",
    "#     image_ori = np.asarray(image_ori)\n",
    "#     images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "\n",
    "#     batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "#     outputs = model.forward(batch_inputs)\n",
    "#     visual = Visualizer(image_ori, metadata=metadata)\n",
    "\n",
    "#     inst_seg = outputs[-1]['instances']\n",
    "#     inst_seg.pred_masks = inst_seg.pred_masks.cpu()\n",
    "#     inst_seg.pred_boxes = BitMasks(inst_seg.pred_masks > 0).get_bounding_boxes()\n",
    "#     demo = visual.draw_instance_predictions(inst_seg) # rgb Image\n",
    "\n",
    "#     if not os.path.exists(output_root):\n",
    "#         os.makedirs(output_root)\n",
    "#     demo.save(os.path.join(output_root, 'inst.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
