{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjin/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  1.13 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "os.environ[\"DATASET\"] = \"../datasets\"\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures import Boxes, ImageList, Instances, BitMasks, BoxMode\n",
    "\n",
    "from hdecoder.BaseModel import BaseModel\n",
    "from hdecoder import build_model\n",
    "\n",
    "from utils.arguments import load_opt_command\n",
    "from utils.misc import hook_metadata, hook_switcher, hook_opt\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "# cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/svlp_focalt_lang.yaml\")]\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/hdecoder/vcoco_modified.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', '../checkpoints/xdecoder_focalt_best_openseg.pt'] \n",
    "cmdline_args.overrides\n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "config_dict\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel(opt, build_model(opt)).train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (model): CDNHOI(\n",
       "    (backbone): D2FocalNet(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=96, out_features=196, bias=True)\n",
       "                (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=96, out_features=196, bias=True)\n",
       "                (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (drop_path): DropPath(drop_prob=0.027)\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=192, out_features=388, bias=True)\n",
       "                (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (drop_path): DropPath(drop_prob=0.055)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=192, out_features=388, bias=True)\n",
       "                (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (drop_path): DropPath(drop_prob=0.082)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.109)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.136)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.164)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.191)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.218)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.245)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=768, out_features=1540, bias=True)\n",
       "                (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (drop_path): DropPath(drop_prob=0.273)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=768, out_features=1540, bias=True)\n",
       "                (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (drop_path): DropPath(drop_prob=0.300)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (hoid_head): CDN(\n",
       "      (encoder): TransformerEncoderHOI(\n",
       "        (adapter_1): Conv2d(\n",
       "          96, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_1): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (adapter_2): Conv2d(\n",
       "          192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_2): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (adapter_3): Conv2d(\n",
       "          384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_3): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (input_proj): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer): TransformerEncoderOnly(\n",
       "          (encoder): TransformerEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (3): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (4): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (5): TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pe_layer): Positional encoding PositionEmbeddingSine\n",
       "            num_pos_feats: 256\n",
       "            temperature: 10000\n",
       "            normalize: True\n",
       "            scale: 6.283185307179586\n",
       "      )\n",
       "      (hoi_decoder): HDecoder(\n",
       "        (hopd_decoder): TransformerDecoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (interaction_decoder): TransformerDecoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              (dropout3): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (query_embed): Embedding(100, 512)\n",
       "      (obj_class_embed): Linear(in_features=512, out_features=82, bias=True)\n",
       "      (verb_class_embed): Linear(in_features=512, out_features=29, bias=True)\n",
       "      (sub_bbox_embed): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (obj_bbox_embed): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (criterion): SetCriterionHOI(\n",
       "      (matcher): HungarianMatcherHOI()\n",
       "    )\n",
       "    (postprocessors): PostProcessHOI()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdecoder.modules.criterion import SetCriterionHOI\n",
    "from hdecoder.modules.matcher import HungarianMatcherHOI\n",
    "\n",
    "matcher = HungarianMatcherHOI(\n",
    "    cost_obj_class=1, \n",
    "    cost_verb_class=1,\n",
    "    cost_bbox=2.5, \n",
    "    cost_giou=1, \n",
    "    cost_matching=1).cuda()\n",
    "\n",
    "weight_dict = {}\n",
    "weight_dict['loss_obj_ce'] = 1\n",
    "weight_dict['loss_verb_ce'] = 2\n",
    "weight_dict['loss_sub_bbox'] = 2.5\n",
    "weight_dict['loss_obj_bbox'] = 2.5\n",
    "weight_dict['loss_sub_giou'] = 1\n",
    "weight_dict['loss_obj_giou'] = 1\n",
    "losses = ['obj_labels', 'verb_labels', 'sub_obj_boxes', 'obj_cardinality']\n",
    "\n",
    "num_obj_classes = 80\n",
    "num_queries = 100\n",
    "num_verb_classes = 29\n",
    "eos_coef = 0.1\n",
    "criterion = SetCriterionHOI(\n",
    "    num_obj_classes, num_queries, num_verb_classes, matcher=matcher,\n",
    "                            weight_dict=weight_dict, eos_coef=eos_coef, losses=losses).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 5400\n"
     ]
    }
   ],
   "source": [
    "from datasets.build import build_train_dataloader\n",
    "train_data_loader = build_train_dataloader(opt)\n",
    "dataset_names = opt['DATASETS']['TRAIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred_obj_logits': tensor([[[ 0.3025, -0.2461, -0.4371,  ..., -0.1586,  0.3977, -0.0141],\n",
      "         [ 0.6444, -0.2453, -0.2314,  ...,  0.0213,  0.5619,  0.1261],\n",
      "         [ 0.3451, -0.2869, -0.3080,  ..., -0.1223,  0.5795,  0.1722],\n",
      "         ...,\n",
      "         [ 0.6946, -0.1942, -0.2288,  ..., -0.4474,  0.5334,  0.2017],\n",
      "         [ 0.6319, -0.1746, -0.4094,  ..., -0.3819,  0.5854,  0.0134],\n",
      "         [ 0.2345, -0.0625, -0.1796,  ..., -0.0327,  0.4712,  0.4044]],\n",
      "\n",
      "        [[ 0.3557, -0.1455, -0.5048,  ..., -0.2123,  0.6393, -0.0415],\n",
      "         [ 0.2060, -0.3879, -0.3742,  ..., -0.4141,  0.7517, -0.1217],\n",
      "         [ 0.5301, -0.0733, -0.4160,  ..., -0.4313,  0.6778, -0.0017],\n",
      "         ...,\n",
      "         [ 0.3470, -0.2779, -0.5717,  ..., -0.2175,  0.5992,  0.0381],\n",
      "         [ 0.6963, -0.2313, -0.3295,  ..., -0.2865,  0.5329, -0.0377],\n",
      "         [ 0.3196, -0.2187, -0.3430,  ..., -0.3468,  0.5875, -0.2478]]],\n",
      "       device='cuda:0'), 'pred_verb_logits': tensor([[[ 0.9059, -0.4457,  0.4983,  ...,  1.3256, -0.1184,  1.8383],\n",
      "         [ 0.5335, -0.7371,  0.3615,  ...,  1.2229, -0.4089,  1.7036],\n",
      "         [ 0.5925, -0.5340,  0.6757,  ...,  1.0921, -0.3395,  1.6468],\n",
      "         ...,\n",
      "         [ 0.4838, -0.7860,  0.4851,  ...,  1.3486, -0.5140,  1.5243],\n",
      "         [ 0.6353, -0.6203,  0.7089,  ...,  1.6146, -0.3425,  1.5665],\n",
      "         [ 0.6746, -0.5771,  0.5808,  ...,  1.2227, -0.5353,  1.7201]],\n",
      "\n",
      "        [[ 0.4145, -0.9141,  0.5334,  ...,  1.0794, -0.4564,  1.6878],\n",
      "         [ 0.6068, -0.9295,  0.4182,  ...,  1.1485, -0.3485,  1.6554],\n",
      "         [ 0.5267, -0.6692,  0.4802,  ...,  1.2386, -0.1756,  1.6414],\n",
      "         ...,\n",
      "         [ 0.6876, -0.7070,  0.5699,  ...,  1.3577, -0.2937,  1.8549],\n",
      "         [ 0.7704, -0.9509,  0.4802,  ...,  1.3763, -0.4412,  1.5050],\n",
      "         [ 0.9243, -0.8020,  0.4409,  ...,  1.2791, -0.2167,  1.3246]]],\n",
      "       device='cuda:0'), 'pred_sub_boxes': tensor([[[0.4880, 0.5362, 0.4898, 0.5266],\n",
      "         [0.4808, 0.5311, 0.4811, 0.5193],\n",
      "         [0.4780, 0.5281, 0.4837, 0.5183],\n",
      "         [0.4965, 0.5229, 0.4946, 0.5225],\n",
      "         [0.4795, 0.5370, 0.4775, 0.5221],\n",
      "         [0.4847, 0.5322, 0.4778, 0.5157],\n",
      "         [0.4747, 0.5259, 0.4870, 0.5163],\n",
      "         [0.4914, 0.5229, 0.4875, 0.5219],\n",
      "         [0.5016, 0.5325, 0.4881, 0.5293],\n",
      "         [0.4793, 0.5265, 0.4927, 0.5147],\n",
      "         [0.4895, 0.5337, 0.4855, 0.5305],\n",
      "         [0.4859, 0.5411, 0.4864, 0.5299],\n",
      "         [0.4818, 0.5392, 0.4765, 0.5216],\n",
      "         [0.4787, 0.5213, 0.4821, 0.5143],\n",
      "         [0.4883, 0.5406, 0.4921, 0.5316],\n",
      "         [0.4908, 0.5345, 0.4860, 0.5195],\n",
      "         [0.4865, 0.5231, 0.4836, 0.5196],\n",
      "         [0.4913, 0.5286, 0.4893, 0.5223],\n",
      "         [0.4880, 0.5316, 0.4823, 0.5170],\n",
      "         [0.4841, 0.5317, 0.4824, 0.5202],\n",
      "         [0.4776, 0.5332, 0.4868, 0.5188],\n",
      "         [0.4932, 0.5303, 0.4927, 0.5278],\n",
      "         [0.4897, 0.5414, 0.4908, 0.5238],\n",
      "         [0.4867, 0.5336, 0.4796, 0.5203],\n",
      "         [0.4787, 0.5287, 0.4830, 0.5179],\n",
      "         [0.4917, 0.5345, 0.4852, 0.5174],\n",
      "         [0.4882, 0.5182, 0.4843, 0.5107],\n",
      "         [0.4915, 0.5287, 0.4904, 0.5174],\n",
      "         [0.4880, 0.5258, 0.4883, 0.5302],\n",
      "         [0.4811, 0.5284, 0.4760, 0.5279],\n",
      "         [0.4892, 0.5429, 0.4857, 0.5308],\n",
      "         [0.4911, 0.5353, 0.4837, 0.5253],\n",
      "         [0.4828, 0.5258, 0.4820, 0.5283],\n",
      "         [0.4860, 0.5282, 0.4778, 0.5162],\n",
      "         [0.4783, 0.5365, 0.4893, 0.5274],\n",
      "         [0.4899, 0.5298, 0.4823, 0.5172],\n",
      "         [0.4749, 0.5325, 0.4858, 0.5160],\n",
      "         [0.4847, 0.5214, 0.4826, 0.5161],\n",
      "         [0.4888, 0.5215, 0.4882, 0.5243],\n",
      "         [0.4852, 0.5288, 0.4820, 0.5259],\n",
      "         [0.4872, 0.5343, 0.4837, 0.5179],\n",
      "         [0.4817, 0.5312, 0.4897, 0.5278],\n",
      "         [0.4851, 0.5310, 0.4863, 0.5227],\n",
      "         [0.4846, 0.5283, 0.4738, 0.5234],\n",
      "         [0.4869, 0.5321, 0.4898, 0.5293],\n",
      "         [0.4897, 0.5363, 0.4770, 0.5192],\n",
      "         [0.4838, 0.5367, 0.4968, 0.5151],\n",
      "         [0.4882, 0.5258, 0.4907, 0.5128],\n",
      "         [0.4859, 0.5244, 0.4903, 0.5171],\n",
      "         [0.4924, 0.5352, 0.4755, 0.5161],\n",
      "         [0.4862, 0.5308, 0.4807, 0.5278],\n",
      "         [0.4829, 0.5333, 0.4899, 0.5102],\n",
      "         [0.4810, 0.5247, 0.4796, 0.5259],\n",
      "         [0.4879, 0.5319, 0.4726, 0.5179],\n",
      "         [0.4841, 0.5320, 0.4825, 0.5192],\n",
      "         [0.4919, 0.5380, 0.4738, 0.5169],\n",
      "         [0.4848, 0.5251, 0.4802, 0.5316],\n",
      "         [0.4886, 0.5342, 0.4807, 0.5221],\n",
      "         [0.4825, 0.5318, 0.4843, 0.5267],\n",
      "         [0.4815, 0.5329, 0.4908, 0.5217],\n",
      "         [0.4861, 0.5355, 0.4869, 0.5201],\n",
      "         [0.4765, 0.5305, 0.4788, 0.5336],\n",
      "         [0.4861, 0.5260, 0.4763, 0.5186],\n",
      "         [0.4848, 0.5297, 0.4909, 0.5176],\n",
      "         [0.4768, 0.5404, 0.4845, 0.5141],\n",
      "         [0.4941, 0.5250, 0.4894, 0.5255],\n",
      "         [0.4884, 0.5307, 0.4804, 0.5181],\n",
      "         [0.4909, 0.5273, 0.4866, 0.5216],\n",
      "         [0.4828, 0.5364, 0.4880, 0.5257],\n",
      "         [0.4847, 0.5240, 0.4748, 0.5268],\n",
      "         [0.4954, 0.5368, 0.4855, 0.5068],\n",
      "         [0.4870, 0.5312, 0.4917, 0.5282],\n",
      "         [0.4937, 0.5308, 0.4846, 0.5234],\n",
      "         [0.4771, 0.5329, 0.4733, 0.5108],\n",
      "         [0.4863, 0.5284, 0.4844, 0.5310],\n",
      "         [0.4864, 0.5329, 0.4827, 0.5247],\n",
      "         [0.4846, 0.5423, 0.4914, 0.5292],\n",
      "         [0.4930, 0.5275, 0.4816, 0.5228],\n",
      "         [0.4894, 0.5424, 0.4927, 0.5102],\n",
      "         [0.4806, 0.5330, 0.4866, 0.5152],\n",
      "         [0.4898, 0.5341, 0.4875, 0.5224],\n",
      "         [0.4846, 0.5313, 0.4906, 0.5192],\n",
      "         [0.4857, 0.5297, 0.4836, 0.5278],\n",
      "         [0.4819, 0.5359, 0.4846, 0.5271],\n",
      "         [0.4847, 0.5362, 0.4883, 0.5262],\n",
      "         [0.4847, 0.5219, 0.4769, 0.5275],\n",
      "         [0.4824, 0.5406, 0.4933, 0.5181],\n",
      "         [0.4911, 0.5273, 0.4864, 0.5238],\n",
      "         [0.4747, 0.5352, 0.4697, 0.5193],\n",
      "         [0.4938, 0.5364, 0.4901, 0.5163],\n",
      "         [0.4785, 0.5298, 0.4855, 0.5180],\n",
      "         [0.4895, 0.5287, 0.4902, 0.5264],\n",
      "         [0.4827, 0.5357, 0.4855, 0.5263],\n",
      "         [0.4815, 0.5258, 0.4852, 0.5292],\n",
      "         [0.4873, 0.5355, 0.4942, 0.5203],\n",
      "         [0.4803, 0.5279, 0.4853, 0.5204],\n",
      "         [0.4843, 0.5338, 0.4890, 0.5208],\n",
      "         [0.4878, 0.5248, 0.4834, 0.5246],\n",
      "         [0.4839, 0.5418, 0.4812, 0.5101],\n",
      "         [0.4934, 0.5414, 0.4883, 0.5129]],\n",
      "\n",
      "        [[0.4857, 0.5374, 0.4887, 0.5283],\n",
      "         [0.4825, 0.5411, 0.4906, 0.5212],\n",
      "         [0.4905, 0.5327, 0.4817, 0.5214],\n",
      "         [0.4805, 0.5310, 0.4792, 0.5295],\n",
      "         [0.4777, 0.5257, 0.4865, 0.5340],\n",
      "         [0.4828, 0.5306, 0.4852, 0.5300],\n",
      "         [0.4894, 0.5296, 0.4814, 0.5256],\n",
      "         [0.4844, 0.5285, 0.4850, 0.5145],\n",
      "         [0.4906, 0.5376, 0.4903, 0.5292],\n",
      "         [0.4849, 0.5390, 0.4861, 0.5270],\n",
      "         [0.4882, 0.5300, 0.4902, 0.5174],\n",
      "         [0.4842, 0.5357, 0.4950, 0.5223],\n",
      "         [0.4846, 0.5358, 0.4885, 0.5214],\n",
      "         [0.5015, 0.5453, 0.4928, 0.5343],\n",
      "         [0.4851, 0.5285, 0.4866, 0.5290],\n",
      "         [0.4856, 0.5410, 0.4786, 0.5159],\n",
      "         [0.4778, 0.5283, 0.4998, 0.5221],\n",
      "         [0.4884, 0.5370, 0.4827, 0.5276],\n",
      "         [0.4833, 0.5321, 0.4928, 0.5198],\n",
      "         [0.4831, 0.5338, 0.4846, 0.5337],\n",
      "         [0.4906, 0.5276, 0.4812, 0.5269],\n",
      "         [0.4815, 0.5335, 0.4854, 0.5139],\n",
      "         [0.4833, 0.5362, 0.4862, 0.5219],\n",
      "         [0.4857, 0.5306, 0.4816, 0.5262],\n",
      "         [0.4795, 0.5360, 0.4894, 0.5288],\n",
      "         [0.4840, 0.5320, 0.4901, 0.5271],\n",
      "         [0.4926, 0.5255, 0.4828, 0.5241],\n",
      "         [0.4833, 0.5370, 0.4977, 0.5183],\n",
      "         [0.4878, 0.5307, 0.4928, 0.5320],\n",
      "         [0.4957, 0.5506, 0.4804, 0.5178],\n",
      "         [0.4830, 0.5312, 0.4877, 0.5227],\n",
      "         [0.4909, 0.5195, 0.4869, 0.5335],\n",
      "         [0.4819, 0.5286, 0.4865, 0.5377],\n",
      "         [0.4908, 0.5330, 0.4983, 0.5249],\n",
      "         [0.4895, 0.5245, 0.4860, 0.5183],\n",
      "         [0.4953, 0.5212, 0.4822, 0.5238],\n",
      "         [0.4853, 0.5352, 0.4868, 0.5173],\n",
      "         [0.4924, 0.5369, 0.4860, 0.5412],\n",
      "         [0.4749, 0.5277, 0.4805, 0.5206],\n",
      "         [0.4797, 0.5414, 0.4847, 0.5273],\n",
      "         [0.4913, 0.5368, 0.4856, 0.5303],\n",
      "         [0.4843, 0.5474, 0.4857, 0.5213],\n",
      "         [0.4856, 0.5309, 0.4868, 0.5297],\n",
      "         [0.4835, 0.5415, 0.4909, 0.5259],\n",
      "         [0.4884, 0.5306, 0.4829, 0.5199],\n",
      "         [0.4830, 0.5378, 0.4744, 0.5221],\n",
      "         [0.4904, 0.5330, 0.4796, 0.5253],\n",
      "         [0.4886, 0.5430, 0.4907, 0.5212],\n",
      "         [0.4763, 0.5411, 0.4792, 0.5264],\n",
      "         [0.4910, 0.5311, 0.4787, 0.5154],\n",
      "         [0.4833, 0.5314, 0.4909, 0.5234],\n",
      "         [0.4856, 0.5365, 0.4827, 0.5214],\n",
      "         [0.4827, 0.5354, 0.4878, 0.5302],\n",
      "         [0.4951, 0.5322, 0.4854, 0.5301],\n",
      "         [0.4899, 0.5427, 0.4787, 0.5265],\n",
      "         [0.4976, 0.5424, 0.4948, 0.5349],\n",
      "         [0.4796, 0.5287, 0.4839, 0.5204],\n",
      "         [0.4867, 0.5447, 0.4893, 0.5150],\n",
      "         [0.4733, 0.5309, 0.4960, 0.5271],\n",
      "         [0.4843, 0.5332, 0.4953, 0.5213],\n",
      "         [0.4901, 0.5357, 0.4860, 0.5238],\n",
      "         [0.5016, 0.5306, 0.4921, 0.5310],\n",
      "         [0.4816, 0.5319, 0.4743, 0.5242],\n",
      "         [0.4877, 0.5409, 0.4904, 0.5228],\n",
      "         [0.4885, 0.5439, 0.4807, 0.5201],\n",
      "         [0.4820, 0.5346, 0.4803, 0.5182],\n",
      "         [0.5018, 0.5273, 0.4808, 0.5259],\n",
      "         [0.4770, 0.5264, 0.4846, 0.5182],\n",
      "         [0.4859, 0.5370, 0.4807, 0.5339],\n",
      "         [0.4905, 0.5320, 0.4885, 0.5298],\n",
      "         [0.4838, 0.5406, 0.4956, 0.5257],\n",
      "         [0.4882, 0.5329, 0.4848, 0.5278],\n",
      "         [0.4884, 0.5404, 0.4755, 0.5171],\n",
      "         [0.4825, 0.5285, 0.4833, 0.5203],\n",
      "         [0.4821, 0.5215, 0.4832, 0.5258],\n",
      "         [0.4910, 0.5351, 0.4881, 0.5231],\n",
      "         [0.4851, 0.5336, 0.4943, 0.5267],\n",
      "         [0.4808, 0.5360, 0.4764, 0.5262],\n",
      "         [0.4914, 0.5313, 0.4724, 0.5325],\n",
      "         [0.4882, 0.5277, 0.4914, 0.5296],\n",
      "         [0.4947, 0.5291, 0.4899, 0.5154],\n",
      "         [0.4924, 0.5270, 0.4890, 0.5300],\n",
      "         [0.4885, 0.5350, 0.4800, 0.5356],\n",
      "         [0.4814, 0.5341, 0.4850, 0.5312],\n",
      "         [0.4945, 0.5382, 0.4889, 0.5205],\n",
      "         [0.4841, 0.5323, 0.4839, 0.5214],\n",
      "         [0.4891, 0.5283, 0.4775, 0.5254],\n",
      "         [0.4939, 0.5349, 0.4878, 0.5239],\n",
      "         [0.4778, 0.5419, 0.4966, 0.5227],\n",
      "         [0.4881, 0.5341, 0.4840, 0.5205],\n",
      "         [0.4736, 0.5350, 0.4967, 0.5202],\n",
      "         [0.4759, 0.5385, 0.4854, 0.5230],\n",
      "         [0.4756, 0.5390, 0.4859, 0.5268],\n",
      "         [0.4868, 0.5328, 0.4856, 0.5223],\n",
      "         [0.4838, 0.5264, 0.4864, 0.5289],\n",
      "         [0.4812, 0.5394, 0.4839, 0.5252],\n",
      "         [0.4905, 0.5296, 0.4812, 0.5292],\n",
      "         [0.4843, 0.5385, 0.4867, 0.5156],\n",
      "         [0.4862, 0.5388, 0.4936, 0.5240],\n",
      "         [0.4930, 0.5354, 0.4927, 0.5241]]], device='cuda:0'), 'pred_obj_boxes': tensor([[[0.4997, 0.5366, 0.5007, 0.5203],\n",
      "         [0.5202, 0.5375, 0.4868, 0.5168],\n",
      "         [0.5129, 0.5407, 0.5038, 0.5091],\n",
      "         [0.5105, 0.5495, 0.5002, 0.5149],\n",
      "         [0.5168, 0.5364, 0.4983, 0.5122],\n",
      "         [0.5125, 0.5332, 0.4990, 0.5129],\n",
      "         [0.5192, 0.5372, 0.4972, 0.5176],\n",
      "         [0.5056, 0.5411, 0.4993, 0.5145],\n",
      "         [0.5048, 0.5338, 0.5003, 0.5100],\n",
      "         [0.5103, 0.5371, 0.4996, 0.5107],\n",
      "         [0.5156, 0.5425, 0.4997, 0.5061],\n",
      "         [0.5029, 0.5367, 0.4917, 0.5092],\n",
      "         [0.5088, 0.5346, 0.5009, 0.5178],\n",
      "         [0.5062, 0.5391, 0.4927, 0.5192],\n",
      "         [0.5113, 0.5368, 0.4920, 0.5131],\n",
      "         [0.5117, 0.5375, 0.5085, 0.5154],\n",
      "         [0.5062, 0.5416, 0.4984, 0.5087],\n",
      "         [0.5072, 0.5356, 0.5004, 0.5291],\n",
      "         [0.5104, 0.5443, 0.4975, 0.5091],\n",
      "         [0.5119, 0.5477, 0.5062, 0.5183],\n",
      "         [0.5044, 0.5403, 0.4931, 0.5093],\n",
      "         [0.5068, 0.5388, 0.4920, 0.5152],\n",
      "         [0.5060, 0.5459, 0.5080, 0.5079],\n",
      "         [0.5040, 0.5342, 0.4941, 0.5091],\n",
      "         [0.5170, 0.5417, 0.5010, 0.5084],\n",
      "         [0.5087, 0.5397, 0.5025, 0.5111],\n",
      "         [0.5122, 0.5435, 0.4935, 0.5105],\n",
      "         [0.5085, 0.5371, 0.4969, 0.5081],\n",
      "         [0.5051, 0.5300, 0.4936, 0.5147],\n",
      "         [0.5105, 0.5309, 0.4996, 0.5097],\n",
      "         [0.5076, 0.5416, 0.4927, 0.5054],\n",
      "         [0.5100, 0.5367, 0.4941, 0.5138],\n",
      "         [0.5096, 0.5392, 0.4979, 0.5102],\n",
      "         [0.5108, 0.5336, 0.4943, 0.5171],\n",
      "         [0.5192, 0.5358, 0.4932, 0.5178],\n",
      "         [0.5118, 0.5437, 0.4966, 0.5076],\n",
      "         [0.5124, 0.5404, 0.5016, 0.5110],\n",
      "         [0.5124, 0.5434, 0.4986, 0.5123],\n",
      "         [0.5165, 0.5386, 0.4965, 0.5070],\n",
      "         [0.5062, 0.5448, 0.4938, 0.5126],\n",
      "         [0.5113, 0.5389, 0.5012, 0.5135],\n",
      "         [0.5044, 0.5404, 0.5033, 0.5096],\n",
      "         [0.5172, 0.5449, 0.4999, 0.5196],\n",
      "         [0.5081, 0.5381, 0.4985, 0.5087],\n",
      "         [0.5046, 0.5378, 0.4945, 0.5065],\n",
      "         [0.5061, 0.5473, 0.5033, 0.5067],\n",
      "         [0.5026, 0.5436, 0.5004, 0.5117],\n",
      "         [0.5107, 0.5463, 0.5023, 0.4936],\n",
      "         [0.5014, 0.5361, 0.4906, 0.5187],\n",
      "         [0.5059, 0.5354, 0.4935, 0.5136],\n",
      "         [0.5142, 0.5376, 0.4993, 0.5115],\n",
      "         [0.5080, 0.5382, 0.4984, 0.5111],\n",
      "         [0.5116, 0.5306, 0.4937, 0.5126],\n",
      "         [0.5104, 0.5357, 0.4908, 0.5028],\n",
      "         [0.5119, 0.5394, 0.4965, 0.5101],\n",
      "         [0.5040, 0.5281, 0.4907, 0.5173],\n",
      "         [0.5112, 0.5312, 0.5024, 0.5091],\n",
      "         [0.5033, 0.5388, 0.5042, 0.5078],\n",
      "         [0.5075, 0.5356, 0.5017, 0.5155],\n",
      "         [0.5082, 0.5345, 0.5037, 0.5137],\n",
      "         [0.5212, 0.5412, 0.5045, 0.5106],\n",
      "         [0.5193, 0.5309, 0.5099, 0.5134],\n",
      "         [0.5068, 0.5356, 0.4948, 0.5128],\n",
      "         [0.5120, 0.5291, 0.5005, 0.5092],\n",
      "         [0.5150, 0.5317, 0.4893, 0.5096],\n",
      "         [0.5079, 0.5322, 0.4972, 0.5113],\n",
      "         [0.5102, 0.5452, 0.4966, 0.5106],\n",
      "         [0.5083, 0.5395, 0.4936, 0.5109],\n",
      "         [0.5115, 0.5421, 0.4996, 0.5084],\n",
      "         [0.5003, 0.5345, 0.4869, 0.5048],\n",
      "         [0.4924, 0.5435, 0.4985, 0.5017],\n",
      "         [0.5001, 0.5503, 0.5011, 0.5123],\n",
      "         [0.5075, 0.5362, 0.4966, 0.5180],\n",
      "         [0.5241, 0.5352, 0.4960, 0.5134],\n",
      "         [0.5124, 0.5468, 0.5102, 0.5212],\n",
      "         [0.5121, 0.5338, 0.4954, 0.5060],\n",
      "         [0.5158, 0.5327, 0.5027, 0.5202],\n",
      "         [0.5140, 0.5429, 0.4994, 0.5058],\n",
      "         [0.5127, 0.5373, 0.5027, 0.5166],\n",
      "         [0.5122, 0.5461, 0.4889, 0.5097],\n",
      "         [0.5152, 0.5359, 0.5011, 0.5160],\n",
      "         [0.5156, 0.5364, 0.4875, 0.5277],\n",
      "         [0.5068, 0.5400, 0.4931, 0.5040],\n",
      "         [0.5114, 0.5350, 0.5011, 0.5103],\n",
      "         [0.5055, 0.5460, 0.5091, 0.5119],\n",
      "         [0.5046, 0.5318, 0.5002, 0.5093],\n",
      "         [0.5094, 0.5410, 0.4909, 0.5132],\n",
      "         [0.5100, 0.5359, 0.5040, 0.5077],\n",
      "         [0.5153, 0.5389, 0.4967, 0.5158],\n",
      "         [0.5097, 0.5266, 0.4931, 0.5177],\n",
      "         [0.5074, 0.5477, 0.5091, 0.5068],\n",
      "         [0.5111, 0.5461, 0.5055, 0.5184],\n",
      "         [0.5211, 0.5421, 0.5004, 0.5172],\n",
      "         [0.5115, 0.5394, 0.5094, 0.5099],\n",
      "         [0.5121, 0.5313, 0.4971, 0.5071],\n",
      "         [0.5069, 0.5394, 0.4914, 0.5131],\n",
      "         [0.5036, 0.5366, 0.4991, 0.5145],\n",
      "         [0.5119, 0.5346, 0.4832, 0.5093],\n",
      "         [0.5148, 0.5317, 0.4962, 0.5099],\n",
      "         [0.5068, 0.5348, 0.4937, 0.5128]],\n",
      "\n",
      "        [[0.5143, 0.5329, 0.4903, 0.5083],\n",
      "         [0.4992, 0.5362, 0.4887, 0.4917],\n",
      "         [0.5042, 0.5439, 0.5020, 0.5028],\n",
      "         [0.5110, 0.5278, 0.5000, 0.4975],\n",
      "         [0.5134, 0.5266, 0.4954, 0.5124],\n",
      "         [0.5070, 0.5384, 0.4893, 0.5038],\n",
      "         [0.5077, 0.5356, 0.4993, 0.5025],\n",
      "         [0.5055, 0.5393, 0.5012, 0.5127],\n",
      "         [0.5152, 0.5342, 0.5054, 0.5068],\n",
      "         [0.5141, 0.5316, 0.4869, 0.5156],\n",
      "         [0.4992, 0.5363, 0.4915, 0.4999],\n",
      "         [0.5069, 0.5386, 0.5018, 0.5075],\n",
      "         [0.5072, 0.5321, 0.4922, 0.5041],\n",
      "         [0.5177, 0.5411, 0.5059, 0.5037],\n",
      "         [0.5038, 0.5346, 0.5014, 0.5031],\n",
      "         [0.5038, 0.5394, 0.4965, 0.5023],\n",
      "         [0.5090, 0.5380, 0.4969, 0.4991],\n",
      "         [0.5024, 0.5390, 0.4975, 0.5008],\n",
      "         [0.5035, 0.5388, 0.4978, 0.5134],\n",
      "         [0.5122, 0.5361, 0.4912, 0.5000],\n",
      "         [0.5171, 0.5359, 0.4960, 0.5100],\n",
      "         [0.5107, 0.5220, 0.4990, 0.5114],\n",
      "         [0.5155, 0.5276, 0.4959, 0.5059],\n",
      "         [0.5169, 0.5372, 0.4935, 0.5049],\n",
      "         [0.5140, 0.5410, 0.4975, 0.5020],\n",
      "         [0.5004, 0.5315, 0.4889, 0.5120],\n",
      "         [0.5117, 0.5376, 0.5031, 0.5054],\n",
      "         [0.5192, 0.5311, 0.4985, 0.5012],\n",
      "         [0.5071, 0.5298, 0.5080, 0.5040],\n",
      "         [0.5059, 0.5401, 0.5010, 0.5046],\n",
      "         [0.5095, 0.5349, 0.5036, 0.5018],\n",
      "         [0.5109, 0.5361, 0.4947, 0.5160],\n",
      "         [0.5018, 0.5415, 0.5037, 0.5032],\n",
      "         [0.5053, 0.5386, 0.4999, 0.5032],\n",
      "         [0.5092, 0.5362, 0.4982, 0.5030],\n",
      "         [0.4982, 0.5296, 0.4900, 0.5031],\n",
      "         [0.5021, 0.5369, 0.5063, 0.5032],\n",
      "         [0.5063, 0.5253, 0.4953, 0.5011],\n",
      "         [0.5168, 0.5312, 0.4925, 0.5139],\n",
      "         [0.5108, 0.5337, 0.4911, 0.5151],\n",
      "         [0.5098, 0.5312, 0.5016, 0.5170],\n",
      "         [0.5044, 0.5387, 0.5000, 0.5038],\n",
      "         [0.5047, 0.5391, 0.5029, 0.5084],\n",
      "         [0.5160, 0.5316, 0.4866, 0.5025],\n",
      "         [0.5167, 0.5244, 0.5008, 0.5063],\n",
      "         [0.5074, 0.5337, 0.4981, 0.5034],\n",
      "         [0.5080, 0.5235, 0.4944, 0.5043],\n",
      "         [0.5029, 0.5476, 0.5016, 0.5018],\n",
      "         [0.5185, 0.5380, 0.5038, 0.5147],\n",
      "         [0.5071, 0.5362, 0.4911, 0.5061],\n",
      "         [0.5056, 0.5338, 0.5010, 0.5060],\n",
      "         [0.5107, 0.5385, 0.4931, 0.4998],\n",
      "         [0.5120, 0.5375, 0.5009, 0.5081],\n",
      "         [0.5046, 0.5343, 0.4928, 0.5150],\n",
      "         [0.5126, 0.5390, 0.4968, 0.5068],\n",
      "         [0.5183, 0.5309, 0.5003, 0.5049],\n",
      "         [0.5129, 0.5228, 0.4933, 0.4993],\n",
      "         [0.5042, 0.5395, 0.4909, 0.5026],\n",
      "         [0.5094, 0.5353, 0.5008, 0.5004],\n",
      "         [0.5137, 0.5316, 0.4941, 0.5081],\n",
      "         [0.5143, 0.5290, 0.4937, 0.4961],\n",
      "         [0.5059, 0.5394, 0.5015, 0.5036],\n",
      "         [0.5109, 0.5374, 0.5039, 0.5058],\n",
      "         [0.5098, 0.5347, 0.4901, 0.5058],\n",
      "         [0.5079, 0.5359, 0.5025, 0.5065],\n",
      "         [0.5142, 0.5256, 0.4843, 0.5076],\n",
      "         [0.5012, 0.5380, 0.4972, 0.4971],\n",
      "         [0.5032, 0.5359, 0.4899, 0.5050],\n",
      "         [0.5148, 0.5280, 0.4980, 0.5079],\n",
      "         [0.5088, 0.5280, 0.4982, 0.5097],\n",
      "         [0.5031, 0.5337, 0.5035, 0.5059],\n",
      "         [0.5144, 0.5301, 0.4988, 0.5075],\n",
      "         [0.5090, 0.5295, 0.4941, 0.5054],\n",
      "         [0.5114, 0.5301, 0.4941, 0.4999],\n",
      "         [0.5117, 0.5323, 0.4890, 0.4983],\n",
      "         [0.5104, 0.5361, 0.5014, 0.5095],\n",
      "         [0.5033, 0.5345, 0.5118, 0.5083],\n",
      "         [0.5205, 0.5314, 0.4841, 0.5012],\n",
      "         [0.5077, 0.5371, 0.4924, 0.5063],\n",
      "         [0.5124, 0.5297, 0.4978, 0.5052],\n",
      "         [0.5112, 0.5308, 0.5026, 0.5046],\n",
      "         [0.5023, 0.5369, 0.4939, 0.5020],\n",
      "         [0.5123, 0.5372, 0.5023, 0.5125],\n",
      "         [0.5151, 0.5358, 0.4915, 0.5025],\n",
      "         [0.5032, 0.5370, 0.4876, 0.5021],\n",
      "         [0.5087, 0.5358, 0.5039, 0.5039],\n",
      "         [0.5096, 0.5288, 0.4998, 0.5106],\n",
      "         [0.5109, 0.5330, 0.5002, 0.5079],\n",
      "         [0.5024, 0.5353, 0.5002, 0.4953],\n",
      "         [0.5150, 0.5373, 0.4944, 0.5079],\n",
      "         [0.5107, 0.5376, 0.4969, 0.5051],\n",
      "         [0.5188, 0.5398, 0.5028, 0.5020],\n",
      "         [0.5087, 0.5299, 0.4863, 0.5045],\n",
      "         [0.4985, 0.5409, 0.4988, 0.5023],\n",
      "         [0.5020, 0.5317, 0.5044, 0.5120],\n",
      "         [0.5116, 0.5353, 0.4940, 0.5022],\n",
      "         [0.5108, 0.5293, 0.4992, 0.5067],\n",
      "         [0.5120, 0.5250, 0.5024, 0.4986],\n",
      "         [0.5052, 0.5354, 0.5013, 0.5059],\n",
      "         [0.5116, 0.5391, 0.4929, 0.4976]]], device='cuda:0'), 'aux_outputs': [{'pred_obj_logits': tensor([[[ 0.7545, -0.1811, -0.8257,  ..., -0.2485,  0.4605,  0.0736],\n",
      "         [ 0.4940, -0.2584, -0.6610,  ..., -0.2543,  0.2392,  0.0705],\n",
      "         [ 0.5162, -0.3268, -0.6754,  ..., -0.2890,  0.5491,  0.0536],\n",
      "         ...,\n",
      "         [ 0.7119, -0.2208, -0.7782,  ..., -0.2959,  0.1706,  0.2995],\n",
      "         [ 0.4348,  0.0148, -0.9789,  ..., -0.4471,  0.4661,  0.2697],\n",
      "         [ 0.2294,  0.0791, -0.6318,  ..., -0.3255,  0.3912,  0.3045]],\n",
      "\n",
      "        [[ 0.0388,  0.0262, -0.6183,  ...,  0.0016,  0.4822, -0.2484],\n",
      "         [ 0.4946, -0.3197, -0.8456,  ..., -0.2138,  0.5473,  0.0246],\n",
      "         [ 0.5631, -0.1302, -0.9497,  ..., -0.5029,  0.3766,  0.2039],\n",
      "         ...,\n",
      "         [ 0.4456, -0.1862, -0.9237,  ..., -0.3511,  0.3773, -0.0919],\n",
      "         [ 0.5613, -0.3003, -0.6225,  ...,  0.0454,  0.3610,  0.0540],\n",
      "         [ 0.4867, -0.1230, -0.2595,  ..., -0.4158,  0.3774, -0.0859]]],\n",
      "       device='cuda:0'), 'pred_verb_logits': tensor([[[ 0.6643, -0.2155,  0.2795,  ...,  1.3121, -0.4175,  1.4680],\n",
      "         [ 0.3304,  0.0036, -0.0821,  ...,  1.1730, -0.4144,  1.1098],\n",
      "         [ 0.2608, -0.0821,  0.1094,  ...,  0.8584, -0.6708,  1.1634],\n",
      "         ...,\n",
      "         [ 0.0778, -0.2131,  0.2331,  ...,  1.1674,  0.1228,  1.1246],\n",
      "         [ 0.5820, -0.0264,  0.5236,  ...,  1.2934, -0.1070,  0.8415],\n",
      "         [ 0.4555, -0.0917,  0.3462,  ...,  1.2287, -0.6087,  1.4758]],\n",
      "\n",
      "        [[ 0.0892, -0.2765, -0.0227,  ...,  0.7220, -0.6646,  1.2001],\n",
      "         [ 0.2727, -0.0600,  0.0151,  ...,  0.9451, -0.3865,  1.2985],\n",
      "         [ 0.0973,  0.0857,  0.3500,  ...,  1.3082, -0.4066,  0.9985],\n",
      "         ...,\n",
      "         [ 0.1899,  0.0417, -0.0469,  ...,  0.9595, -0.6585,  1.5252],\n",
      "         [ 0.3192, -0.2562,  0.2968,  ...,  1.0274, -0.4541,  1.4368],\n",
      "         [ 0.5078, -0.2564,  0.1167,  ...,  1.0855, -0.5948,  1.0428]]],\n",
      "       device='cuda:0'), 'pred_sub_boxes': tensor([[[0.4979, 0.5482, 0.4844, 0.5244],\n",
      "         [0.4806, 0.5347, 0.4848, 0.5200],\n",
      "         [0.4851, 0.5359, 0.4812, 0.5286],\n",
      "         [0.5007, 0.5413, 0.4845, 0.5242],\n",
      "         [0.4830, 0.5620, 0.4812, 0.5328],\n",
      "         [0.4734, 0.5477, 0.4835, 0.5245],\n",
      "         [0.4625, 0.5376, 0.4866, 0.5173],\n",
      "         [0.4888, 0.5381, 0.4905, 0.5220],\n",
      "         [0.5014, 0.5390, 0.4941, 0.5212],\n",
      "         [0.4746, 0.5464, 0.4996, 0.5215],\n",
      "         [0.4896, 0.5340, 0.4793, 0.5327],\n",
      "         [0.4788, 0.5569, 0.4918, 0.5267],\n",
      "         [0.4795, 0.5483, 0.4705, 0.5242],\n",
      "         [0.4781, 0.5421, 0.4790, 0.5171],\n",
      "         [0.4839, 0.5534, 0.4983, 0.5259],\n",
      "         [0.5021, 0.5409, 0.4914, 0.5109],\n",
      "         [0.4979, 0.5294, 0.4833, 0.5144],\n",
      "         [0.4860, 0.5289, 0.4884, 0.5178],\n",
      "         [0.4887, 0.5442, 0.4954, 0.5186],\n",
      "         [0.4929, 0.5461, 0.4887, 0.5236],\n",
      "         [0.4903, 0.5442, 0.4904, 0.5210],\n",
      "         [0.4963, 0.5444, 0.4917, 0.5314],\n",
      "         [0.4946, 0.5583, 0.4830, 0.5302],\n",
      "         [0.4881, 0.5273, 0.4808, 0.5144],\n",
      "         [0.4888, 0.5296, 0.4859, 0.5135],\n",
      "         [0.4862, 0.5527, 0.5075, 0.5212],\n",
      "         [0.4759, 0.5395, 0.4968, 0.5188],\n",
      "         [0.4888, 0.5345, 0.4921, 0.5182],\n",
      "         [0.4832, 0.5483, 0.4934, 0.5244],\n",
      "         [0.4892, 0.5359, 0.4884, 0.5372],\n",
      "         [0.4797, 0.5452, 0.4840, 0.5241],\n",
      "         [0.4941, 0.5446, 0.4918, 0.5199],\n",
      "         [0.4866, 0.5426, 0.4927, 0.5159],\n",
      "         [0.4880, 0.5484, 0.4854, 0.5124],\n",
      "         [0.4755, 0.5459, 0.4952, 0.5200],\n",
      "         [0.4903, 0.5357, 0.4747, 0.5186],\n",
      "         [0.4875, 0.5472, 0.4950, 0.5133],\n",
      "         [0.4925, 0.5314, 0.4881, 0.5283],\n",
      "         [0.4852, 0.5457, 0.4911, 0.5222],\n",
      "         [0.4976, 0.5440, 0.4937, 0.5314],\n",
      "         [0.4883, 0.5489, 0.4929, 0.5242],\n",
      "         [0.4777, 0.5390, 0.4938, 0.5302],\n",
      "         [0.4854, 0.5521, 0.5012, 0.5203],\n",
      "         [0.4898, 0.5464, 0.4886, 0.5222],\n",
      "         [0.4923, 0.5475, 0.4884, 0.5360],\n",
      "         [0.4982, 0.5475, 0.4801, 0.5311],\n",
      "         [0.4857, 0.5369, 0.4986, 0.5297],\n",
      "         [0.4835, 0.5369, 0.4989, 0.5213],\n",
      "         [0.4911, 0.5524, 0.4923, 0.5205],\n",
      "         [0.4875, 0.5481, 0.4868, 0.5297],\n",
      "         [0.4825, 0.5463, 0.4857, 0.5304],\n",
      "         [0.4826, 0.5469, 0.4942, 0.5197],\n",
      "         [0.4824, 0.5381, 0.4811, 0.5213],\n",
      "         [0.4964, 0.5453, 0.4830, 0.5131],\n",
      "         [0.4808, 0.5461, 0.4848, 0.5259],\n",
      "         [0.4901, 0.5402, 0.4771, 0.5235],\n",
      "         [0.4859, 0.5456, 0.4817, 0.5327],\n",
      "         [0.4907, 0.5481, 0.4809, 0.5215],\n",
      "         [0.4888, 0.5467, 0.4974, 0.5232],\n",
      "         [0.4819, 0.5496, 0.4814, 0.5285],\n",
      "         [0.4827, 0.5472, 0.5022, 0.5175],\n",
      "         [0.4844, 0.5342, 0.4848, 0.5357],\n",
      "         [0.4878, 0.5496, 0.4879, 0.5106],\n",
      "         [0.4827, 0.5492, 0.4911, 0.5190],\n",
      "         [0.4828, 0.5372, 0.4805, 0.5140],\n",
      "         [0.4982, 0.5406, 0.4877, 0.5325],\n",
      "         [0.4758, 0.5469, 0.4884, 0.5107],\n",
      "         [0.4827, 0.5382, 0.4933, 0.5260],\n",
      "         [0.4843, 0.5412, 0.4882, 0.5344],\n",
      "         [0.4884, 0.5276, 0.4843, 0.5206],\n",
      "         [0.5011, 0.5494, 0.4784, 0.4985],\n",
      "         [0.4932, 0.5366, 0.4817, 0.5240],\n",
      "         [0.4916, 0.5464, 0.4788, 0.5236],\n",
      "         [0.4906, 0.5469, 0.4844, 0.5207],\n",
      "         [0.4956, 0.5515, 0.4933, 0.5165],\n",
      "         [0.4917, 0.5412, 0.4816, 0.5272],\n",
      "         [0.4838, 0.5468, 0.4935, 0.5228],\n",
      "         [0.4844, 0.5440, 0.4863, 0.5270],\n",
      "         [0.4853, 0.5516, 0.4894, 0.5123],\n",
      "         [0.4871, 0.5434, 0.4896, 0.5280],\n",
      "         [0.4872, 0.5330, 0.4958, 0.5222],\n",
      "         [0.4796, 0.5469, 0.5052, 0.5145],\n",
      "         [0.4910, 0.5321, 0.4819, 0.5278],\n",
      "         [0.4865, 0.5544, 0.4891, 0.5199],\n",
      "         [0.4790, 0.5440, 0.4799, 0.5145],\n",
      "         [0.4888, 0.5366, 0.4848, 0.5338],\n",
      "         [0.4883, 0.5410, 0.4945, 0.5178],\n",
      "         [0.4867, 0.5400, 0.4936, 0.5209],\n",
      "         [0.4784, 0.5523, 0.4747, 0.5242],\n",
      "         [0.4821, 0.5474, 0.4961, 0.5095],\n",
      "         [0.4776, 0.5342, 0.4869, 0.5211],\n",
      "         [0.4878, 0.5402, 0.4847, 0.5304],\n",
      "         [0.4805, 0.5419, 0.4864, 0.5222],\n",
      "         [0.4794, 0.5297, 0.4833, 0.5193],\n",
      "         [0.4861, 0.5419, 0.4938, 0.5244],\n",
      "         [0.4921, 0.5403, 0.4846, 0.5128],\n",
      "         [0.4846, 0.5358, 0.4801, 0.5265],\n",
      "         [0.4883, 0.5475, 0.4944, 0.5245],\n",
      "         [0.4893, 0.5466, 0.4932, 0.5113],\n",
      "         [0.4919, 0.5453, 0.4892, 0.5079]],\n",
      "\n",
      "        [[0.4860, 0.5485, 0.4853, 0.5273],\n",
      "         [0.4911, 0.5479, 0.4984, 0.5214],\n",
      "         [0.4976, 0.5395, 0.4894, 0.5291],\n",
      "         [0.4967, 0.5370, 0.4788, 0.5377],\n",
      "         [0.4763, 0.5502, 0.4863, 0.5270],\n",
      "         [0.4916, 0.5513, 0.4808, 0.5224],\n",
      "         [0.4863, 0.5277, 0.4927, 0.5224],\n",
      "         [0.4897, 0.5322, 0.4877, 0.5240],\n",
      "         [0.4835, 0.5388, 0.4870, 0.5357],\n",
      "         [0.4910, 0.5492, 0.4877, 0.5330],\n",
      "         [0.5007, 0.5381, 0.4995, 0.5239],\n",
      "         [0.4913, 0.5499, 0.5009, 0.5311],\n",
      "         [0.4762, 0.5386, 0.4930, 0.5293],\n",
      "         [0.5004, 0.5540, 0.4893, 0.5278],\n",
      "         [0.4896, 0.5443, 0.4866, 0.5333],\n",
      "         [0.4934, 0.5425, 0.4780, 0.5214],\n",
      "         [0.4900, 0.5460, 0.4955, 0.5270],\n",
      "         [0.5023, 0.5556, 0.4824, 0.5386],\n",
      "         [0.4874, 0.5625, 0.4978, 0.5248],\n",
      "         [0.4933, 0.5422, 0.4816, 0.5356],\n",
      "         [0.4883, 0.5510, 0.4896, 0.5260],\n",
      "         [0.4881, 0.5422, 0.4946, 0.5204],\n",
      "         [0.4866, 0.5415, 0.4837, 0.5226],\n",
      "         [0.4811, 0.5445, 0.4985, 0.5146],\n",
      "         [0.4833, 0.5422, 0.4995, 0.5270],\n",
      "         [0.4889, 0.5468, 0.4850, 0.5247],\n",
      "         [0.4893, 0.5508, 0.4854, 0.5159],\n",
      "         [0.4938, 0.5370, 0.4901, 0.5232],\n",
      "         [0.4910, 0.5388, 0.4983, 0.5241],\n",
      "         [0.4971, 0.5521, 0.4942, 0.5296],\n",
      "         [0.4939, 0.5411, 0.4806, 0.5308],\n",
      "         [0.4934, 0.5286, 0.4900, 0.5296],\n",
      "         [0.4877, 0.5519, 0.4865, 0.5300],\n",
      "         [0.4968, 0.5379, 0.4935, 0.5229],\n",
      "         [0.4938, 0.5355, 0.4918, 0.5270],\n",
      "         [0.4911, 0.5345, 0.4867, 0.5191],\n",
      "         [0.4948, 0.5455, 0.4824, 0.5157],\n",
      "         [0.4908, 0.5488, 0.4967, 0.5384],\n",
      "         [0.4808, 0.5357, 0.4848, 0.5369],\n",
      "         [0.4765, 0.5450, 0.4800, 0.5426],\n",
      "         [0.4942, 0.5359, 0.4785, 0.5247],\n",
      "         [0.4839, 0.5506, 0.4903, 0.5154],\n",
      "         [0.4920, 0.5336, 0.4889, 0.5316],\n",
      "         [0.4836, 0.5514, 0.5053, 0.5196],\n",
      "         [0.4933, 0.5407, 0.4902, 0.5277],\n",
      "         [0.4786, 0.5456, 0.4859, 0.5243],\n",
      "         [0.4917, 0.5381, 0.4814, 0.5325],\n",
      "         [0.4900, 0.5565, 0.4995, 0.5124],\n",
      "         [0.4848, 0.5469, 0.4800, 0.5308],\n",
      "         [0.5027, 0.5437, 0.4840, 0.5287],\n",
      "         [0.4696, 0.5394, 0.4819, 0.5076],\n",
      "         [0.4790, 0.5541, 0.4883, 0.5241],\n",
      "         [0.4849, 0.5463, 0.4911, 0.5421],\n",
      "         [0.4979, 0.5446, 0.4873, 0.5311],\n",
      "         [0.4920, 0.5429, 0.4760, 0.5229],\n",
      "         [0.4889, 0.5484, 0.4953, 0.5404],\n",
      "         [0.4822, 0.5513, 0.4777, 0.5171],\n",
      "         [0.4891, 0.5496, 0.4854, 0.5178],\n",
      "         [0.4686, 0.5501, 0.5065, 0.5282],\n",
      "         [0.4914, 0.5413, 0.4898, 0.5128],\n",
      "         [0.4985, 0.5419, 0.4954, 0.5284],\n",
      "         [0.5053, 0.5312, 0.4880, 0.5272],\n",
      "         [0.4863, 0.5430, 0.4847, 0.5417],\n",
      "         [0.4917, 0.5541, 0.4926, 0.5180],\n",
      "         [0.4914, 0.5517, 0.4878, 0.5331],\n",
      "         [0.4823, 0.5447, 0.4777, 0.5202],\n",
      "         [0.5015, 0.5336, 0.4828, 0.5205],\n",
      "         [0.4790, 0.5295, 0.4911, 0.5179],\n",
      "         [0.4974, 0.5466, 0.4932, 0.5284],\n",
      "         [0.4961, 0.5429, 0.4874, 0.5315],\n",
      "         [0.4844, 0.5549, 0.4978, 0.5221],\n",
      "         [0.4934, 0.5330, 0.4867, 0.5310],\n",
      "         [0.4845, 0.5456, 0.4871, 0.5294],\n",
      "         [0.4893, 0.5352, 0.4891, 0.5289],\n",
      "         [0.4956, 0.5291, 0.4835, 0.5225],\n",
      "         [0.4816, 0.5394, 0.4970, 0.5306],\n",
      "         [0.4966, 0.5414, 0.4985, 0.5288],\n",
      "         [0.4933, 0.5403, 0.4863, 0.5239],\n",
      "         [0.4936, 0.5369, 0.4815, 0.5308],\n",
      "         [0.4842, 0.5365, 0.4840, 0.5360],\n",
      "         [0.4906, 0.5474, 0.4912, 0.5224],\n",
      "         [0.4910, 0.5418, 0.4891, 0.5287],\n",
      "         [0.4869, 0.5433, 0.4905, 0.5289],\n",
      "         [0.4789, 0.5464, 0.4931, 0.5347],\n",
      "         [0.4937, 0.5435, 0.4951, 0.5270],\n",
      "         [0.4917, 0.5384, 0.4907, 0.5176],\n",
      "         [0.4833, 0.5417, 0.4792, 0.5348],\n",
      "         [0.4873, 0.5538, 0.4987, 0.5083],\n",
      "         [0.4736, 0.5439, 0.4963, 0.5147],\n",
      "         [0.4973, 0.5397, 0.4955, 0.5409],\n",
      "         [0.4916, 0.5441, 0.5056, 0.5250],\n",
      "         [0.4774, 0.5447, 0.4825, 0.5306],\n",
      "         [0.4820, 0.5479, 0.4921, 0.5325],\n",
      "         [0.4913, 0.5469, 0.4984, 0.5229],\n",
      "         [0.4981, 0.5257, 0.4944, 0.5286],\n",
      "         [0.4882, 0.5360, 0.4812, 0.5374],\n",
      "         [0.4909, 0.5448, 0.4798, 0.5221],\n",
      "         [0.4881, 0.5499, 0.4873, 0.5153],\n",
      "         [0.4950, 0.5446, 0.4967, 0.5271],\n",
      "         [0.4932, 0.5584, 0.4844, 0.5354]]], device='cuda:0'), 'pred_obj_boxes': tensor([[[0.5030, 0.5422, 0.5016, 0.5163],\n",
      "         [0.5152, 0.5385, 0.5027, 0.5065],\n",
      "         [0.5206, 0.5374, 0.5077, 0.5089],\n",
      "         [0.5169, 0.5424, 0.5002, 0.5046],\n",
      "         [0.5147, 0.5249, 0.5045, 0.5231],\n",
      "         [0.5114, 0.5372, 0.5010, 0.5163],\n",
      "         [0.5241, 0.5370, 0.5017, 0.5126],\n",
      "         [0.5038, 0.5334, 0.5052, 0.5075],\n",
      "         [0.5067, 0.5369, 0.5111, 0.5067],\n",
      "         [0.5016, 0.5464, 0.5020, 0.4930],\n",
      "         [0.5267, 0.5488, 0.5097, 0.5109],\n",
      "         [0.5100, 0.5450, 0.4985, 0.5165],\n",
      "         [0.5082, 0.5381, 0.5045, 0.5206],\n",
      "         [0.5136, 0.5286, 0.5003, 0.5130],\n",
      "         [0.5158, 0.5471, 0.5006, 0.5094],\n",
      "         [0.5105, 0.5346, 0.4989, 0.5071],\n",
      "         [0.5119, 0.5428, 0.5078, 0.5004],\n",
      "         [0.5230, 0.5355, 0.5053, 0.5256],\n",
      "         [0.5194, 0.5469, 0.4987, 0.4997],\n",
      "         [0.5123, 0.5372, 0.5081, 0.5222],\n",
      "         [0.4999, 0.5369, 0.5037, 0.5012],\n",
      "         [0.5190, 0.5326, 0.5061, 0.5121],\n",
      "         [0.5062, 0.5433, 0.5010, 0.4995],\n",
      "         [0.5079, 0.5288, 0.5126, 0.5166],\n",
      "         [0.5127, 0.5322, 0.5056, 0.5126],\n",
      "         [0.5037, 0.5403, 0.5137, 0.5165],\n",
      "         [0.5187, 0.5465, 0.5080, 0.5057],\n",
      "         [0.5095, 0.5415, 0.4943, 0.5065],\n",
      "         [0.5105, 0.5360, 0.5014, 0.5095],\n",
      "         [0.5101, 0.5437, 0.5027, 0.5020],\n",
      "         [0.5124, 0.5349, 0.5112, 0.5025],\n",
      "         [0.5068, 0.5297, 0.4931, 0.5097],\n",
      "         [0.5115, 0.5321, 0.5028, 0.5044],\n",
      "         [0.5133, 0.5355, 0.5034, 0.5126],\n",
      "         [0.5252, 0.5483, 0.5031, 0.5066],\n",
      "         [0.5122, 0.5387, 0.5068, 0.5044],\n",
      "         [0.5118, 0.5360, 0.5043, 0.5042],\n",
      "         [0.5275, 0.5490, 0.5083, 0.5055],\n",
      "         [0.5123, 0.5307, 0.5055, 0.5010],\n",
      "         [0.5014, 0.5485, 0.5100, 0.5055],\n",
      "         [0.5151, 0.5403, 0.5055, 0.5027],\n",
      "         [0.5002, 0.5360, 0.5190, 0.5091],\n",
      "         [0.5116, 0.5381, 0.5109, 0.5183],\n",
      "         [0.5058, 0.5309, 0.5096, 0.4988],\n",
      "         [0.5067, 0.5259, 0.5063, 0.5109],\n",
      "         [0.5139, 0.5458, 0.5162, 0.5020],\n",
      "         [0.5037, 0.5401, 0.5157, 0.5038],\n",
      "         [0.5223, 0.5340, 0.5208, 0.4895],\n",
      "         [0.5064, 0.5432, 0.4912, 0.5072],\n",
      "         [0.5138, 0.5308, 0.5091, 0.5143],\n",
      "         [0.5212, 0.5402, 0.5172, 0.5078],\n",
      "         [0.5102, 0.5350, 0.5059, 0.5110],\n",
      "         [0.5188, 0.5287, 0.5028, 0.5121],\n",
      "         [0.5102, 0.5330, 0.4927, 0.5002],\n",
      "         [0.5184, 0.5217, 0.5069, 0.5014],\n",
      "         [0.4990, 0.5398, 0.5045, 0.5210],\n",
      "         [0.5043, 0.5368, 0.5107, 0.5088],\n",
      "         [0.5082, 0.5342, 0.4982, 0.5104],\n",
      "         [0.5047, 0.5365, 0.5189, 0.5185],\n",
      "         [0.5074, 0.5353, 0.5049, 0.5050],\n",
      "         [0.5256, 0.5378, 0.5098, 0.5054],\n",
      "         [0.5275, 0.5254, 0.5243, 0.5167],\n",
      "         [0.5013, 0.5246, 0.4949, 0.4969],\n",
      "         [0.5150, 0.5371, 0.5059, 0.5106],\n",
      "         [0.5075, 0.5396, 0.5031, 0.5065],\n",
      "         [0.5038, 0.5333, 0.5022, 0.5060],\n",
      "         [0.5221, 0.5552, 0.4981, 0.5126],\n",
      "         [0.5126, 0.5370, 0.5049, 0.5105],\n",
      "         [0.5007, 0.5352, 0.5119, 0.5112],\n",
      "         [0.4974, 0.5405, 0.4941, 0.5012],\n",
      "         [0.5088, 0.5357, 0.5069, 0.4987],\n",
      "         [0.5010, 0.5499, 0.5016, 0.5057],\n",
      "         [0.5150, 0.5283, 0.4949, 0.5116],\n",
      "         [0.5152, 0.5302, 0.5073, 0.5012],\n",
      "         [0.5089, 0.5384, 0.5028, 0.5171],\n",
      "         [0.5072, 0.5364, 0.5011, 0.5064],\n",
      "         [0.5244, 0.5399, 0.4951, 0.5086],\n",
      "         [0.5077, 0.5319, 0.4992, 0.5114],\n",
      "         [0.5171, 0.5410, 0.5034, 0.5213],\n",
      "         [0.5052, 0.5432, 0.5022, 0.4940],\n",
      "         [0.5054, 0.5474, 0.5059, 0.5015],\n",
      "         [0.5214, 0.5418, 0.5024, 0.5160],\n",
      "         [0.5064, 0.5405, 0.5052, 0.5025],\n",
      "         [0.5132, 0.5359, 0.5004, 0.5112],\n",
      "         [0.5138, 0.5547, 0.5248, 0.5114],\n",
      "         [0.5088, 0.5492, 0.5011, 0.5199],\n",
      "         [0.5133, 0.5403, 0.4913, 0.5056],\n",
      "         [0.5053, 0.5368, 0.5039, 0.5107],\n",
      "         [0.5233, 0.5375, 0.5022, 0.5125],\n",
      "         [0.5028, 0.5260, 0.4935, 0.5002],\n",
      "         [0.5115, 0.5442, 0.5107, 0.5153],\n",
      "         [0.5078, 0.5386, 0.5070, 0.5140],\n",
      "         [0.5156, 0.5437, 0.5015, 0.5130],\n",
      "         [0.5059, 0.5413, 0.5017, 0.5107],\n",
      "         [0.5098, 0.5363, 0.5018, 0.5048],\n",
      "         [0.5027, 0.5406, 0.4980, 0.5054],\n",
      "         [0.5038, 0.5376, 0.5137, 0.5064],\n",
      "         [0.5087, 0.5429, 0.5025, 0.5006],\n",
      "         [0.5167, 0.5344, 0.5201, 0.4979],\n",
      "         [0.5101, 0.5399, 0.4910, 0.5047]],\n",
      "\n",
      "        [[0.5166, 0.5427, 0.5077, 0.5029],\n",
      "         [0.4922, 0.5384, 0.4954, 0.4973],\n",
      "         [0.5093, 0.5292, 0.5168, 0.5043],\n",
      "         [0.5199, 0.5288, 0.5051, 0.4977],\n",
      "         [0.5182, 0.5345, 0.5015, 0.5105],\n",
      "         [0.5139, 0.5265, 0.4923, 0.5067],\n",
      "         [0.5216, 0.5402, 0.5049, 0.4925],\n",
      "         [0.5181, 0.5349, 0.5069, 0.5092],\n",
      "         [0.5149, 0.5332, 0.5129, 0.5099],\n",
      "         [0.5206, 0.5301, 0.4847, 0.5086],\n",
      "         [0.4990, 0.5349, 0.4966, 0.4939],\n",
      "         [0.5100, 0.5436, 0.5115, 0.4972],\n",
      "         [0.5239, 0.5282, 0.5031, 0.5135],\n",
      "         [0.5253, 0.5419, 0.5116, 0.5040],\n",
      "         [0.5175, 0.5412, 0.5089, 0.5041],\n",
      "         [0.5013, 0.5261, 0.5058, 0.5032],\n",
      "         [0.5055, 0.5465, 0.5137, 0.4902],\n",
      "         [0.5143, 0.5367, 0.5144, 0.5066],\n",
      "         [0.4991, 0.5249, 0.5086, 0.5114],\n",
      "         [0.5057, 0.5391, 0.5111, 0.4959],\n",
      "         [0.5165, 0.5400, 0.5001, 0.4999],\n",
      "         [0.5067, 0.5324, 0.5021, 0.4953],\n",
      "         [0.5089, 0.5243, 0.4975, 0.5016],\n",
      "         [0.5198, 0.5506, 0.5033, 0.4948],\n",
      "         [0.5125, 0.5349, 0.4963, 0.4897],\n",
      "         [0.5025, 0.5372, 0.5016, 0.4985],\n",
      "         [0.5106, 0.5330, 0.4928, 0.5055],\n",
      "         [0.5091, 0.5350, 0.5009, 0.5025],\n",
      "         [0.5008, 0.5381, 0.5101, 0.4948],\n",
      "         [0.5185, 0.5376, 0.5071, 0.5013],\n",
      "         [0.5102, 0.5380, 0.5165, 0.4988],\n",
      "         [0.5128, 0.5433, 0.4853, 0.5089],\n",
      "         [0.5093, 0.5413, 0.5182, 0.5027],\n",
      "         [0.5058, 0.5369, 0.5007, 0.4931],\n",
      "         [0.5104, 0.5312, 0.4947, 0.5095],\n",
      "         [0.5051, 0.5262, 0.5067, 0.4973],\n",
      "         [0.5220, 0.5338, 0.5124, 0.4911],\n",
      "         [0.5085, 0.5311, 0.5103, 0.4983],\n",
      "         [0.5137, 0.5344, 0.4956, 0.5113],\n",
      "         [0.5207, 0.5378, 0.4949, 0.5114],\n",
      "         [0.5172, 0.5245, 0.5081, 0.5153],\n",
      "         [0.5036, 0.5376, 0.5057, 0.5007],\n",
      "         [0.5075, 0.5416, 0.4980, 0.4933],\n",
      "         [0.5131, 0.5420, 0.5134, 0.4908],\n",
      "         [0.5160, 0.5361, 0.4965, 0.5019],\n",
      "         [0.5138, 0.5354, 0.5006, 0.4999],\n",
      "         [0.5275, 0.5309, 0.5034, 0.5024],\n",
      "         [0.5102, 0.5366, 0.4967, 0.4978],\n",
      "         [0.5138, 0.5463, 0.5064, 0.5025],\n",
      "         [0.5146, 0.5478, 0.4968, 0.4940],\n",
      "         [0.5193, 0.5272, 0.5037, 0.5034],\n",
      "         [0.5163, 0.5315, 0.5138, 0.5029],\n",
      "         [0.5199, 0.5377, 0.5103, 0.5029],\n",
      "         [0.5122, 0.5323, 0.5122, 0.5064],\n",
      "         [0.5094, 0.5399, 0.5032, 0.4995],\n",
      "         [0.5186, 0.5318, 0.5086, 0.4990],\n",
      "         [0.4989, 0.5330, 0.4932, 0.4915],\n",
      "         [0.4971, 0.5287, 0.5036, 0.5021],\n",
      "         [0.5116, 0.5318, 0.5114, 0.5003],\n",
      "         [0.5090, 0.5308, 0.4978, 0.4998],\n",
      "         [0.5077, 0.5438, 0.4996, 0.4990],\n",
      "         [0.5051, 0.5287, 0.5067, 0.5123],\n",
      "         [0.5185, 0.5232, 0.5065, 0.5110],\n",
      "         [0.5120, 0.5409, 0.4853, 0.5061],\n",
      "         [0.5121, 0.5401, 0.5089, 0.4948],\n",
      "         [0.4991, 0.5261, 0.4897, 0.5018],\n",
      "         [0.5113, 0.5405, 0.5092, 0.4990],\n",
      "         [0.5080, 0.5359, 0.4997, 0.5030],\n",
      "         [0.5197, 0.5290, 0.5043, 0.5110],\n",
      "         [0.5136, 0.5343, 0.5047, 0.4982],\n",
      "         [0.4884, 0.5359, 0.5145, 0.5068],\n",
      "         [0.5178, 0.5404, 0.5018, 0.4980],\n",
      "         [0.5016, 0.5383, 0.5025, 0.4878],\n",
      "         [0.5037, 0.5433, 0.5025, 0.5008],\n",
      "         [0.5151, 0.5330, 0.5016, 0.4932],\n",
      "         [0.5020, 0.5339, 0.5068, 0.5227],\n",
      "         [0.5074, 0.5347, 0.5150, 0.4997],\n",
      "         [0.5187, 0.5405, 0.4882, 0.4971],\n",
      "         [0.5159, 0.5399, 0.5033, 0.5013],\n",
      "         [0.5143, 0.5295, 0.5029, 0.5003],\n",
      "         [0.5199, 0.5280, 0.5057, 0.5067],\n",
      "         [0.5200, 0.5367, 0.5028, 0.5073],\n",
      "         [0.5116, 0.5346, 0.5093, 0.4989],\n",
      "         [0.5247, 0.5342, 0.5022, 0.5054],\n",
      "         [0.5033, 0.5382, 0.4977, 0.5042],\n",
      "         [0.5276, 0.5345, 0.5056, 0.4950],\n",
      "         [0.5113, 0.5264, 0.5103, 0.4983],\n",
      "         [0.5296, 0.5355, 0.4934, 0.5020],\n",
      "         [0.4996, 0.5342, 0.5016, 0.5016],\n",
      "         [0.5173, 0.5280, 0.5085, 0.5116],\n",
      "         [0.5125, 0.5415, 0.4920, 0.4986],\n",
      "         [0.5153, 0.5407, 0.5052, 0.4886],\n",
      "         [0.5074, 0.5287, 0.5126, 0.5119],\n",
      "         [0.5090, 0.5377, 0.4986, 0.5034],\n",
      "         [0.4940, 0.5412, 0.4970, 0.5013],\n",
      "         [0.5149, 0.5403, 0.4999, 0.4912],\n",
      "         [0.5127, 0.5268, 0.5109, 0.5046],\n",
      "         [0.5093, 0.5283, 0.4940, 0.4990],\n",
      "         [0.5064, 0.5345, 0.5074, 0.5045],\n",
      "         [0.5097, 0.5357, 0.4935, 0.4930]]], device='cuda:0')}, {'pred_obj_logits': tensor([[[ 0.6058, -0.2263, -0.5464,  ..., -0.2626,  0.3508,  0.0485],\n",
      "         [ 0.4602, -0.2449, -0.4836,  ..., -0.2386,  0.4235,  0.0100],\n",
      "         [ 0.4099, -0.3866, -0.4662,  ..., -0.0497,  0.5760,  0.0580],\n",
      "         ...,\n",
      "         [ 0.6570, -0.2067, -0.5976,  ..., -0.3634,  0.3702,  0.2254],\n",
      "         [ 0.3684, -0.2140, -0.6070,  ..., -0.4148,  0.5992,  0.1694],\n",
      "         [ 0.1935,  0.1323, -0.3741,  ..., -0.2462,  0.4509,  0.3166]],\n",
      "\n",
      "        [[ 0.2629, -0.1388, -0.5695,  ..., -0.2598,  0.5334, -0.1698],\n",
      "         [ 0.2999, -0.2739, -0.8149,  ..., -0.4445,  0.6020, -0.0284],\n",
      "         [ 0.6120, -0.2725, -0.6788,  ..., -0.3382,  0.4259,  0.0490],\n",
      "         ...,\n",
      "         [ 0.4452, -0.0699, -0.7134,  ..., -0.4078,  0.4039, -0.0974],\n",
      "         [ 0.7087, -0.0862, -0.4748,  ..., -0.1851,  0.4203,  0.0899],\n",
      "         [ 0.4506, -0.0994, -0.2895,  ..., -0.3247,  0.4301, -0.2031]]],\n",
      "       device='cuda:0'), 'pred_verb_logits': tensor([[[ 0.9983, -0.4095,  0.4732,  ...,  1.3460, -0.2897,  1.5720],\n",
      "         [ 0.6304, -0.5380,  0.4190,  ...,  1.2828, -0.4124,  1.6448],\n",
      "         [ 0.2655, -0.4300,  0.4204,  ...,  1.0011, -0.6906,  1.5330],\n",
      "         ...,\n",
      "         [ 0.5738, -0.5128,  0.4392,  ...,  1.2301, -0.2719,  1.3988],\n",
      "         [ 0.4338, -0.3940,  0.6519,  ...,  1.4596, -0.3822,  1.4191],\n",
      "         [ 0.6509, -0.4559,  0.6296,  ...,  1.4012, -0.6799,  1.6859]],\n",
      "\n",
      "        [[ 0.2362, -0.7892,  0.4282,  ...,  0.8062, -0.5550,  1.6336],\n",
      "         [ 0.5113, -0.5065,  0.3723,  ...,  1.1261, -0.3149,  1.4495],\n",
      "         [ 0.4609, -0.4361,  0.3562,  ...,  1.3903, -0.3620,  1.4381],\n",
      "         ...,\n",
      "         [ 0.5847, -0.3562,  0.4261,  ...,  1.2400, -0.5064,  1.8029],\n",
      "         [ 0.6829, -0.7557,  0.4860,  ...,  1.4365, -0.4880,  1.4470],\n",
      "         [ 0.8396, -0.5415,  0.5532,  ...,  1.2011, -0.4233,  1.4191]]],\n",
      "       device='cuda:0'), 'pred_sub_boxes': tensor([[[0.4876, 0.5393, 0.4901, 0.5331],\n",
      "         [0.4774, 0.5297, 0.4834, 0.5267],\n",
      "         [0.4810, 0.5371, 0.4820, 0.5240],\n",
      "         [0.5015, 0.5346, 0.4877, 0.5267],\n",
      "         [0.4781, 0.5447, 0.4768, 0.5286],\n",
      "         [0.4735, 0.5387, 0.4769, 0.5215],\n",
      "         [0.4671, 0.5332, 0.4885, 0.5227],\n",
      "         [0.4894, 0.5295, 0.4863, 0.5339],\n",
      "         [0.4986, 0.5362, 0.4885, 0.5188],\n",
      "         [0.4716, 0.5272, 0.4984, 0.5217],\n",
      "         [0.4864, 0.5350, 0.4840, 0.5301],\n",
      "         [0.4783, 0.5434, 0.4867, 0.5290],\n",
      "         [0.4835, 0.5423, 0.4818, 0.5223],\n",
      "         [0.4777, 0.5345, 0.4812, 0.5159],\n",
      "         [0.4896, 0.5448, 0.4949, 0.5310],\n",
      "         [0.4981, 0.5379, 0.4874, 0.5223],\n",
      "         [0.4927, 0.5320, 0.4852, 0.5180],\n",
      "         [0.4898, 0.5345, 0.4894, 0.5234],\n",
      "         [0.4853, 0.5321, 0.4905, 0.5160],\n",
      "         [0.4907, 0.5340, 0.4848, 0.5265],\n",
      "         [0.4821, 0.5379, 0.4865, 0.5246],\n",
      "         [0.4878, 0.5290, 0.4938, 0.5315],\n",
      "         [0.4938, 0.5498, 0.4869, 0.5295],\n",
      "         [0.4860, 0.5209, 0.4785, 0.5209],\n",
      "         [0.4787, 0.5248, 0.4842, 0.5208],\n",
      "         [0.4896, 0.5447, 0.4946, 0.5241],\n",
      "         [0.4899, 0.5245, 0.4949, 0.5160],\n",
      "         [0.4913, 0.5280, 0.4833, 0.5267],\n",
      "         [0.4863, 0.5353, 0.4876, 0.5251],\n",
      "         [0.4875, 0.5341, 0.4817, 0.5250],\n",
      "         [0.4907, 0.5401, 0.4869, 0.5261],\n",
      "         [0.4893, 0.5382, 0.4847, 0.5286],\n",
      "         [0.4871, 0.5328, 0.4810, 0.5242],\n",
      "         [0.4810, 0.5366, 0.4820, 0.5179],\n",
      "         [0.4723, 0.5377, 0.4870, 0.5285],\n",
      "         [0.4902, 0.5285, 0.4741, 0.5157],\n",
      "         [0.4807, 0.5308, 0.4880, 0.5215],\n",
      "         [0.4875, 0.5232, 0.4852, 0.5245],\n",
      "         [0.4845, 0.5323, 0.4826, 0.5215],\n",
      "         [0.4950, 0.5327, 0.4863, 0.5358],\n",
      "         [0.4822, 0.5448, 0.4838, 0.5199],\n",
      "         [0.4775, 0.5352, 0.4938, 0.5264],\n",
      "         [0.4821, 0.5355, 0.4933, 0.5201],\n",
      "         [0.4861, 0.5333, 0.4779, 0.5261],\n",
      "         [0.4958, 0.5358, 0.4835, 0.5347],\n",
      "         [0.4912, 0.5403, 0.4790, 0.5290],\n",
      "         [0.4797, 0.5366, 0.4941, 0.5215],\n",
      "         [0.4868, 0.5325, 0.4962, 0.5178],\n",
      "         [0.4973, 0.5390, 0.4916, 0.5183],\n",
      "         [0.4926, 0.5410, 0.4787, 0.5254],\n",
      "         [0.4816, 0.5359, 0.4835, 0.5321],\n",
      "         [0.4829, 0.5387, 0.4872, 0.5223],\n",
      "         [0.4831, 0.5257, 0.4783, 0.5261],\n",
      "         [0.4900, 0.5353, 0.4758, 0.5211],\n",
      "         [0.4801, 0.5328, 0.4867, 0.5197],\n",
      "         [0.4969, 0.5402, 0.4799, 0.5190],\n",
      "         [0.4871, 0.5281, 0.4775, 0.5363],\n",
      "         [0.4833, 0.5424, 0.4758, 0.5207],\n",
      "         [0.4874, 0.5364, 0.4861, 0.5297],\n",
      "         [0.4836, 0.5419, 0.4883, 0.5298],\n",
      "         [0.4856, 0.5378, 0.4916, 0.5209],\n",
      "         [0.4851, 0.5275, 0.4759, 0.5357],\n",
      "         [0.4867, 0.5370, 0.4817, 0.5163],\n",
      "         [0.4855, 0.5402, 0.4921, 0.5193],\n",
      "         [0.4821, 0.5396, 0.4799, 0.5217],\n",
      "         [0.4966, 0.5272, 0.4896, 0.5323],\n",
      "         [0.4855, 0.5368, 0.4848, 0.5175],\n",
      "         [0.4890, 0.5258, 0.4883, 0.5312],\n",
      "         [0.4889, 0.5379, 0.4868, 0.5364],\n",
      "         [0.4800, 0.5259, 0.4845, 0.5259],\n",
      "         [0.4992, 0.5417, 0.4786, 0.5048],\n",
      "         [0.4879, 0.5360, 0.4888, 0.5271],\n",
      "         [0.4949, 0.5361, 0.4775, 0.5268],\n",
      "         [0.4786, 0.5395, 0.4809, 0.5204],\n",
      "         [0.4918, 0.5352, 0.4854, 0.5253],\n",
      "         [0.4854, 0.5295, 0.4818, 0.5291],\n",
      "         [0.4845, 0.5372, 0.4863, 0.5277],\n",
      "         [0.4903, 0.5374, 0.4828, 0.5238],\n",
      "         [0.4882, 0.5434, 0.4850, 0.5155],\n",
      "         [0.4762, 0.5398, 0.4828, 0.5221],\n",
      "         [0.4830, 0.5298, 0.4941, 0.5206],\n",
      "         [0.4825, 0.5362, 0.5001, 0.5237],\n",
      "         [0.4903, 0.5296, 0.4818, 0.5319],\n",
      "         [0.4794, 0.5381, 0.4879, 0.5248],\n",
      "         [0.4765, 0.5390, 0.4878, 0.5214],\n",
      "         [0.4870, 0.5340, 0.4770, 0.5352],\n",
      "         [0.4892, 0.5402, 0.4872, 0.5176],\n",
      "         [0.4938, 0.5325, 0.4919, 0.5229],\n",
      "         [0.4762, 0.5475, 0.4729, 0.5243],\n",
      "         [0.4924, 0.5441, 0.4984, 0.5098],\n",
      "         [0.4782, 0.5361, 0.4858, 0.5192],\n",
      "         [0.4880, 0.5286, 0.4897, 0.5340],\n",
      "         [0.4817, 0.5406, 0.4824, 0.5304],\n",
      "         [0.4820, 0.5251, 0.4827, 0.5267],\n",
      "         [0.4818, 0.5378, 0.4959, 0.5229],\n",
      "         [0.4819, 0.5313, 0.4821, 0.5175],\n",
      "         [0.4802, 0.5342, 0.4802, 0.5289],\n",
      "         [0.4828, 0.5350, 0.4871, 0.5264],\n",
      "         [0.4831, 0.5390, 0.4867, 0.5132],\n",
      "         [0.4930, 0.5375, 0.4878, 0.5192]],\n",
      "\n",
      "        [[0.4824, 0.5383, 0.4911, 0.5285],\n",
      "         [0.4866, 0.5436, 0.4901, 0.5210],\n",
      "         [0.4914, 0.5353, 0.4792, 0.5293],\n",
      "         [0.4892, 0.5337, 0.4847, 0.5347],\n",
      "         [0.4810, 0.5338, 0.4884, 0.5352],\n",
      "         [0.4904, 0.5394, 0.4854, 0.5271],\n",
      "         [0.4901, 0.5307, 0.4840, 0.5241],\n",
      "         [0.4861, 0.5313, 0.4880, 0.5182],\n",
      "         [0.4820, 0.5303, 0.4835, 0.5419],\n",
      "         [0.4910, 0.5405, 0.4894, 0.5282],\n",
      "         [0.4974, 0.5319, 0.4948, 0.5225],\n",
      "         [0.4887, 0.5352, 0.4965, 0.5302],\n",
      "         [0.4813, 0.5330, 0.4915, 0.5261],\n",
      "         [0.5063, 0.5475, 0.4774, 0.5355],\n",
      "         [0.4894, 0.5304, 0.4846, 0.5367],\n",
      "         [0.4868, 0.5389, 0.4778, 0.5145],\n",
      "         [0.4826, 0.5376, 0.4912, 0.5223],\n",
      "         [0.4898, 0.5478, 0.4838, 0.5348],\n",
      "         [0.4810, 0.5445, 0.4926, 0.5232],\n",
      "         [0.4918, 0.5386, 0.4876, 0.5398],\n",
      "         [0.4856, 0.5406, 0.4795, 0.5308],\n",
      "         [0.4903, 0.5376, 0.4893, 0.5200],\n",
      "         [0.4840, 0.5424, 0.4790, 0.5209],\n",
      "         [0.4881, 0.5396, 0.4900, 0.5287],\n",
      "         [0.4759, 0.5404, 0.4927, 0.5359],\n",
      "         [0.4874, 0.5364, 0.4874, 0.5288],\n",
      "         [0.4899, 0.5412, 0.4797, 0.5276],\n",
      "         [0.4919, 0.5374, 0.4938, 0.5225],\n",
      "         [0.4882, 0.5305, 0.4915, 0.5315],\n",
      "         [0.4912, 0.5531, 0.4899, 0.5333],\n",
      "         [0.4841, 0.5299, 0.4847, 0.5270],\n",
      "         [0.4895, 0.5231, 0.4844, 0.5371],\n",
      "         [0.4870, 0.5370, 0.4868, 0.5395],\n",
      "         [0.4906, 0.5331, 0.4914, 0.5226],\n",
      "         [0.4903, 0.5259, 0.4874, 0.5254],\n",
      "         [0.4997, 0.5297, 0.4831, 0.5275],\n",
      "         [0.4919, 0.5365, 0.4785, 0.5202],\n",
      "         [0.4890, 0.5439, 0.4932, 0.5461],\n",
      "         [0.4784, 0.5344, 0.4838, 0.5314],\n",
      "         [0.4721, 0.5354, 0.4802, 0.5417],\n",
      "         [0.4922, 0.5291, 0.4790, 0.5318],\n",
      "         [0.4854, 0.5513, 0.4905, 0.5140],\n",
      "         [0.4952, 0.5253, 0.4867, 0.5318],\n",
      "         [0.4804, 0.5437, 0.4964, 0.5262],\n",
      "         [0.4874, 0.5270, 0.4857, 0.5247],\n",
      "         [0.4789, 0.5499, 0.4783, 0.5236],\n",
      "         [0.4943, 0.5340, 0.4778, 0.5295],\n",
      "         [0.4913, 0.5486, 0.4868, 0.5306],\n",
      "         [0.4740, 0.5497, 0.4795, 0.5345],\n",
      "         [0.4941, 0.5422, 0.4836, 0.5222],\n",
      "         [0.4747, 0.5388, 0.4824, 0.5183],\n",
      "         [0.4818, 0.5460, 0.4825, 0.5231],\n",
      "         [0.4812, 0.5391, 0.4936, 0.5403],\n",
      "         [0.5001, 0.5350, 0.4919, 0.5340],\n",
      "         [0.4875, 0.5426, 0.4748, 0.5284],\n",
      "         [0.4923, 0.5409, 0.4908, 0.5367],\n",
      "         [0.4832, 0.5329, 0.4771, 0.5226],\n",
      "         [0.4853, 0.5414, 0.4917, 0.5205],\n",
      "         [0.4688, 0.5304, 0.4969, 0.5328],\n",
      "         [0.4848, 0.5424, 0.4895, 0.5237],\n",
      "         [0.4926, 0.5377, 0.4877, 0.5278],\n",
      "         [0.4997, 0.5243, 0.4887, 0.5317],\n",
      "         [0.4895, 0.5401, 0.4785, 0.5374],\n",
      "         [0.4896, 0.5448, 0.4921, 0.5181],\n",
      "         [0.4883, 0.5466, 0.4837, 0.5288],\n",
      "         [0.4818, 0.5358, 0.4821, 0.5151],\n",
      "         [0.4954, 0.5285, 0.4798, 0.5230],\n",
      "         [0.4776, 0.5243, 0.4887, 0.5240],\n",
      "         [0.4910, 0.5396, 0.4859, 0.5291],\n",
      "         [0.4902, 0.5333, 0.4888, 0.5381],\n",
      "         [0.4827, 0.5488, 0.4954, 0.5260],\n",
      "         [0.4886, 0.5232, 0.4859, 0.5288],\n",
      "         [0.4881, 0.5308, 0.4844, 0.5183],\n",
      "         [0.4842, 0.5275, 0.4821, 0.5312],\n",
      "         [0.4837, 0.5263, 0.4860, 0.5338],\n",
      "         [0.4852, 0.5314, 0.4938, 0.5272],\n",
      "         [0.4870, 0.5412, 0.4924, 0.5332],\n",
      "         [0.4821, 0.5353, 0.4810, 0.5285],\n",
      "         [0.4900, 0.5404, 0.4755, 0.5363],\n",
      "         [0.4878, 0.5326, 0.4860, 0.5364],\n",
      "         [0.4884, 0.5312, 0.4886, 0.5152],\n",
      "         [0.4894, 0.5354, 0.4940, 0.5310],\n",
      "         [0.4855, 0.5355, 0.4888, 0.5388],\n",
      "         [0.4841, 0.5361, 0.4916, 0.5337],\n",
      "         [0.4988, 0.5410, 0.4974, 0.5265],\n",
      "         [0.4859, 0.5397, 0.4869, 0.5250],\n",
      "         [0.4824, 0.5307, 0.4798, 0.5357],\n",
      "         [0.4906, 0.5436, 0.4929, 0.5173],\n",
      "         [0.4753, 0.5456, 0.4968, 0.5217],\n",
      "         [0.4888, 0.5254, 0.4937, 0.5314],\n",
      "         [0.4819, 0.5404, 0.4953, 0.5262],\n",
      "         [0.4778, 0.5408, 0.4845, 0.5259],\n",
      "         [0.4792, 0.5395, 0.4879, 0.5336],\n",
      "         [0.4861, 0.5381, 0.4919, 0.5315],\n",
      "         [0.4873, 0.5205, 0.4831, 0.5336],\n",
      "         [0.4831, 0.5391, 0.4812, 0.5367],\n",
      "         [0.4885, 0.5408, 0.4785, 0.5259],\n",
      "         [0.4860, 0.5415, 0.4908, 0.5196],\n",
      "         [0.4879, 0.5421, 0.4995, 0.5286],\n",
      "         [0.4987, 0.5511, 0.4888, 0.5322]]], device='cuda:0'), 'pred_obj_boxes': tensor([[[0.5031, 0.5348, 0.4984, 0.5154],\n",
      "         [0.5111, 0.5368, 0.4884, 0.5098],\n",
      "         [0.5176, 0.5426, 0.4975, 0.5068],\n",
      "         [0.5116, 0.5491, 0.5021, 0.5118],\n",
      "         [0.5135, 0.5279, 0.4995, 0.5161],\n",
      "         [0.5081, 0.5373, 0.4936, 0.5152],\n",
      "         [0.5167, 0.5325, 0.4923, 0.5174],\n",
      "         [0.5020, 0.5411, 0.4918, 0.5123],\n",
      "         [0.5039, 0.5378, 0.5012, 0.5073],\n",
      "         [0.5072, 0.5396, 0.4925, 0.5049],\n",
      "         [0.5189, 0.5499, 0.4982, 0.5090],\n",
      "         [0.5034, 0.5432, 0.4930, 0.5103],\n",
      "         [0.5052, 0.5445, 0.4937, 0.5232],\n",
      "         [0.5112, 0.5401, 0.4922, 0.5105],\n",
      "         [0.5099, 0.5460, 0.4884, 0.5085],\n",
      "         [0.5079, 0.5347, 0.4993, 0.5083],\n",
      "         [0.5045, 0.5463, 0.4966, 0.5042],\n",
      "         [0.5105, 0.5424, 0.4971, 0.5284],\n",
      "         [0.5164, 0.5504, 0.4956, 0.5081],\n",
      "         [0.5129, 0.5505, 0.5067, 0.5238],\n",
      "         [0.5049, 0.5410, 0.4949, 0.5036],\n",
      "         [0.5073, 0.5470, 0.5036, 0.5107],\n",
      "         [0.5005, 0.5511, 0.4929, 0.5085],\n",
      "         [0.5012, 0.5381, 0.4960, 0.5072],\n",
      "         [0.5083, 0.5418, 0.4983, 0.5089],\n",
      "         [0.5066, 0.5405, 0.4977, 0.5112],\n",
      "         [0.5130, 0.5482, 0.5001, 0.5035],\n",
      "         [0.5102, 0.5392, 0.4936, 0.5073],\n",
      "         [0.5066, 0.5390, 0.4998, 0.5039],\n",
      "         [0.5153, 0.5416, 0.4961, 0.5085],\n",
      "         [0.5049, 0.5387, 0.4938, 0.4989],\n",
      "         [0.5087, 0.5301, 0.4942, 0.5113],\n",
      "         [0.5089, 0.5382, 0.4944, 0.5039],\n",
      "         [0.5045, 0.5397, 0.4905, 0.5142],\n",
      "         [0.5319, 0.5450, 0.4899, 0.5122],\n",
      "         [0.5127, 0.5412, 0.5023, 0.5024],\n",
      "         [0.5138, 0.5372, 0.4950, 0.5077],\n",
      "         [0.5190, 0.5466, 0.4972, 0.5114],\n",
      "         [0.5154, 0.5363, 0.4919, 0.5051],\n",
      "         [0.5075, 0.5486, 0.4964, 0.5076],\n",
      "         [0.5189, 0.5424, 0.4938, 0.5032],\n",
      "         [0.4942, 0.5445, 0.5112, 0.5087],\n",
      "         [0.5130, 0.5408, 0.5042, 0.5162],\n",
      "         [0.5014, 0.5352, 0.5013, 0.4990],\n",
      "         [0.5124, 0.5328, 0.4933, 0.5083],\n",
      "         [0.5073, 0.5557, 0.5108, 0.5026],\n",
      "         [0.5045, 0.5432, 0.5060, 0.5085],\n",
      "         [0.5084, 0.5422, 0.5022, 0.4897],\n",
      "         [0.5042, 0.5417, 0.4919, 0.5094],\n",
      "         [0.5071, 0.5387, 0.4942, 0.5113],\n",
      "         [0.5192, 0.5460, 0.5056, 0.5045],\n",
      "         [0.5075, 0.5391, 0.4989, 0.5073],\n",
      "         [0.5057, 0.5361, 0.4967, 0.5107],\n",
      "         [0.5118, 0.5345, 0.4864, 0.5077],\n",
      "         [0.5172, 0.5317, 0.4942, 0.5116],\n",
      "         [0.5002, 0.5391, 0.4915, 0.5134],\n",
      "         [0.5084, 0.5344, 0.5073, 0.5077],\n",
      "         [0.4967, 0.5342, 0.4967, 0.5040],\n",
      "         [0.5049, 0.5368, 0.5113, 0.5154],\n",
      "         [0.5078, 0.5390, 0.5006, 0.5081],\n",
      "         [0.5149, 0.5429, 0.5072, 0.5095],\n",
      "         [0.5222, 0.5295, 0.5120, 0.5117],\n",
      "         [0.4965, 0.5378, 0.4897, 0.5039],\n",
      "         [0.5148, 0.5378, 0.4967, 0.5110],\n",
      "         [0.5121, 0.5399, 0.4959, 0.5024],\n",
      "         [0.5082, 0.5334, 0.4940, 0.5094],\n",
      "         [0.5138, 0.5510, 0.4934, 0.5112],\n",
      "         [0.5068, 0.5424, 0.4992, 0.5067],\n",
      "         [0.5123, 0.5451, 0.5031, 0.5066],\n",
      "         [0.5014, 0.5490, 0.4851, 0.4998],\n",
      "         [0.4985, 0.5445, 0.4954, 0.5030],\n",
      "         [0.4980, 0.5511, 0.4934, 0.5077],\n",
      "         [0.5064, 0.5383, 0.4889, 0.5161],\n",
      "         [0.5169, 0.5347, 0.5022, 0.5080],\n",
      "         [0.5135, 0.5444, 0.5014, 0.5181],\n",
      "         [0.5027, 0.5420, 0.4909, 0.5015],\n",
      "         [0.5174, 0.5362, 0.4886, 0.5114],\n",
      "         [0.5137, 0.5414, 0.4940, 0.5065],\n",
      "         [0.5152, 0.5387, 0.4949, 0.5177],\n",
      "         [0.5057, 0.5462, 0.4917, 0.5008],\n",
      "         [0.5002, 0.5454, 0.5039, 0.5074],\n",
      "         [0.5225, 0.5423, 0.4913, 0.5177],\n",
      "         [0.5037, 0.5484, 0.4991, 0.4977],\n",
      "         [0.5164, 0.5381, 0.5025, 0.5037],\n",
      "         [0.5053, 0.5534, 0.5080, 0.5119],\n",
      "         [0.5048, 0.5421, 0.5048, 0.5129],\n",
      "         [0.5106, 0.5401, 0.4846, 0.5102],\n",
      "         [0.5046, 0.5448, 0.5000, 0.5121],\n",
      "         [0.5171, 0.5445, 0.5008, 0.5152],\n",
      "         [0.4979, 0.5303, 0.4869, 0.5124],\n",
      "         [0.5068, 0.5480, 0.5089, 0.5128],\n",
      "         [0.5045, 0.5506, 0.5058, 0.5142],\n",
      "         [0.5175, 0.5509, 0.5030, 0.5092],\n",
      "         [0.5028, 0.5476, 0.5030, 0.5064],\n",
      "         [0.5058, 0.5339, 0.4964, 0.5056],\n",
      "         [0.5061, 0.5404, 0.4919, 0.5037],\n",
      "         [0.5094, 0.5373, 0.5068, 0.5040],\n",
      "         [0.5110, 0.5367, 0.4846, 0.5071],\n",
      "         [0.5199, 0.5351, 0.5024, 0.5012],\n",
      "         [0.5111, 0.5373, 0.4883, 0.5113]],\n",
      "\n",
      "        [[0.5133, 0.5390, 0.5035, 0.5087],\n",
      "         [0.4995, 0.5439, 0.4901, 0.4929],\n",
      "         [0.5012, 0.5421, 0.5056, 0.4987],\n",
      "         [0.5098, 0.5321, 0.4964, 0.4957],\n",
      "         [0.5125, 0.5286, 0.4945, 0.5120],\n",
      "         [0.5123, 0.5340, 0.4842, 0.5013],\n",
      "         [0.5141, 0.5366, 0.5014, 0.5003],\n",
      "         [0.5112, 0.5394, 0.4946, 0.5088],\n",
      "         [0.5156, 0.5348, 0.5079, 0.5081],\n",
      "         [0.5131, 0.5396, 0.4828, 0.5122],\n",
      "         [0.4899, 0.5432, 0.4859, 0.4987],\n",
      "         [0.5066, 0.5390, 0.5016, 0.4996],\n",
      "         [0.5166, 0.5327, 0.4948, 0.5085],\n",
      "         [0.5191, 0.5441, 0.5071, 0.5027],\n",
      "         [0.5038, 0.5329, 0.5034, 0.5045],\n",
      "         [0.5028, 0.5381, 0.4932, 0.4988],\n",
      "         [0.5044, 0.5475, 0.4964, 0.4895],\n",
      "         [0.5080, 0.5394, 0.5015, 0.5014],\n",
      "         [0.5093, 0.5364, 0.4971, 0.5112],\n",
      "         [0.5000, 0.5417, 0.4973, 0.4950],\n",
      "         [0.5111, 0.5399, 0.4979, 0.5022],\n",
      "         [0.5068, 0.5307, 0.4964, 0.4932],\n",
      "         [0.5122, 0.5305, 0.4909, 0.4990],\n",
      "         [0.5111, 0.5492, 0.4964, 0.4921],\n",
      "         [0.5190, 0.5397, 0.4924, 0.4956],\n",
      "         [0.4979, 0.5376, 0.4960, 0.5034],\n",
      "         [0.5105, 0.5380, 0.5014, 0.5055],\n",
      "         [0.5134, 0.5343, 0.4959, 0.4991],\n",
      "         [0.5061, 0.5382, 0.5044, 0.4997],\n",
      "         [0.5101, 0.5337, 0.4980, 0.4976],\n",
      "         [0.5102, 0.5360, 0.5092, 0.4981],\n",
      "         [0.5035, 0.5460, 0.4866, 0.5136],\n",
      "         [0.5037, 0.5402, 0.5057, 0.5023],\n",
      "         [0.5052, 0.5391, 0.5025, 0.4943],\n",
      "         [0.5069, 0.5357, 0.4866, 0.5015],\n",
      "         [0.5064, 0.5295, 0.4982, 0.5035],\n",
      "         [0.5095, 0.5395, 0.5147, 0.4927],\n",
      "         [0.5081, 0.5288, 0.4932, 0.4991],\n",
      "         [0.5133, 0.5370, 0.4949, 0.5114],\n",
      "         [0.5190, 0.5361, 0.4894, 0.5159],\n",
      "         [0.5101, 0.5317, 0.4999, 0.5156],\n",
      "         [0.5052, 0.5361, 0.4989, 0.5001],\n",
      "         [0.5035, 0.5411, 0.4990, 0.4982],\n",
      "         [0.5136, 0.5405, 0.4955, 0.4995],\n",
      "         [0.5131, 0.5357, 0.4951, 0.5069],\n",
      "         [0.5075, 0.5371, 0.5019, 0.5034],\n",
      "         [0.5174, 0.5301, 0.4989, 0.5068],\n",
      "         [0.5032, 0.5421, 0.4909, 0.4990],\n",
      "         [0.5194, 0.5446, 0.5031, 0.5092],\n",
      "         [0.5009, 0.5430, 0.4926, 0.4997],\n",
      "         [0.5030, 0.5320, 0.5002, 0.5068],\n",
      "         [0.5196, 0.5377, 0.4998, 0.5086],\n",
      "         [0.5174, 0.5424, 0.5015, 0.4994],\n",
      "         [0.5019, 0.5354, 0.4975, 0.5070],\n",
      "         [0.5093, 0.5415, 0.4974, 0.5019],\n",
      "         [0.5245, 0.5371, 0.4992, 0.5013],\n",
      "         [0.5064, 0.5294, 0.4906, 0.4932],\n",
      "         [0.5029, 0.5391, 0.4948, 0.5018],\n",
      "         [0.5071, 0.5379, 0.4955, 0.4986],\n",
      "         [0.5062, 0.5329, 0.4945, 0.5017],\n",
      "         [0.5153, 0.5348, 0.4912, 0.4966],\n",
      "         [0.5024, 0.5397, 0.5015, 0.5036],\n",
      "         [0.5160, 0.5301, 0.5037, 0.5081],\n",
      "         [0.5142, 0.5393, 0.4848, 0.4972],\n",
      "         [0.5098, 0.5343, 0.5064, 0.5001],\n",
      "         [0.5065, 0.5276, 0.4831, 0.5052],\n",
      "         [0.5079, 0.5411, 0.5002, 0.5008],\n",
      "         [0.5055, 0.5424, 0.4891, 0.4991],\n",
      "         [0.5114, 0.5294, 0.4957, 0.5091],\n",
      "         [0.5062, 0.5348, 0.4980, 0.5042],\n",
      "         [0.4907, 0.5339, 0.5067, 0.5085],\n",
      "         [0.5069, 0.5415, 0.4971, 0.5021],\n",
      "         [0.5052, 0.5312, 0.4932, 0.4953],\n",
      "         [0.5043, 0.5373, 0.4953, 0.4964],\n",
      "         [0.5062, 0.5316, 0.4931, 0.4960],\n",
      "         [0.5109, 0.5380, 0.5045, 0.5137],\n",
      "         [0.5023, 0.5392, 0.5087, 0.5027],\n",
      "         [0.5233, 0.5341, 0.4811, 0.5007],\n",
      "         [0.5085, 0.5380, 0.4903, 0.5088],\n",
      "         [0.5044, 0.5350, 0.4924, 0.5002],\n",
      "         [0.5163, 0.5274, 0.5023, 0.5060],\n",
      "         [0.5052, 0.5376, 0.4948, 0.5073],\n",
      "         [0.5123, 0.5387, 0.5017, 0.5002],\n",
      "         [0.5232, 0.5323, 0.4924, 0.4973],\n",
      "         [0.4968, 0.5399, 0.4931, 0.5008],\n",
      "         [0.5143, 0.5378, 0.5064, 0.4993],\n",
      "         [0.5076, 0.5199, 0.5060, 0.5111],\n",
      "         [0.5136, 0.5399, 0.4980, 0.5038],\n",
      "         [0.5010, 0.5340, 0.4935, 0.4924],\n",
      "         [0.5079, 0.5398, 0.4952, 0.5101],\n",
      "         [0.5117, 0.5432, 0.4908, 0.4976],\n",
      "         [0.5132, 0.5331, 0.4991, 0.4975],\n",
      "         [0.5103, 0.5314, 0.4976, 0.5128],\n",
      "         [0.4978, 0.5397, 0.4985, 0.5000],\n",
      "         [0.4920, 0.5447, 0.5043, 0.5120],\n",
      "         [0.5180, 0.5389, 0.4898, 0.4975],\n",
      "         [0.5097, 0.5306, 0.5043, 0.5075],\n",
      "         [0.5039, 0.5290, 0.4943, 0.5050],\n",
      "         [0.5011, 0.5416, 0.5091, 0.5014],\n",
      "         [0.5040, 0.5332, 0.4974, 0.5003]]], device='cuda:0')}]}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Cannot find field 'items' in the given Instances!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/djjin/Mygit/X-Decoder/notebooks/06_hoi_modified_train_test.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/06_hoi_modified_train_test.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/06_hoi_modified_train_test.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# print(batch)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/06_hoi_modified_train_test.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m losses \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/06_hoi_modified_train_test.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m pprint(losses)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/BaseModel.py:19\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 19\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/architectures/hoi_model.py:119\u001b[0m, in \u001b[0;36mCDNHOI.forward\u001b[0;34m(self, batched_inputs, mode)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, batched_inputs, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvcoco\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 119\u001b[0m         losses_hoi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_hoi(batched_inputs[\u001b[39m\"\u001b[39;49m\u001b[39mvcoco\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    120\u001b[0m         \u001b[39mreturn\u001b[39;00m losses_hoi\n\u001b[1;32m    121\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/architectures/hoi_model.py:137\u001b[0m, in \u001b[0;36mCDNHOI.forward_hoi\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    135\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhoid_head(features, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    136\u001b[0m \u001b[39mprint\u001b[39m(out)\n\u001b[0;32m--> 137\u001b[0m targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_targets(batched_inputs)\n\u001b[1;32m    138\u001b[0m losses_hoi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(out, targets)\n\u001b[1;32m    140\u001b[0m \u001b[39mdel\u001b[39;00m out\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/hdecoder/architectures/hoi_model.py:147\u001b[0m, in \u001b[0;36mCDNHOI._prepare_targets\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch_per_image \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batched_inputs):\n\u001b[1;32m    146\u001b[0m     targets \u001b[39m=\u001b[39m batch_per_image[\u001b[39m\"\u001b[39m\u001b[39minstances\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 147\u001b[0m     new_targets\u001b[39m.\u001b[39mappend({k: v\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m targets\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m new_targets\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_X_Decoder/lib/python3.9/site-packages/detectron2/structures/instances.py:65\u001b[0m, in \u001b[0;36mInstances.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     64\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_fields\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fields:\n\u001b[0;32m---> 65\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot find field \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m in the given Instances!\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fields[name]\n",
      "\u001b[0;31mAttributeError\u001b[0m: Cannot find field 'items' in the given Instances!"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        if idx > 2:\n",
    "            break\n",
    "        # print(batch)\n",
    "        losses = model(batch)\n",
    "        pprint(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
