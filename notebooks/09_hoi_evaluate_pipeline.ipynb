{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjin/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  1.13 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/home/djjin/Mygit/X-Decoder/notebooks, universal_newlines=False, shell=None, istream=None)\n",
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/home/djjin/Mygit/X-Decoder/notebooks, universal_newlines=False, shell=None, istream=None)\n",
      "DEBUG:wandb.docker.auth:Trying paths: ['/home/djjin/.docker/config.json', '/home/djjin/.dockercfg']\n",
      "DEBUG:wandb.docker.auth:No config file found\n",
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from hdecoder.BaseModel import BaseModel\n",
    "from hdecoder import build_model\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WEIGHT', 'RESUME_FROM'] ['true', '../checkpoints/xdecoder_focalt_best_openseg.pt']\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "# cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/svlp_focalt_lang.yaml\")]\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/vcoco.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', 'true', 'RESUME_FROM', '../checkpoints/xdecoder_focalt_best_openseg.pt'] \n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "print(keys, vals)\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../checkpoints/xdecoder_focalt_best_openseg.pt\n"
     ]
    }
   ],
   "source": [
    "pretrained_pth = os.path.join(opt['RESUME_FROM'])\n",
    "print(pretrained_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw1.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.dw2.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_1, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_2, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.bias, Model Shape: torch.Size([196]) <-> Ckpt Shape: torch.Size([196])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.weight, Model Shape: torch.Size([196, 96]) <-> Ckpt Shape: torch.Size([196, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([96, 1, 5, 5]) <-> Ckpt Shape: torch.Size([96, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([96, 1, 7, 7]) <-> Ckpt Shape: torch.Size([96, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.weight, Model Shape: torch.Size([96, 96, 1, 1]) <-> Ckpt Shape: torch.Size([96, 96, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw1.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.dw2.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_1, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_2, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.bias, Model Shape: torch.Size([196]) <-> Ckpt Shape: torch.Size([196])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.weight, Model Shape: torch.Size([196, 96]) <-> Ckpt Shape: torch.Size([196, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([96, 1, 3, 3]) <-> Ckpt Shape: torch.Size([96, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([96, 1, 5, 5]) <-> Ckpt Shape: torch.Size([96, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([96, 1, 7, 7]) <-> Ckpt Shape: torch.Size([96, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.weight, Model Shape: torch.Size([96, 96, 1, 1]) <-> Ckpt Shape: torch.Size([96, 96, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.weight, Model Shape: torch.Size([192, 96, 3, 3]) <-> Ckpt Shape: torch.Size([192, 96, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw1.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.dw2.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.bias, Model Shape: torch.Size([388]) <-> Ckpt Shape: torch.Size([388])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.weight, Model Shape: torch.Size([388, 192]) <-> Ckpt Shape: torch.Size([388, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw1.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.dw2.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.bias, Model Shape: torch.Size([388]) <-> Ckpt Shape: torch.Size([388])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.weight, Model Shape: torch.Size([388, 192]) <-> Ckpt Shape: torch.Size([388, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.weight, Model Shape: torch.Size([384, 192, 3, 3]) <-> Ckpt Shape: torch.Size([384, 192, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw1.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.dw2.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.bias, Model Shape: torch.Size([772]) <-> Ckpt Shape: torch.Size([772])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.weight, Model Shape: torch.Size([772, 384]) <-> Ckpt Shape: torch.Size([772, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.weight, Model Shape: torch.Size([768, 384, 3, 3]) <-> Ckpt Shape: torch.Size([768, 384, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw1.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.dw2.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.bias, Model Shape: torch.Size([1540]) <-> Ckpt Shape: torch.Size([1540])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.weight, Model Shape: torch.Size([1540, 768]) <-> Ckpt Shape: torch.Size([1540, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw1.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.dw2.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.bias, Model Shape: torch.Size([1540]) <-> Ckpt Shape: torch.Size([1540])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.weight, Model Shape: torch.Size([1540, 768]) <-> Ckpt Shape: torch.Size([1540, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.norm0.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.norm0.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
      "INFO:utils.model:Loaded backbone.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
      "INFO:utils.model:Loaded backbone.norm3.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.norm3.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.norm.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.norm.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])\n",
      "INFO:utils.model:Loaded backbone.patch_embed.proj.weight, Model Shape: torch.Size([96, 3, 7, 7]) <-> Ckpt Shape: torch.Size([96, 3, 7, 7])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_1.weight, Model Shape: torch.Size([512, 96, 1, 1]) <-> Ckpt Shape: torch.Size([512, 96, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_2.weight, Model Shape: torch.Size([512, 192, 1, 1]) <-> Ckpt Shape: torch.Size([512, 192, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.adapter_3.weight, Model Shape: torch.Size([512, 384, 1, 1]) <-> Ckpt Shape: torch.Size([512, 384, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.input_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.input_proj.weight, Model Shape: torch.Size([512, 768, 1, 1]) <-> Ckpt Shape: torch.Size([512, 768, 1, 1])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_1.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_2.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_3.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.layer_4.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.mask_features.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.mask_features.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
      "INFO:utils.model:Loaded hoid_head.encoder.transformer.encoder.layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.norm.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.hopd_decoder.norm.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear1.bias, Model Shape: torch.Size([2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm1.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm2.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm2.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm3.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.norm3.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.norm.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.hoi_decoder.interaction_decoder.norm.weight, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.0.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.0.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.1.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.2.bias, Model Shape: torch.Size([4])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_bbox_embed.layers.2.weight, Model Shape: torch.Size([4, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_class_embed.bias, Model Shape: torch.Size([81])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.obj_class_embed.weight, Model Shape: torch.Size([81, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.query_embed.weight, Model Shape: torch.Size([101, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.0.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.0.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.1.bias, Model Shape: torch.Size([512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.1.weight, Model Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.2.bias, Model Shape: torch.Size([4])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.sub_bbox_embed.layers.2.weight, Model Shape: torch.Size([4, 512])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.verb_class_embed.bias, Model Shape: torch.Size([29])\n",
      "WARNING:utils.model:*UNLOADED* hoid_head.verb_class_embed.weight, Model Shape: torch.Size([29, 512])\n",
      "WARNING:utils.model:$UNUSED$ backbone_proj, Ckpt Shape: torch.Size([768, 512])\n",
      "WARNING:utils.model:$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.caping_embed, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.class_embed, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.decoder_norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.decoder_norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding, Ckpt Shape: torch.Size([77, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.token_embedding.weight, Ckpt Shape: torch.Size([49408, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.lang_proj, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.lang_encoder.logit_scale, Ckpt Shape: torch.Size([])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.level_embed.weight, Ckpt Shape: torch.Size([3, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.0.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.0.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.1.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.1.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.mask_embed.layers.2.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.query_embed.weight, Ckpt Shape: torch.Size([101, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.query_feat.weight, Ckpt Shape: torch.Size([101, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.query_feat_caping.weight, Ckpt Shape: torch.Size([77, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.self_attn_mask, Ckpt Shape: torch.Size([1, 178, 178])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.0.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.1.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.2.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.3.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.4.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.5.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.6.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.7.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear1.bias, Ckpt Shape: torch.Size([2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear1.weight, Ckpt Shape: torch.Size([2048, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear2.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.linear2.weight, Ckpt Shape: torch.Size([512, 2048])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_ffn_layers.8.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.norm.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.norm.weight, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_bias, Ckpt Shape: torch.Size([1536])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_weight, Ckpt Shape: torch.Size([1536, 512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.bias, Ckpt Shape: torch.Size([512])\n",
      "WARNING:utils.model:$UNUSED$ sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.weight, Ckpt Shape: torch.Size([512, 512])\n",
      "WARNING:utils.model:*UNMATCHED* criterion.empty_weight, Model Shape: torch.Size([81]) <-> Ckpt Shape: torch.Size([134])\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained_pth).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "INFO:detectron2.data.common:Serializing 4946 elements to byte tensors and concatenating them all ...\n",
      "INFO:detectron2.data.common:Serialized dataset takes 3.27 MiB\n"
     ]
    }
   ],
   "source": [
    "from datasets import build_evaluator, build_eval_dataloader\n",
    "dataloaders = build_eval_dataloader(opt)\n",
    "dataset_names = opt[\"DATASETS\"][\"TEST\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vcoco\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'TEST'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/djjin/Mygit/X-Decoder/notebooks/09_hoi_evaluate_pipeline.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/09_hoi_evaluate_pipeline.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m summary \u001b[39m=\u001b[39m {}\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/09_hoi_evaluate_pipeline.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataloader, dataset_name \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(dataloaders, dataset_names):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/09_hoi_evaluate_pipeline.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# build evaluator\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/09_hoi_evaluate_pipeline.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     evaluator \u001b[39m=\u001b[39m build_evaluator(opt, dataset_name, opt[\u001b[39m\"\u001b[39;49m\u001b[39mSAVE_DIR\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/djjin/Mygit/X-Decoder/notebooks/09_hoi_evaluate_pipeline.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     evaluator\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/Mygit/X-Decoder/datasets/build.py:533\u001b[0m, in \u001b[0;36mbuild_evaluator\u001b[0;34m(cfg, dataset_name, output_folder)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mif\u001b[39;00m evaluator_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcoco\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    531\u001b[0m     evaluator_list\u001b[39m.\u001b[39mappend(COCOEvaluator(dataset_name, output_dir\u001b[39m=\u001b[39moutput_folder))\n\u001b[0;32m--> 533\u001b[0m cfg_model_decoder_test \u001b[39m=\u001b[39m cfg[\u001b[39m\"\u001b[39;49m\u001b[39mMODEL\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mDECODER\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mTEST\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    534\u001b[0m \u001b[39m# panoptic segmentation\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m evaluator_type \u001b[39min\u001b[39;00m [\n\u001b[1;32m    536\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcoco_panoptic_seg\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39made20k_panoptic_seg\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbdd_panoptic_pano\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    542\u001b[0m ]:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TEST'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# init metadata\n",
    "scores = {}\n",
    "summary = {}\n",
    "for dataloader, dataset_name in zip(dataloaders, dataset_names):\n",
    "    # build evaluator\n",
    "    evaluator = build_evaluator(opt, dataset_name, opt[\"SAVE_DIR\"])\n",
    "    evaluator.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CUDA_VISIBLE_DEVICES=0 python entry.py evaluate \\\n",
    "            --conf_files configs/xdecoder/segvlp_focalt_lang.yaml \\\n",
    "            --overrides \\\n",
    "            COCO.INPUT.IMAGE_SIZE 1024 \\\n",
    "            MODEL.DECODER.CAPTIONING.ENABLED True \\\n",
    "            MODEL.DECODER.RETRIEVAL.ENABLED True \\\n",
    "            MODEL.DECODER.GROUNDING.ENABLED True \\\n",
    "            COCO.TEST.BATCH_SIZE_TOTAL 1 \\\n",
    "            COCO.TRAIN.BATCH_SIZE_TOTAL 1 \\\n",
    "            COCO.TRAIN.BATCH_SIZE_PER_GPU 1 \\\n",
    "            ADE20K.TEST.BATCH_SIZE_TOTAL 1 \\\n",
    "            VLP.TEST.BATCH_SIZE_TOTAL 32 \\\n",
    "            VLP.TRAIN.BATCH_SIZE_TOTAL 32 \\\n",
    "            VLP.TRAIN.BATCH_SIZE_PER_GPU 32 \\\n",
    "            MODEL.DECODER.HIDDEN_DIM 512 \\\n",
    "            MODEL.ENCODER.CONVS_DIM 512 \\\n",
    "            MODEL.ENCODER.MASK_DIM 512 \\\n",
    "            FP16 True \\\n",
    "            DONT_LOAD_MODEL False \\\n",
    "            PYLEARN_MODEL checkpoints/xdecoder_focalt_last.pt \\\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_Decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
