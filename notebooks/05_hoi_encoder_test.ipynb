{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  2.0 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n",
      "['xdecoder', 'body', 'decoder', 'xdecoder']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures import Boxes, ImageList, Instances, BitMasks, BoxMode\n",
    "\n",
    "from xdecoder.BaseModel import BaseModel\n",
    "from xdecoder import build_model\n",
    "\n",
    "from utils.arguments import load_opt_command\n",
    "from utils.misc import hook_metadata, hook_switcher, hook_opt\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "# cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/svlp_focalt_lang.yaml\")]\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/test_vcoco.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', '../checkpoints/xdecoder_focalt_best_openseg.pt'] \n",
    "cmdline_args.overrides\n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "config_dict\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../checkpoints/xdecoder_focalt_best_openseg.pt\n"
     ]
    }
   ],
   "source": [
    "pretrained_pth = os.path.join(opt['WEIGHT'])\n",
    "output_root = './output'\n",
    "image_pth = '../images/animals.png'\n",
    "print(pretrained_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*UNLOADED* sem_seg_head.predictor.pos_embed_caping.weight, Model Shape: torch.Size([77, 512])\n",
      "$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])\n",
      "$UNUSED$ sem_seg_head.predictor.query_feat_caping.weight, Ckpt Shape: torch.Size([77, 512])\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained_pth).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (model): GeneralizedXdecoder(\n",
       "    (backbone): D2FocalNet(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=96, out_features=196, bias=True)\n",
       "                (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=96, out_features=196, bias=True)\n",
       "                (h): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              (drop_path): DropPath(drop_prob=0.027)\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=192, out_features=388, bias=True)\n",
       "                (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (drop_path): DropPath(drop_prob=0.055)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=192, out_features=388, bias=True)\n",
       "                (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "              (drop_path): DropPath(drop_prob=0.082)\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.109)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.136)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.164)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.191)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.218)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): FocalModulationBlock(\n",
       "              (dw1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=384, out_features=772, bias=True)\n",
       "                (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "              (drop_path): DropPath(drop_prob=0.245)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchEmbed(\n",
       "            (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): FocalModulationBlock(\n",
       "              (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=768, out_features=1540, bias=True)\n",
       "                (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (drop_path): DropPath(drop_prob=0.273)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FocalModulationBlock(\n",
       "              (dw1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (modulation): FocalModulation(\n",
       "                (f): Linear(in_features=768, out_features=1540, bias=True)\n",
       "                (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): GELU(approximate='none')\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (focal_layers): ModuleList(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
       "                    (1): GELU(approximate='none')\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (dw2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "              (drop_path): DropPath(drop_prob=0.300)\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (sem_seg_head): XdecoderHead(\n",
       "      (pixel_decoder): TransformerEncoderPixelDecoder(\n",
       "        (adapter_1): Conv2d(\n",
       "          96, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_1): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (adapter_2): Conv2d(\n",
       "          192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_2): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (adapter_3): Conv2d(\n",
       "          384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_3): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (input_proj): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer): TransformerEncoderOnly(\n",
       "          (encoder): TransformerEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-5): 6 x TransformerEncoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pe_layer): Positional encoding PositionEmbeddingSine\n",
       "            num_pos_feats: 256\n",
       "            temperature: 10000\n",
       "            normalize: True\n",
       "            scale: 6.283185307179586\n",
       "        (layer_4): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (predictor): XDecoder(\n",
       "        (pe_layer): Positional encoding PositionEmbeddingSine\n",
       "            num_pos_feats: 256\n",
       "            temperature: 10000\n",
       "            normalize: True\n",
       "            scale: 6.283185307179586\n",
       "        (transformer_self_attention_layers): ModuleList(\n",
       "          (0-8): 9 x SelfAttentionLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (transformer_cross_attention_layers): ModuleList(\n",
       "          (0-8): 9 x CrossAttentionLayer(\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (transformer_ffn_layers): ModuleList(\n",
       "          (0-8): 9 x FFNLayer(\n",
       "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (query_feat): Embedding(101, 512)\n",
       "        (query_embed): Embedding(101, 512)\n",
       "        (level_embed): Embedding(3, 512)\n",
       "        (input_proj): ModuleList(\n",
       "          (0-2): 3 x Sequential()\n",
       "        )\n",
       "        (lang_encoder): LanguageEncoder(\n",
       "          (lang_encoder): Transformer(\n",
       "            (token_embedding): Embedding(49408, 512)\n",
       "            (resblocks): ModuleList(\n",
       "              (0-11): 12 x ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm()\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm()\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ln_final): LayerNorm()\n",
       "          )\n",
       "        )\n",
       "        (mask_embed): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (pos_embed_caping): Embedding(77, 512)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone (FocalNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = model.model.backbone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hdecoder.body.encoder.transformer_encoder_hoi import TransformerEncoderHOI\n",
    "from hdecoder.body.encoder.registry import register_encoder\n",
    "\n",
    "hoi_encoder = TransformerEncoderHOI(opt, backbone.output_shape()).cuda()\n",
    "state_dict = model.model.sem_seg_head.pixel_decoder.state_dict()\n",
    "hoi_encoder.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdecoder.body.decoder.hdecoder import HDecoder\n",
    "hoi_decoder = HDecoder(opt, return_intermediate_dec=True).cuda()\n",
    "hidden_dim = hoi_decoder.d_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "query_embed = nn.Embedding(100, hidden_dim).cuda()\n",
    "query_embed.weight.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdecoder.body.decoder.modules import MLP\n",
    "num_obj_classes = 81\n",
    "num_verb_classes = 29\n",
    "\n",
    "obj_class_embed = nn.Linear(hidden_dim, num_obj_classes + 1).cuda()\n",
    "verb_class_embed = nn.Linear(hidden_dim, num_verb_classes).cuda()\n",
    "sub_bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3).cuda()\n",
    "obj_bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_layers_hopd = 3\n",
    "dec_layers_interaction = 3\n",
    "@torch.jit.unused\n",
    "def _set_aux_loss(outputs_obj_class, outputs_verb_class, outputs_sub_coord, outputs_obj_coord, outputs_matching=None):\n",
    "    min_dec_layers_num = min(dec_layers_hopd, dec_layers_interaction)\n",
    "    return [{'pred_obj_logits': a, 'pred_verb_logits': b, 'pred_sub_boxes': c, 'pred_obj_boxes': d}\n",
    "            for a, b, c, d in zip(outputs_obj_class[-min_dec_layers_num : -1], outputs_verb_class[-min_dec_layers_num : -1], \\\n",
    "                                    outputs_sub_coord[-min_dec_layers_num : -1], outputs_obj_coord[-min_dec_layers_num : -1])]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import nn\n",
    "\n",
    "from utils.box_ops import box_cxcywh_to_xyxy, generalized_box_iou\n",
    "\n",
    "class HungarianMatcherHOI(nn.Module):\n",
    "    def __init__(self, cost_obj_class: float = 1, cost_verb_class: float = 1, cost_bbox: float = 1,\n",
    "                 cost_giou: float = 1, cost_matching: float = 1, use_matching: bool = False):\n",
    "        super().__init__()\n",
    "        self.cost_obj_class = cost_obj_class\n",
    "        self.cost_verb_class = cost_verb_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "        self.cost_matching = cost_matching\n",
    "        self.use_matching = use_matching\n",
    "        assert cost_obj_class != 0 or cost_verb_class != 0 or cost_bbox != 0 or cost_giou != 0 or cost_matching != 0, 'all costs cant be 0'\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        bs, num_queries = outputs['pred_obj_logits'].shape[:2]\n",
    "        out_obj_prob = outputs['pred_obj_logits'].flatten(0, 1).softmax(-1)\n",
    "        out_verb_prob = outputs['pred_verb_logits'].flatten(0, 1).sigmoid()\n",
    "        out_sub_bbox = outputs['pred_sub_boxes'].flatten(0, 1)\n",
    "        out_obj_bbox = outputs['pred_obj_boxes'].flatten(0, 1)\n",
    "\n",
    "        tgt_obj_labels = torch.cat([v['obj_labels'] for v in targets])\n",
    "        tgt_verb_labels = torch.cat([v['verb_labels'] for v in targets])\n",
    "        tgt_verb_labels_permute = tgt_verb_labels.permute(1, 0)\n",
    "        tgt_sub_boxes = torch.cat([v['sub_boxes'] for v in targets])\n",
    "        tgt_obj_boxes = torch.cat([v['obj_boxes'] for v in targets])\n",
    "\n",
    "        cost_obj_class = -out_obj_prob[:, tgt_obj_labels]\n",
    "\n",
    "        tgt_verb_labels_permute = tgt_verb_labels.permute(1, 0)\n",
    "        cost_verb_class = -(out_verb_prob.matmul(tgt_verb_labels_permute) / \\\n",
    "                            (tgt_verb_labels_permute.sum(dim=0, keepdim=True) + 1e-4) + \\\n",
    "                            (1 - out_verb_prob).matmul(1 - tgt_verb_labels_permute) / \\\n",
    "                            ((1 - tgt_verb_labels_permute).sum(dim=0, keepdim=True) + 1e-4)) / 2\n",
    "\n",
    "        cost_sub_bbox = torch.cdist(out_sub_bbox, tgt_sub_boxes, p=1)\n",
    "        cost_obj_bbox = torch.cdist(out_obj_bbox, tgt_obj_boxes, p=1) * (tgt_obj_boxes != 0).any(dim=1).unsqueeze(0)\n",
    "        if cost_sub_bbox.shape[1] == 0:\n",
    "            cost_bbox = cost_sub_bbox\n",
    "        else:\n",
    "            cost_bbox = torch.stack((cost_sub_bbox, cost_obj_bbox)).max(dim=0)[0]\n",
    "\n",
    "        cost_sub_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_sub_bbox), box_cxcywh_to_xyxy(tgt_sub_boxes))\n",
    "        cost_obj_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_obj_bbox), box_cxcywh_to_xyxy(tgt_obj_boxes)) + \\\n",
    "                        cost_sub_giou * (tgt_obj_boxes == 0).all(dim=1).unsqueeze(0)\n",
    "        if cost_sub_giou.shape[1] == 0:\n",
    "            cost_giou = cost_sub_giou\n",
    "        else:\n",
    "            cost_giou = torch.stack((cost_sub_giou, cost_obj_giou)).max(dim=0)[0]\n",
    "\n",
    "        C = self.cost_obj_class * cost_obj_class + self.cost_verb_class * cost_verb_class + \\\n",
    "            self.cost_bbox * cost_bbox + self.cost_giou * cost_giou\n",
    "\n",
    "        if self.use_matching:\n",
    "            tgt_matching_labels = torch.cat([v['matching_labels'] for v in targets])\n",
    "            out_matching_prob = outputs['pred_matching_logits'].flatten(0, 1).softmax(-1)\n",
    "            cost_matching = -out_matching_prob[:, tgt_matching_labels]\n",
    "            C += self.cost_matching * cost_matching\n",
    "\n",
    "\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v['obj_labels']) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcherHOI(\n",
    "    cost_obj_class=1, \n",
    "    cost_verb_class=1,\n",
    "    cost_bbox=2.5, \n",
    "    cost_giou=1, \n",
    "    cost_matching=1)\n",
    "\n",
    "weight_dict = {}\n",
    "weight_dict['loss_obj_ce'] = 1\n",
    "weight_dict['loss_verb_ce'] = 2\n",
    "weight_dict['loss_sub_bbox'] = 2.5\n",
    "weight_dict['loss_obj_bbox'] = 2.5\n",
    "weight_dict['loss_sub_giou'] = 1\n",
    "weight_dict['loss_obj_giou'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dec_layers_num = min(dec_layers_hopd, dec_layers_interaction)\n",
    "aux_weight_dict = {}\n",
    "for i in range(min_dec_layers_num - 1):\n",
    "    aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
    "weight_dict.update(aux_weight_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "import math\n",
    "from utils.misc import accuracy, is_dist_avail_and_initialized, get_world_size\n",
    "\n",
    "class SetCriterionHOI(nn.Module):\n",
    "\n",
    "    def __init__(self, num_obj_classes, num_queries, num_verb_classes, matcher, weight_dict, eos_coef, losses):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_obj_classes = num_obj_classes\n",
    "        self.num_queries = num_queries\n",
    "        self.num_verb_classes = num_verb_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "        empty_weight = torch.ones(self.num_obj_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "        self.alpha = 0.5\n",
    "        self.obj_nums_init = [5397, 238, 332, 321, 5, 6, 45, 90, 59, 20, \\\n",
    "                                13, 5, 6, 313, 28, 25, 46, 277, 20, 16, \\\n",
    "                                154, 0, 7, 13, 356, 191, 458, 66, 337, 1364, \\\n",
    "                                1382, 958, 1166, 68, 258, 221, 1317, 1428, 759, 201, \\\n",
    "                                190, 444, 274, 587, 124, 107, 102, 37, 226, 16, \\\n",
    "                                30, 22, 187, 320, 222, 465, 893, 213, 56, 322, \\\n",
    "                                306, 13, 55, 834, 23, 104, 38, 861, 11, 27, \\\n",
    "                                0, 16, 22, 405, 50, 14, 145, 63, 9, 11]\n",
    "\n",
    "\n",
    "        self.obj_nums_init.append(3 * sum(self.obj_nums_init))  # 3 times fg for bg init\n",
    "        self.verb_nums_init = [4001, 4598, 1989, 488, 656, 3825, 367, 367, 677, 677, \\\n",
    "                                700, 471, 354, 498, 300, 313, 300, 300, 622, 458, \\\n",
    "                                500, 498, 489, 1545, 133, 142, 38, 116, 388]\n",
    "\n",
    "\n",
    "        self.verb_nums_init.append(3 * sum(self.verb_nums_init))\n",
    "\n",
    "        self.obj_reweight = False\n",
    "        self.verb_reweight = False\n",
    "        self.use_static_weights = False\n",
    "        \n",
    "        Maxsize = 4704*1.0\n",
    "\n",
    "        if self.obj_reweight:\n",
    "            self.q_obj = Queue(maxsize=Maxsize)\n",
    "            self.p_obj = 0.7\n",
    "            self.obj_weights_init = self.cal_weights(self.obj_nums_init, p=self.p_obj)\n",
    "\n",
    "        if self.verb_reweight:\n",
    "            self.q_verb = Queue(maxsize=Maxsize)\n",
    "            self.p_verb = 0.7\n",
    "            self.verb_weights_init = self.cal_weights(self.verb_nums_init, p=self.p_verb)\n",
    "\n",
    "    def cal_weights(self, label_nums, p=0.5):\n",
    "        num_fgs = len(label_nums[:-1])\n",
    "        weight = [0] * (num_fgs + 1)\n",
    "        num_all = sum(label_nums[:-1])\n",
    "\n",
    "        for index in range(num_fgs):\n",
    "            if label_nums[index] == 0: continue\n",
    "            weight[index] = np.power(num_all/label_nums[index], p)\n",
    "\n",
    "        weight = np.array(weight)\n",
    "        weight = weight / np.mean(weight[weight>0])\n",
    "\n",
    "        weight[-1] = np.power(num_all/label_nums[-1], p) if label_nums[-1] != 0 else 0\n",
    "\n",
    "        weight = torch.FloatTensor(weight).cuda()\n",
    "        return weight\n",
    "\n",
    "    def loss_obj_labels(self, outputs, targets, indices, num_interactions, log=True):\n",
    "        assert 'pred_obj_logits' in outputs\n",
    "        src_logits = outputs['pred_obj_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t['obj_labels'][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_obj_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        if not self.obj_reweight:\n",
    "            obj_weights = self.empty_weight\n",
    "        elif self.use_static_weights:\n",
    "            obj_weights = self.obj_weights_init\n",
    "        else:\n",
    "            obj_label_nums_in_batch = [0] * (self.num_obj_classes + 1)\n",
    "            for target_class in target_classes:\n",
    "                for label in target_class:\n",
    "                    obj_label_nums_in_batch[label] += 1\n",
    "\n",
    "            if self.q_obj.full(): self.q_obj.get()\n",
    "            self.q_obj.put(np.array(obj_label_nums_in_batch))\n",
    "            accumulated_obj_label_nums = np.sum(self.q_obj.queue, axis=0)\n",
    "            obj_weights = self.cal_weights(accumulated_obj_label_nums, p=self.p_obj)\n",
    "\n",
    "            aphal = min(math.pow(0.999, self.q_obj.qsize()),0.9)\n",
    "            obj_weights = aphal * self.obj_weights_init + (1 - aphal) * obj_weights\n",
    "\n",
    "        loss_obj_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, obj_weights)\n",
    "        losses = {'loss_obj_ce': loss_obj_ce}\n",
    "\n",
    "        if log:\n",
    "            losses['obj_class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def loss_obj_cardinality(self, outputs, targets, indices, num_interactions):\n",
    "        pred_logits = outputs['pred_obj_logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor([len(v['obj_labels']) for v in targets], device=device)\n",
    "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'obj_cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_verb_labels(self, outputs, targets, indices, num_interactions):\n",
    "        assert 'pred_verb_logits' in outputs\n",
    "        src_logits = outputs['pred_verb_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t['verb_labels'][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.zeros_like(src_logits)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        if not self.verb_reweight:\n",
    "            verb_weights = None\n",
    "        elif self.use_static_weights:\n",
    "            verb_weights = self.verb_weights_init\n",
    "        else:\n",
    "            verb_label_nums_in_batch = [0] * (self.num_verb_classes + 1)\n",
    "            for target_class in target_classes:\n",
    "                for label in target_class:\n",
    "                    label_classes = torch.where(label > 0)[0]\n",
    "                    if len(label_classes) == 0:\n",
    "                        verb_label_nums_in_batch[-1] += 1\n",
    "                    else:\n",
    "                        for label_class in label_classes:\n",
    "                            verb_label_nums_in_batch[label_class] += 1\n",
    "\n",
    "            if self.q_verb.full(): self.q_verb.get()\n",
    "            self.q_verb.put(np.array(verb_label_nums_in_batch))\n",
    "            accumulated_verb_label_nums = np.sum(self.q_verb.queue, axis=0)\n",
    "            verb_weights = self.cal_weights(accumulated_verb_label_nums, p=self.p_verb)\n",
    "\n",
    "            aphal = min(math.pow(0.999, self.q_verb.qsize()),0.9)\n",
    "            verb_weights = aphal * self.verb_weights_init + (1 - aphal) * verb_weights\n",
    "\n",
    "        src_logits = src_logits.sigmoid()\n",
    "        loss_verb_ce = self._neg_loss(src_logits, target_classes, weights=verb_weights, alpha=self.alpha)\n",
    "\n",
    "        losses = {'loss_verb_ce': loss_verb_ce}\n",
    "        return losses\n",
    "\n",
    "    def loss_sub_obj_boxes(self, outputs, targets, indices, num_interactions):\n",
    "        assert 'pred_sub_boxes' in outputs and 'pred_obj_boxes' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_sub_boxes = outputs['pred_sub_boxes'][idx]\n",
    "        src_obj_boxes = outputs['pred_obj_boxes'][idx]\n",
    "        target_sub_boxes = torch.cat([t['sub_boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "        target_obj_boxes = torch.cat([t['obj_boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        exist_obj_boxes = (target_obj_boxes != 0).any(dim=1)\n",
    "\n",
    "        losses = {}\n",
    "        if src_sub_boxes.shape[0] == 0:\n",
    "            losses['loss_sub_bbox'] = src_sub_boxes.sum()\n",
    "            losses['loss_obj_bbox'] = src_obj_boxes.sum()\n",
    "            losses['loss_sub_giou'] = src_sub_boxes.sum()\n",
    "            losses['loss_obj_giou'] = src_obj_boxes.sum()\n",
    "        else:\n",
    "            loss_sub_bbox = F.l1_loss(src_sub_boxes, target_sub_boxes, reduction='none')\n",
    "            loss_obj_bbox = F.l1_loss(src_obj_boxes, target_obj_boxes, reduction='none')\n",
    "            losses['loss_sub_bbox'] = loss_sub_bbox.sum() / num_interactions\n",
    "            losses['loss_obj_bbox'] = (loss_obj_bbox * exist_obj_boxes.unsqueeze(1)).sum() / (exist_obj_boxes.sum() + 1e-4)\n",
    "            loss_sub_giou = 1 - torch.diag(generalized_box_iou(box_cxcywh_to_xyxy(src_sub_boxes),\n",
    "                                                               box_cxcywh_to_xyxy(target_sub_boxes)))\n",
    "            loss_obj_giou = 1 - torch.diag(generalized_box_iou(box_cxcywh_to_xyxy(src_obj_boxes),\n",
    "                                                               box_cxcywh_to_xyxy(target_obj_boxes)))\n",
    "            losses['loss_sub_giou'] = loss_sub_giou.sum() / num_interactions\n",
    "            losses['loss_obj_giou'] = (loss_obj_giou * exist_obj_boxes).sum() / (exist_obj_boxes.sum() + 1e-4)\n",
    "        return losses\n",
    "\n",
    "    def loss_matching_labels(self, outputs, targets, indices, num_interactions, log=True):\n",
    "        assert 'pred_matching_logits' in outputs\n",
    "        src_logits = outputs['pred_matching_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t['matching_labels'][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], 0,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        loss_matching = F.cross_entropy(src_logits.transpose(1, 2), target_classes)\n",
    "        losses = {'loss_matching': loss_matching}\n",
    "\n",
    "        if log:\n",
    "            losses['matching_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses\n",
    "\n",
    "    def _neg_loss(self, pred, gt, weights=None, alpha=0.25):\n",
    "        pos_inds = gt.eq(1).float()\n",
    "        neg_inds = gt.lt(1).float()\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        pos_loss = alpha * torch.log(pred) * torch.pow(1 - pred, 2) * pos_inds\n",
    "        if weights is not None:\n",
    "            pos_loss = pos_loss * weights[:-1]\n",
    "\n",
    "        neg_loss = (1 - alpha) * torch.log(1 - pred) * torch.pow(pred, 2) * neg_inds\n",
    "\n",
    "        num_pos  = pos_inds.float().sum()\n",
    "        pos_loss = pos_loss.sum()\n",
    "        neg_loss = neg_loss.sum()\n",
    "\n",
    "        if num_pos == 0:\n",
    "            loss = loss - neg_loss\n",
    "        else:\n",
    "            loss = loss - (pos_loss + neg_loss) / num_pos\n",
    "        return loss\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num, **kwargs):\n",
    "        loss_map = {\n",
    "            'obj_labels': self.loss_obj_labels,\n",
    "            'obj_cardinality': self.loss_obj_cardinality,\n",
    "            'verb_labels': self.loss_verb_labels,\n",
    "            'sub_obj_boxes': self.loss_sub_obj_boxes,\n",
    "            'matching_labels': self.loss_matching_labels\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        num_interactions = sum(len(t['obj_labels']) for t in targets)\n",
    "        num_interactions = torch.as_tensor([num_interactions], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "        if is_dist_avail_and_initialized():\n",
    "            torch.distributed.all_reduce(num_interactions)\n",
    "        num_interactions = torch.clamp(num_interactions / get_world_size(), min=1).item()\n",
    "\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_interactions))\n",
    "\n",
    "        if 'aux_outputs' in outputs:\n",
    "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
    "                indices = self.matcher(aux_outputs, targets)\n",
    "                for loss in self.losses:\n",
    "                    kwargs = {}\n",
    "                    if loss == 'obj_labels':\n",
    "                        kwargs = {'log': False}\n",
    "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_interactions, **kwargs)\n",
    "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
    "                    losses.update(l_dict)\n",
    "\n",
    "        return losses\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostProcessHOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessHOI(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.subject_category_id = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, target_sizes):\n",
    "        out_obj_logits = outputs['pred_obj_logits']\n",
    "        out_verb_logits = outputs['pred_verb_logits']\n",
    "        out_sub_boxes = outputs['pred_sub_boxes']\n",
    "        out_obj_boxes = outputs['pred_obj_boxes']\n",
    "\n",
    "        assert len(out_obj_logits) == len(target_sizes)\n",
    "        assert target_sizes.shape[1] == 2\n",
    "\n",
    "        obj_prob = F.softmax(out_obj_logits, -1)\n",
    "        obj_scores, obj_labels = obj_prob[..., :-1].max(-1)\n",
    "\n",
    "        verb_scores = out_verb_logits.sigmoid()\n",
    "\n",
    "        img_h, img_w = target_sizes.unbind(1)\n",
    "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(verb_scores.device)\n",
    "        sub_boxes = box_cxcywh_to_xyxy(out_sub_boxes)\n",
    "        sub_boxes = sub_boxes * scale_fct[:, None, :]\n",
    "        obj_boxes = box_cxcywh_to_xyxy(out_obj_boxes)\n",
    "        obj_boxes = obj_boxes * scale_fct[:, None, :]\n",
    "\n",
    "        results = []\n",
    "        for index in range(len(obj_scores)):\n",
    "            os, ol, vs, sb, ob =  obj_scores[index], obj_labels[index], verb_scores[index], sub_boxes[index], obj_boxes[index]\n",
    "            sl = torch.full_like(ol, self.subject_category_id)\n",
    "            l = torch.cat((sl, ol))\n",
    "            b = torch.cat((sb, ob))\n",
    "            results.append({'labels': l.to('cpu'), 'boxes': b.to('cpu')})\n",
    "\n",
    "            vs = vs * os.unsqueeze(1)\n",
    "            ids = torch.arange(b.shape[0])\n",
    "\n",
    "            results[-1].update({'verb_scores': vs.to('cpu'), 'sub_ids': ids[:ids.shape[0] // 2],\n",
    "                                'obj_ids': ids[ids.shape[0] // 2:]})\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['obj_labels', 'verb_labels', 'sub_obj_boxes', 'obj_cardinality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obj_classes = 80\n",
    "num_queries = 100\n",
    "num_verb_classes = 27\n",
    "eos_coef = 0.1\n",
    "\n",
    "criterion = SetCriterionHOI(num_obj_classes, num_queries, num_verb_classes, matcher=matcher,\n",
    "                            weight_dict=weight_dict, eos_coef=eos_coef, losses=losses)\n",
    "criterion.to('cuda')\n",
    "postprocessors = {'hoi': PostProcessHOI()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, criterion, postprocessors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling forward() may cause unpredicted behavior of PixelDecoder module.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred_obj_logits': tensor([[[ 0.0716, -0.1811,  0.8853,  ..., -1.3349,  0.9013,  0.7508],\n",
      "         [ 0.1343, -0.1103,  0.5826,  ..., -1.0912,  1.0660,  0.8378],\n",
      "         [ 0.1196,  0.1614,  0.8855,  ..., -1.0058,  0.9663,  1.0236],\n",
      "         ...,\n",
      "         [-0.1798,  0.0443,  0.9540,  ..., -1.3170,  0.9375,  1.0993],\n",
      "         [ 0.2225, -0.1904,  0.7245,  ..., -0.9400,  0.6332,  0.7639],\n",
      "         [-0.0591,  0.2582,  1.0210,  ..., -1.0246,  1.1216,  0.8750]]],\n",
      "       device='cuda:0'), 'pred_verb_logits': tensor([[[-0.0118, -0.2418,  0.2691,  ..., -0.7811, -0.6395,  0.2486],\n",
      "         [-0.4491, -0.1349,  0.2583,  ..., -1.1000, -1.0485,  0.1363],\n",
      "         [-0.1654, -0.2236,  0.1395,  ..., -0.8794, -0.5250, -0.3411],\n",
      "         ...,\n",
      "         [-0.1160, -0.2981,  0.2313,  ..., -0.8096, -0.4913, -0.0524],\n",
      "         [-0.1175, -0.3126,  0.1487,  ..., -0.7988, -0.4732, -0.2040],\n",
      "         [ 0.2337, -0.1062,  0.1518,  ..., -0.5899, -0.3760, -0.1633]]],\n",
      "       device='cuda:0'), 'pred_sub_boxes': tensor([[[0.4903, 0.5543, 0.5423, 0.4823],\n",
      "         [0.4993, 0.5583, 0.5435, 0.4773],\n",
      "         [0.5018, 0.5532, 0.5425, 0.4750],\n",
      "         [0.4970, 0.5531, 0.5402, 0.4801],\n",
      "         [0.4967, 0.5543, 0.5400, 0.4772],\n",
      "         [0.4892, 0.5556, 0.5353, 0.4673],\n",
      "         [0.4907, 0.5465, 0.5282, 0.4730],\n",
      "         [0.4899, 0.5578, 0.5446, 0.4763],\n",
      "         [0.4839, 0.5548, 0.5355, 0.4791],\n",
      "         [0.4902, 0.5629, 0.5352, 0.4639],\n",
      "         [0.5008, 0.5499, 0.5442, 0.4706],\n",
      "         [0.4932, 0.5591, 0.5311, 0.4669],\n",
      "         [0.4931, 0.5665, 0.5446, 0.4794],\n",
      "         [0.4845, 0.5583, 0.5259, 0.4763],\n",
      "         [0.4955, 0.5563, 0.5369, 0.4683],\n",
      "         [0.4887, 0.5509, 0.5404, 0.4768],\n",
      "         [0.4963, 0.5542, 0.5459, 0.4673],\n",
      "         [0.4902, 0.5575, 0.5338, 0.4829],\n",
      "         [0.4960, 0.5464, 0.5391, 0.4687],\n",
      "         [0.4973, 0.5573, 0.5409, 0.4790],\n",
      "         [0.5059, 0.5527, 0.5264, 0.4786],\n",
      "         [0.4938, 0.5483, 0.5391, 0.4686],\n",
      "         [0.5032, 0.5555, 0.5416, 0.4790],\n",
      "         [0.5039, 0.5428, 0.5473, 0.4647],\n",
      "         [0.4983, 0.5501, 0.5379, 0.4644],\n",
      "         [0.4988, 0.5569, 0.5402, 0.4712],\n",
      "         [0.5025, 0.5588, 0.5430, 0.4734],\n",
      "         [0.4974, 0.5534, 0.5340, 0.4835],\n",
      "         [0.4922, 0.5589, 0.5445, 0.4882],\n",
      "         [0.4921, 0.5558, 0.5361, 0.4766],\n",
      "         [0.4910, 0.5654, 0.5333, 0.4756],\n",
      "         [0.5061, 0.5640, 0.5400, 0.4678],\n",
      "         [0.5022, 0.5554, 0.5361, 0.4701],\n",
      "         [0.4940, 0.5421, 0.5284, 0.4723],\n",
      "         [0.4917, 0.5567, 0.5362, 0.4775],\n",
      "         [0.4888, 0.5692, 0.5352, 0.4772],\n",
      "         [0.4953, 0.5547, 0.5422, 0.4766],\n",
      "         [0.4868, 0.5495, 0.5437, 0.4748],\n",
      "         [0.4992, 0.5449, 0.5353, 0.4852],\n",
      "         [0.4982, 0.5548, 0.5406, 0.4714],\n",
      "         [0.4853, 0.5498, 0.5493, 0.4791],\n",
      "         [0.4954, 0.5600, 0.5310, 0.4800],\n",
      "         [0.4965, 0.5630, 0.5353, 0.4635],\n",
      "         [0.5013, 0.5592, 0.5461, 0.4791],\n",
      "         [0.4952, 0.5542, 0.5385, 0.4693],\n",
      "         [0.5081, 0.5591, 0.5341, 0.4745],\n",
      "         [0.5111, 0.5475, 0.5359, 0.4742],\n",
      "         [0.4971, 0.5540, 0.5425, 0.4774],\n",
      "         [0.4970, 0.5523, 0.5481, 0.4678],\n",
      "         [0.4926, 0.5553, 0.5433, 0.4716],\n",
      "         [0.4931, 0.5591, 0.5463, 0.4642],\n",
      "         [0.4938, 0.5605, 0.5383, 0.4753],\n",
      "         [0.5079, 0.5498, 0.5425, 0.4720],\n",
      "         [0.4912, 0.5637, 0.5327, 0.4959],\n",
      "         [0.4909, 0.5638, 0.5468, 0.4779],\n",
      "         [0.4897, 0.5523, 0.5361, 0.4740],\n",
      "         [0.5110, 0.5623, 0.5365, 0.4794],\n",
      "         [0.4933, 0.5468, 0.5420, 0.4701],\n",
      "         [0.4948, 0.5508, 0.5403, 0.4743],\n",
      "         [0.5007, 0.5637, 0.5391, 0.4766],\n",
      "         [0.5040, 0.5543, 0.5436, 0.4785],\n",
      "         [0.4924, 0.5439, 0.5345, 0.4766],\n",
      "         [0.4929, 0.5646, 0.5319, 0.4773],\n",
      "         [0.4842, 0.5466, 0.5383, 0.4783],\n",
      "         [0.4965, 0.5534, 0.5485, 0.4703],\n",
      "         [0.4860, 0.5651, 0.5324, 0.4833],\n",
      "         [0.4953, 0.5482, 0.5303, 0.4844],\n",
      "         [0.4959, 0.5488, 0.5309, 0.4694],\n",
      "         [0.4918, 0.5459, 0.5571, 0.4834],\n",
      "         [0.4913, 0.5478, 0.5267, 0.4710],\n",
      "         [0.5028, 0.5547, 0.5319, 0.4749],\n",
      "         [0.4899, 0.5592, 0.5327, 0.4706],\n",
      "         [0.5011, 0.5631, 0.5280, 0.4783],\n",
      "         [0.4921, 0.5572, 0.5373, 0.4715],\n",
      "         [0.4896, 0.5601, 0.5428, 0.4724],\n",
      "         [0.4888, 0.5625, 0.5357, 0.4841],\n",
      "         [0.4959, 0.5569, 0.5406, 0.4752],\n",
      "         [0.4931, 0.5614, 0.5317, 0.4632],\n",
      "         [0.4941, 0.5505, 0.5337, 0.4675],\n",
      "         [0.4976, 0.5553, 0.5456, 0.4683],\n",
      "         [0.4947, 0.5591, 0.5381, 0.4767],\n",
      "         [0.4868, 0.5494, 0.5294, 0.4693],\n",
      "         [0.5047, 0.5547, 0.5402, 0.4796],\n",
      "         [0.4928, 0.5609, 0.5344, 0.4808],\n",
      "         [0.4804, 0.5477, 0.5424, 0.4742],\n",
      "         [0.5099, 0.5519, 0.5450, 0.4811],\n",
      "         [0.5002, 0.5577, 0.5384, 0.4879],\n",
      "         [0.4929, 0.5433, 0.5408, 0.4767],\n",
      "         [0.4786, 0.5543, 0.5402, 0.4636],\n",
      "         [0.5003, 0.5563, 0.5334, 0.4781],\n",
      "         [0.5022, 0.5502, 0.5371, 0.4717],\n",
      "         [0.4998, 0.5568, 0.5434, 0.4677],\n",
      "         [0.4866, 0.5506, 0.5362, 0.4767],\n",
      "         [0.4893, 0.5592, 0.5364, 0.4796],\n",
      "         [0.4919, 0.5601, 0.5390, 0.4784],\n",
      "         [0.4889, 0.5591, 0.5332, 0.4720],\n",
      "         [0.4915, 0.5568, 0.5370, 0.4799],\n",
      "         [0.5049, 0.5584, 0.5427, 0.4762],\n",
      "         [0.4905, 0.5657, 0.5324, 0.4847],\n",
      "         [0.4977, 0.5613, 0.5369, 0.4676]]], device='cuda:0'), 'pred_obj_boxes': tensor([[[0.4790, 0.4694, 0.5207, 0.5094],\n",
      "         [0.4847, 0.4657, 0.5358, 0.5085],\n",
      "         [0.4821, 0.4651, 0.5315, 0.5165],\n",
      "         [0.4945, 0.4728, 0.5133, 0.5079],\n",
      "         [0.4827, 0.4762, 0.5221, 0.5015],\n",
      "         [0.4829, 0.4705, 0.5291, 0.4995],\n",
      "         [0.4768, 0.4584, 0.5373, 0.5093],\n",
      "         [0.4797, 0.4746, 0.5215, 0.5017],\n",
      "         [0.4770, 0.4704, 0.5248, 0.4978],\n",
      "         [0.4920, 0.4697, 0.5326, 0.5068],\n",
      "         [0.4780, 0.4669, 0.5256, 0.5134],\n",
      "         [0.4708, 0.4604, 0.5348, 0.5146],\n",
      "         [0.4883, 0.4793, 0.5449, 0.5096],\n",
      "         [0.4776, 0.4661, 0.5232, 0.5037],\n",
      "         [0.4910, 0.4679, 0.5195, 0.5076],\n",
      "         [0.4936, 0.4651, 0.5297, 0.5085],\n",
      "         [0.4839, 0.4661, 0.5255, 0.5046],\n",
      "         [0.4873, 0.4697, 0.5415, 0.5032],\n",
      "         [0.4823, 0.4721, 0.5389, 0.5102],\n",
      "         [0.4835, 0.4775, 0.5384, 0.5047],\n",
      "         [0.4886, 0.4714, 0.5325, 0.5043],\n",
      "         [0.4810, 0.4664, 0.5337, 0.4998],\n",
      "         [0.4818, 0.4687, 0.5227, 0.5105],\n",
      "         [0.4985, 0.4697, 0.5331, 0.5080],\n",
      "         [0.4868, 0.4756, 0.5375, 0.5106],\n",
      "         [0.4724, 0.4496, 0.5322, 0.5081],\n",
      "         [0.4739, 0.4722, 0.5396, 0.5093],\n",
      "         [0.4900, 0.4721, 0.5317, 0.5145],\n",
      "         [0.4882, 0.4761, 0.5291, 0.5160],\n",
      "         [0.4791, 0.4717, 0.5236, 0.4972],\n",
      "         [0.4753, 0.4684, 0.5209, 0.5036],\n",
      "         [0.4864, 0.4644, 0.5179, 0.4913],\n",
      "         [0.4788, 0.4697, 0.5246, 0.5149],\n",
      "         [0.4853, 0.4595, 0.5431, 0.5001],\n",
      "         [0.4732, 0.4676, 0.5339, 0.5004],\n",
      "         [0.4871, 0.4751, 0.5364, 0.5101],\n",
      "         [0.4782, 0.4775, 0.5215, 0.5173],\n",
      "         [0.4875, 0.4693, 0.5314, 0.5064],\n",
      "         [0.4841, 0.4774, 0.5328, 0.5057],\n",
      "         [0.4786, 0.4750, 0.5265, 0.5012],\n",
      "         [0.4967, 0.4723, 0.5282, 0.5028],\n",
      "         [0.4852, 0.4725, 0.5253, 0.5027],\n",
      "         [0.4898, 0.4762, 0.5324, 0.5105],\n",
      "         [0.4826, 0.4776, 0.5296, 0.5080],\n",
      "         [0.4791, 0.4700, 0.5168, 0.5079],\n",
      "         [0.4911, 0.4666, 0.5333, 0.5055],\n",
      "         [0.4733, 0.4636, 0.5310, 0.5060],\n",
      "         [0.4762, 0.4667, 0.5226, 0.5079],\n",
      "         [0.4745, 0.4658, 0.5162, 0.5061],\n",
      "         [0.4825, 0.4630, 0.5337, 0.5195],\n",
      "         [0.4901, 0.4737, 0.5206, 0.5088],\n",
      "         [0.4983, 0.4664, 0.5271, 0.5005],\n",
      "         [0.4833, 0.4664, 0.5234, 0.5117],\n",
      "         [0.4803, 0.4654, 0.5143, 0.4964],\n",
      "         [0.4854, 0.4675, 0.5429, 0.5068],\n",
      "         [0.4755, 0.4643, 0.5265, 0.4968],\n",
      "         [0.4816, 0.4605, 0.5204, 0.4945],\n",
      "         [0.4856, 0.4740, 0.5347, 0.5128],\n",
      "         [0.4771, 0.4637, 0.5280, 0.5160],\n",
      "         [0.4732, 0.4738, 0.5298, 0.5084],\n",
      "         [0.4797, 0.4676, 0.5303, 0.5118],\n",
      "         [0.4732, 0.4765, 0.5214, 0.5080],\n",
      "         [0.4890, 0.4710, 0.5323, 0.5072],\n",
      "         [0.4781, 0.4846, 0.5374, 0.5163],\n",
      "         [0.4959, 0.4673, 0.5307, 0.5055],\n",
      "         [0.4805, 0.4728, 0.5187, 0.5157],\n",
      "         [0.4730, 0.4689, 0.5238, 0.5128],\n",
      "         [0.4844, 0.4749, 0.5213, 0.5043],\n",
      "         [0.4878, 0.4716, 0.5335, 0.5049],\n",
      "         [0.4788, 0.4743, 0.5314, 0.5058],\n",
      "         [0.4876, 0.4701, 0.5329, 0.5072],\n",
      "         [0.4725, 0.4739, 0.5277, 0.5082],\n",
      "         [0.4929, 0.4639, 0.5250, 0.5043],\n",
      "         [0.4827, 0.4688, 0.5363, 0.5085],\n",
      "         [0.4801, 0.4670, 0.5437, 0.5119],\n",
      "         [0.4768, 0.4736, 0.5243, 0.5008],\n",
      "         [0.4942, 0.4654, 0.5221, 0.5007],\n",
      "         [0.4862, 0.4645, 0.5349, 0.5031],\n",
      "         [0.4808, 0.4791, 0.5267, 0.5124],\n",
      "         [0.4937, 0.4771, 0.5207, 0.5063],\n",
      "         [0.4832, 0.4624, 0.5262, 0.5011],\n",
      "         [0.4861, 0.4767, 0.5248, 0.5083],\n",
      "         [0.4802, 0.4673, 0.5367, 0.5202],\n",
      "         [0.4835, 0.4736, 0.5381, 0.5039],\n",
      "         [0.4826, 0.4852, 0.5304, 0.5099],\n",
      "         [0.4799, 0.4736, 0.5254, 0.5141],\n",
      "         [0.4750, 0.4709, 0.5305, 0.5097],\n",
      "         [0.4783, 0.4710, 0.5294, 0.5063],\n",
      "         [0.4890, 0.4715, 0.5290, 0.5141],\n",
      "         [0.4737, 0.4662, 0.5216, 0.5170],\n",
      "         [0.4879, 0.4563, 0.5279, 0.4990],\n",
      "         [0.4870, 0.4676, 0.5204, 0.4993],\n",
      "         [0.4958, 0.4778, 0.5334, 0.5140],\n",
      "         [0.4840, 0.4641, 0.5180, 0.5016],\n",
      "         [0.4689, 0.4738, 0.5215, 0.5113],\n",
      "         [0.4809, 0.4567, 0.5299, 0.4981],\n",
      "         [0.4731, 0.4659, 0.5148, 0.5079],\n",
      "         [0.4868, 0.4697, 0.5291, 0.5160],\n",
      "         [0.4866, 0.4762, 0.5155, 0.5114],\n",
      "         [0.4910, 0.4758, 0.5339, 0.5001]]], device='cuda:0'), 'aux_outputs': [{'pred_obj_logits': tensor([[[ 0.3469,  0.0989,  1.0289,  ..., -1.0790,  0.3442,  0.8141],\n",
      "         [ 0.1822,  0.0415,  0.8959,  ..., -0.7206,  0.5333,  0.8793],\n",
      "         [ 0.4709,  0.0617,  1.0882,  ..., -0.5101,  0.7647,  1.1641],\n",
      "         ...,\n",
      "         [ 0.1614,  0.1135,  1.3755,  ..., -0.8849,  0.2213,  0.7746],\n",
      "         [ 0.6440, -0.1822,  0.8851,  ..., -0.9143,  0.3402,  0.9611],\n",
      "         [ 0.3961,  0.4988,  1.1562,  ..., -0.6978,  0.3984,  0.8118]]],\n",
      "       device='cuda:0'), 'pred_verb_logits': tensor([[[ 0.2996, -0.1676,  0.6638,  ..., -1.0358, -0.0691,  0.4931],\n",
      "         [-0.0439, -0.0985,  0.7872,  ..., -1.2696, -0.6336,  0.2326],\n",
      "         [ 0.0566, -0.3592,  0.2030,  ..., -1.2317, -0.1917, -0.0510],\n",
      "         ...,\n",
      "         [ 0.3777, -0.3419,  0.2900,  ..., -1.2692, -0.2374, -0.0530],\n",
      "         [ 0.2408, -0.4264,  0.1533,  ..., -1.0392, -0.3548,  0.0619],\n",
      "         [ 0.5086, -0.0269,  0.2744,  ..., -0.8423, -0.0497, -0.1352]]],\n",
      "       device='cuda:0'), 'pred_sub_boxes': tensor([[[0.4976, 0.5543, 0.5153, 0.4759],\n",
      "         [0.4995, 0.5627, 0.5272, 0.4742],\n",
      "         [0.5003, 0.5597, 0.5217, 0.4828],\n",
      "         [0.4844, 0.5624, 0.5239, 0.4783],\n",
      "         [0.5038, 0.5550, 0.5192, 0.4833],\n",
      "         [0.4870, 0.5630, 0.5148, 0.4792],\n",
      "         [0.4913, 0.5457, 0.5120, 0.4677],\n",
      "         [0.4876, 0.5596, 0.5262, 0.4825],\n",
      "         [0.4862, 0.5641, 0.5159, 0.4756],\n",
      "         [0.5030, 0.5669, 0.5028, 0.4800],\n",
      "         [0.4935, 0.5513, 0.5140, 0.4859],\n",
      "         [0.4871, 0.5577, 0.5165, 0.4565],\n",
      "         [0.4895, 0.5642, 0.5234, 0.4840],\n",
      "         [0.4971, 0.5643, 0.5123, 0.4843],\n",
      "         [0.4935, 0.5511, 0.5030, 0.4672],\n",
      "         [0.5022, 0.5519, 0.5177, 0.4765],\n",
      "         [0.4910, 0.5596, 0.5217, 0.4677],\n",
      "         [0.4994, 0.5540, 0.5261, 0.4847],\n",
      "         [0.4910, 0.5530, 0.5195, 0.4673],\n",
      "         [0.5068, 0.5635, 0.4998, 0.4857],\n",
      "         [0.5037, 0.5528, 0.5125, 0.4714],\n",
      "         [0.4953, 0.5453, 0.5113, 0.4717],\n",
      "         [0.4916, 0.5627, 0.5020, 0.4721],\n",
      "         [0.5057, 0.5508, 0.5145, 0.4792],\n",
      "         [0.4881, 0.5631, 0.5162, 0.4748],\n",
      "         [0.5023, 0.5585, 0.5153, 0.4663],\n",
      "         [0.4964, 0.5530, 0.5385, 0.4665],\n",
      "         [0.5008, 0.5525, 0.5044, 0.4845],\n",
      "         [0.4890, 0.5601, 0.5179, 0.4899],\n",
      "         [0.4929, 0.5513, 0.5020, 0.4768],\n",
      "         [0.4890, 0.5645, 0.5165, 0.4928],\n",
      "         [0.4989, 0.5667, 0.5096, 0.4575],\n",
      "         [0.5021, 0.5592, 0.5160, 0.4726],\n",
      "         [0.4940, 0.5472, 0.5055, 0.4782],\n",
      "         [0.4911, 0.5547, 0.5194, 0.4831],\n",
      "         [0.5008, 0.5592, 0.5198, 0.4830],\n",
      "         [0.4937, 0.5681, 0.5189, 0.4778],\n",
      "         [0.4824, 0.5479, 0.5232, 0.4689],\n",
      "         [0.4997, 0.5519, 0.5129, 0.4990],\n",
      "         [0.4963, 0.5539, 0.5156, 0.4816],\n",
      "         [0.4901, 0.5515, 0.5104, 0.4858],\n",
      "         [0.4945, 0.5603, 0.5144, 0.4706],\n",
      "         [0.5020, 0.5632, 0.5058, 0.4652],\n",
      "         [0.5020, 0.5636, 0.5068, 0.4772],\n",
      "         [0.4870, 0.5638, 0.5129, 0.4671],\n",
      "         [0.4948, 0.5539, 0.5173, 0.4791],\n",
      "         [0.5052, 0.5394, 0.5190, 0.4753],\n",
      "         [0.4983, 0.5592, 0.5122, 0.4790],\n",
      "         [0.5010, 0.5541, 0.5221, 0.4789],\n",
      "         [0.4923, 0.5476, 0.5226, 0.4729],\n",
      "         [0.4853, 0.5623, 0.5279, 0.4673],\n",
      "         [0.5032, 0.5637, 0.5215, 0.4823],\n",
      "         [0.5071, 0.5495, 0.5199, 0.4630],\n",
      "         [0.4934, 0.5639, 0.5045, 0.5191],\n",
      "         [0.4987, 0.5613, 0.5218, 0.4870],\n",
      "         [0.4866, 0.5553, 0.5175, 0.4693],\n",
      "         [0.5031, 0.5579, 0.5187, 0.4804],\n",
      "         [0.4797, 0.5457, 0.5097, 0.4671],\n",
      "         [0.4980, 0.5521, 0.5304, 0.4874],\n",
      "         [0.5033, 0.5599, 0.5226, 0.4785],\n",
      "         [0.4834, 0.5582, 0.5138, 0.4766],\n",
      "         [0.4834, 0.5363, 0.5148, 0.4760],\n",
      "         [0.4756, 0.5616, 0.5030, 0.4700],\n",
      "         [0.4890, 0.5468, 0.5242, 0.4841],\n",
      "         [0.5051, 0.5562, 0.5249, 0.4691],\n",
      "         [0.4842, 0.5601, 0.5069, 0.4765],\n",
      "         [0.4851, 0.5488, 0.5167, 0.4761],\n",
      "         [0.4868, 0.5474, 0.5093, 0.4837],\n",
      "         [0.4900, 0.5513, 0.5362, 0.4840],\n",
      "         [0.4866, 0.5418, 0.5104, 0.4838],\n",
      "         [0.4881, 0.5591, 0.5118, 0.4939],\n",
      "         [0.4885, 0.5665, 0.5098, 0.4751],\n",
      "         [0.4943, 0.5512, 0.5077, 0.4800],\n",
      "         [0.4969, 0.5519, 0.5035, 0.4745],\n",
      "         [0.4984, 0.5531, 0.5106, 0.4770],\n",
      "         [0.4974, 0.5637, 0.5207, 0.4840],\n",
      "         [0.5046, 0.5599, 0.5156, 0.4746],\n",
      "         [0.4926, 0.5549, 0.5179, 0.4750],\n",
      "         [0.4857, 0.5487, 0.5140, 0.4728],\n",
      "         [0.5014, 0.5596, 0.5162, 0.4683],\n",
      "         [0.4954, 0.5610, 0.5180, 0.4818],\n",
      "         [0.5039, 0.5471, 0.5105, 0.4566],\n",
      "         [0.5139, 0.5553, 0.5224, 0.4727],\n",
      "         [0.4996, 0.5650, 0.5172, 0.4896],\n",
      "         [0.4889, 0.5504, 0.5063, 0.4677],\n",
      "         [0.4989, 0.5530, 0.5097, 0.4958],\n",
      "         [0.4985, 0.5671, 0.5175, 0.4800],\n",
      "         [0.4901, 0.5443, 0.5127, 0.4804],\n",
      "         [0.4864, 0.5579, 0.5190, 0.4843],\n",
      "         [0.4907, 0.5581, 0.5157, 0.4753],\n",
      "         [0.4902, 0.5449, 0.5307, 0.4821],\n",
      "         [0.4960, 0.5565, 0.5180, 0.4725],\n",
      "         [0.4861, 0.5575, 0.5157, 0.4775],\n",
      "         [0.4946, 0.5568, 0.5195, 0.4730],\n",
      "         [0.5057, 0.5580, 0.5109, 0.4717],\n",
      "         [0.4828, 0.5536, 0.5148, 0.4684],\n",
      "         [0.4991, 0.5671, 0.5224, 0.4799],\n",
      "         [0.4929, 0.5538, 0.5169, 0.4820],\n",
      "         [0.4878, 0.5627, 0.5264, 0.4846],\n",
      "         [0.4881, 0.5557, 0.5201, 0.4850]]], device='cuda:0'), 'pred_obj_boxes': tensor([[[0.4602, 0.4726, 0.5152, 0.4949],\n",
      "         [0.4692, 0.4615, 0.5351, 0.5045],\n",
      "         [0.4529, 0.4538, 0.5305, 0.4973],\n",
      "         [0.4753, 0.4682, 0.5126, 0.4998],\n",
      "         [0.4598, 0.4745, 0.5325, 0.4980],\n",
      "         [0.4632, 0.4736, 0.5212, 0.4946],\n",
      "         [0.4595, 0.4614, 0.5246, 0.5079],\n",
      "         [0.4589, 0.4755, 0.5134, 0.4971],\n",
      "         [0.4523, 0.4724, 0.5276, 0.4860],\n",
      "         [0.4621, 0.4581, 0.5212, 0.4933],\n",
      "         [0.4571, 0.4691, 0.5242, 0.4938],\n",
      "         [0.4593, 0.4721, 0.5302, 0.5016],\n",
      "         [0.4760, 0.4658, 0.5314, 0.5057],\n",
      "         [0.4599, 0.4647, 0.5166, 0.5017],\n",
      "         [0.4750, 0.4624, 0.5070, 0.5069],\n",
      "         [0.4714, 0.4671, 0.5141, 0.5042],\n",
      "         [0.4627, 0.4669, 0.5216, 0.4906],\n",
      "         [0.4796, 0.4805, 0.5316, 0.4983],\n",
      "         [0.4624, 0.4711, 0.5257, 0.4938],\n",
      "         [0.4716, 0.4725, 0.5321, 0.4964],\n",
      "         [0.4726, 0.4755, 0.5410, 0.4933],\n",
      "         [0.4496, 0.4705, 0.5252, 0.4943],\n",
      "         [0.4833, 0.4661, 0.5219, 0.5096],\n",
      "         [0.4783, 0.4632, 0.5266, 0.4995],\n",
      "         [0.4597, 0.4824, 0.5311, 0.5066],\n",
      "         [0.4572, 0.4554, 0.5140, 0.4965],\n",
      "         [0.4616, 0.4749, 0.5291, 0.4941],\n",
      "         [0.4720, 0.4786, 0.5275, 0.4965],\n",
      "         [0.4597, 0.4642, 0.5205, 0.5070],\n",
      "         [0.4581, 0.4733, 0.5207, 0.4935],\n",
      "         [0.4554, 0.4603, 0.5186, 0.4897],\n",
      "         [0.4630, 0.4628, 0.5243, 0.4972],\n",
      "         [0.4575, 0.4640, 0.5159, 0.5021],\n",
      "         [0.4763, 0.4691, 0.5394, 0.5047],\n",
      "         [0.4538, 0.4653, 0.5271, 0.4990],\n",
      "         [0.4630, 0.4678, 0.5280, 0.4991],\n",
      "         [0.4545, 0.4705, 0.5262, 0.5065],\n",
      "         [0.4603, 0.4562, 0.5251, 0.4943],\n",
      "         [0.4661, 0.4737, 0.5370, 0.4963],\n",
      "         [0.4491, 0.4744, 0.5263, 0.5001],\n",
      "         [0.4719, 0.4653, 0.5106, 0.4931],\n",
      "         [0.4609, 0.4789, 0.5088, 0.4979],\n",
      "         [0.4674, 0.4792, 0.5123, 0.5040],\n",
      "         [0.4747, 0.4730, 0.5136, 0.5008],\n",
      "         [0.4612, 0.4754, 0.5111, 0.5094],\n",
      "         [0.4705, 0.4674, 0.5221, 0.5038],\n",
      "         [0.4615, 0.4776, 0.5308, 0.5101],\n",
      "         [0.4550, 0.4600, 0.5147, 0.4882],\n",
      "         [0.4616, 0.4634, 0.5224, 0.4826],\n",
      "         [0.4672, 0.4634, 0.5181, 0.5076],\n",
      "         [0.4672, 0.4693, 0.5166, 0.5028],\n",
      "         [0.4608, 0.4680, 0.5082, 0.4924],\n",
      "         [0.4644, 0.4589, 0.5238, 0.5038],\n",
      "         [0.4467, 0.4632, 0.5054, 0.5070],\n",
      "         [0.4766, 0.4641, 0.5368, 0.4984],\n",
      "         [0.4654, 0.4615, 0.5185, 0.4908],\n",
      "         [0.4588, 0.4527, 0.5150, 0.4834],\n",
      "         [0.4632, 0.4566, 0.5344, 0.5061],\n",
      "         [0.4654, 0.4685, 0.5183, 0.4962],\n",
      "         [0.4623, 0.4716, 0.5259, 0.5003],\n",
      "         [0.4529, 0.4724, 0.5264, 0.5133],\n",
      "         [0.4575, 0.4830, 0.5131, 0.4930],\n",
      "         [0.4735, 0.4723, 0.5298, 0.5070],\n",
      "         [0.4497, 0.4854, 0.5252, 0.5040],\n",
      "         [0.4771, 0.4635, 0.5300, 0.5082],\n",
      "         [0.4681, 0.4731, 0.5115, 0.4982],\n",
      "         [0.4571, 0.4639, 0.5139, 0.5050],\n",
      "         [0.4669, 0.4678, 0.5065, 0.5088],\n",
      "         [0.4643, 0.4717, 0.5110, 0.4860],\n",
      "         [0.4550, 0.4769, 0.5228, 0.4984],\n",
      "         [0.4685, 0.4758, 0.5315, 0.5071],\n",
      "         [0.4584, 0.4597, 0.5066, 0.4969],\n",
      "         [0.4829, 0.4666, 0.5220, 0.4968],\n",
      "         [0.4632, 0.4630, 0.5257, 0.4978],\n",
      "         [0.4623, 0.4749, 0.5459, 0.4999],\n",
      "         [0.4524, 0.4683, 0.5188, 0.4869],\n",
      "         [0.4768, 0.4653, 0.5166, 0.5078],\n",
      "         [0.4643, 0.4690, 0.5361, 0.4890],\n",
      "         [0.4644, 0.4735, 0.5287, 0.5182],\n",
      "         [0.4787, 0.4683, 0.5090, 0.4899],\n",
      "         [0.4663, 0.4643, 0.5276, 0.4882],\n",
      "         [0.4647, 0.4727, 0.5229, 0.5149],\n",
      "         [0.4596, 0.4691, 0.5263, 0.5128],\n",
      "         [0.4675, 0.4767, 0.5334, 0.5025],\n",
      "         [0.4641, 0.4709, 0.5266, 0.5078],\n",
      "         [0.4685, 0.4849, 0.5102, 0.4914],\n",
      "         [0.4603, 0.4576, 0.5189, 0.5034],\n",
      "         [0.4596, 0.4692, 0.5112, 0.5060],\n",
      "         [0.4673, 0.4845, 0.5224, 0.5066],\n",
      "         [0.4617, 0.4605, 0.5186, 0.5059],\n",
      "         [0.4703, 0.4550, 0.5278, 0.5020],\n",
      "         [0.4701, 0.4643, 0.5204, 0.4983],\n",
      "         [0.4705, 0.4783, 0.5281, 0.5130],\n",
      "         [0.4699, 0.4658, 0.5144, 0.4958],\n",
      "         [0.4518, 0.4647, 0.5189, 0.5058],\n",
      "         [0.4775, 0.4673, 0.5234, 0.5069],\n",
      "         [0.4561, 0.4702, 0.5166, 0.5015],\n",
      "         [0.4598, 0.4787, 0.5280, 0.5073],\n",
      "         [0.4846, 0.4720, 0.5156, 0.4997],\n",
      "         [0.4513, 0.4689, 0.5182, 0.4995]]], device='cuda:0')}, {'pred_obj_logits': tensor([[[ 0.2263, -0.1164,  0.9439,  ..., -1.2918,  0.6344,  0.8358],\n",
      "         [ 0.0838, -0.0398,  0.8194,  ..., -1.0357,  0.8891,  0.8758],\n",
      "         [ 0.2560,  0.1001,  1.1790,  ..., -0.8226,  0.8820,  1.0817],\n",
      "         ...,\n",
      "         [ 0.0364,  0.2323,  1.2409,  ..., -1.2653,  0.6948,  0.9809],\n",
      "         [ 0.4467, -0.1387,  0.8609,  ..., -0.9690,  0.5519,  0.9025],\n",
      "         [ 0.2022,  0.3861,  1.1066,  ..., -0.9011,  0.8307,  0.8169]]],\n",
      "       device='cuda:0'), 'pred_verb_logits': tensor([[[ 0.0663, -0.0840,  0.6324,  ..., -1.0489, -0.3523,  0.3327],\n",
      "         [-0.2148, -0.0668,  0.5156,  ..., -1.3765, -0.8039,  0.1306],\n",
      "         [ 0.0074, -0.1018,  0.1363,  ..., -1.0319, -0.3095, -0.2575],\n",
      "         ...,\n",
      "         [ 0.0434, -0.2021,  0.3288,  ..., -1.0422, -0.4842, -0.0709],\n",
      "         [-0.0987, -0.4024,  0.0166,  ..., -0.9137, -0.3995, -0.1081],\n",
      "         [ 0.3832,  0.0316,  0.2188,  ..., -0.8098, -0.2016, -0.1933]]],\n",
      "       device='cuda:0'), 'pred_sub_boxes': tensor([[[0.4888, 0.5539, 0.5311, 0.4866],\n",
      "         [0.4890, 0.5588, 0.5380, 0.4747],\n",
      "         [0.5011, 0.5533, 0.5329, 0.4789],\n",
      "         [0.4865, 0.5562, 0.5296, 0.4805],\n",
      "         [0.5005, 0.5582, 0.5353, 0.4896],\n",
      "         [0.4853, 0.5582, 0.5296, 0.4747],\n",
      "         [0.4934, 0.5488, 0.5314, 0.4685],\n",
      "         [0.4854, 0.5569, 0.5364, 0.4880],\n",
      "         [0.4893, 0.5637, 0.5301, 0.4801],\n",
      "         [0.4952, 0.5669, 0.5253, 0.4703],\n",
      "         [0.4955, 0.5487, 0.5283, 0.4849],\n",
      "         [0.4919, 0.5546, 0.5275, 0.4687],\n",
      "         [0.4922, 0.5700, 0.5384, 0.4904],\n",
      "         [0.4838, 0.5545, 0.5262, 0.4811],\n",
      "         [0.4843, 0.5539, 0.5206, 0.4683],\n",
      "         [0.4928, 0.5518, 0.5329, 0.4894],\n",
      "         [0.4907, 0.5615, 0.5393, 0.4668],\n",
      "         [0.4875, 0.5559, 0.5348, 0.4869],\n",
      "         [0.4985, 0.5487, 0.5380, 0.4735],\n",
      "         [0.4927, 0.5612, 0.5219, 0.4824],\n",
      "         [0.5035, 0.5550, 0.5236, 0.4768],\n",
      "         [0.4888, 0.5495, 0.5301, 0.4741],\n",
      "         [0.4902, 0.5555, 0.5236, 0.4774],\n",
      "         [0.5034, 0.5472, 0.5291, 0.4748],\n",
      "         [0.4912, 0.5566, 0.5342, 0.4684],\n",
      "         [0.5001, 0.5590, 0.5289, 0.4737],\n",
      "         [0.4927, 0.5492, 0.5448, 0.4711],\n",
      "         [0.5002, 0.5483, 0.5233, 0.4889],\n",
      "         [0.4882, 0.5638, 0.5342, 0.4862],\n",
      "         [0.4933, 0.5486, 0.5271, 0.4697],\n",
      "         [0.4874, 0.5616, 0.5303, 0.4842],\n",
      "         [0.4966, 0.5618, 0.5298, 0.4676],\n",
      "         [0.4999, 0.5607, 0.5278, 0.4668],\n",
      "         [0.4907, 0.5414, 0.5224, 0.4781],\n",
      "         [0.4863, 0.5585, 0.5290, 0.4741],\n",
      "         [0.4911, 0.5671, 0.5249, 0.4802],\n",
      "         [0.4949, 0.5595, 0.5396, 0.4751],\n",
      "         [0.4855, 0.5467, 0.5415, 0.4727],\n",
      "         [0.4931, 0.5532, 0.5307, 0.4895],\n",
      "         [0.4949, 0.5543, 0.5341, 0.4812],\n",
      "         [0.4815, 0.5491, 0.5354, 0.4904],\n",
      "         [0.4884, 0.5594, 0.5248, 0.4847],\n",
      "         [0.4965, 0.5552, 0.5301, 0.4715],\n",
      "         [0.4936, 0.5621, 0.5321, 0.4829],\n",
      "         [0.4904, 0.5562, 0.5325, 0.4712],\n",
      "         [0.5045, 0.5592, 0.5279, 0.4818],\n",
      "         [0.5043, 0.5403, 0.5314, 0.4735],\n",
      "         [0.5016, 0.5531, 0.5332, 0.4758],\n",
      "         [0.4976, 0.5544, 0.5333, 0.4769],\n",
      "         [0.4876, 0.5496, 0.5418, 0.4824],\n",
      "         [0.4774, 0.5600, 0.5425, 0.4669],\n",
      "         [0.4987, 0.5639, 0.5341, 0.4770],\n",
      "         [0.5057, 0.5529, 0.5352, 0.4670],\n",
      "         [0.4902, 0.5689, 0.5238, 0.5138],\n",
      "         [0.4988, 0.5594, 0.5376, 0.4769],\n",
      "         [0.4935, 0.5615, 0.5309, 0.4758],\n",
      "         [0.5093, 0.5658, 0.5284, 0.4822],\n",
      "         [0.4770, 0.5467, 0.5284, 0.4731],\n",
      "         [0.4956, 0.5547, 0.5370, 0.4792],\n",
      "         [0.5069, 0.5643, 0.5387, 0.4814],\n",
      "         [0.4867, 0.5521, 0.5300, 0.4814],\n",
      "         [0.4886, 0.5391, 0.5270, 0.4767],\n",
      "         [0.4878, 0.5657, 0.5197, 0.4720],\n",
      "         [0.4839, 0.5458, 0.5396, 0.4763],\n",
      "         [0.4917, 0.5573, 0.5386, 0.4735],\n",
      "         [0.4808, 0.5657, 0.5279, 0.4773],\n",
      "         [0.4897, 0.5451, 0.5245, 0.4842],\n",
      "         [0.4888, 0.5444, 0.5225, 0.4798],\n",
      "         [0.4920, 0.5453, 0.5542, 0.4892],\n",
      "         [0.4815, 0.5397, 0.5257, 0.4719],\n",
      "         [0.4881, 0.5547, 0.5236, 0.4831],\n",
      "         [0.4824, 0.5593, 0.5260, 0.4826],\n",
      "         [0.5001, 0.5557, 0.5255, 0.4769],\n",
      "         [0.4907, 0.5516, 0.5285, 0.4717],\n",
      "         [0.4921, 0.5646, 0.5324, 0.4809],\n",
      "         [0.4864, 0.5657, 0.5318, 0.4876],\n",
      "         [0.5014, 0.5507, 0.5348, 0.4794],\n",
      "         [0.4959, 0.5628, 0.5373, 0.4740],\n",
      "         [0.4931, 0.5423, 0.5252, 0.4681],\n",
      "         [0.4978, 0.5527, 0.5383, 0.4711],\n",
      "         [0.4866, 0.5644, 0.5303, 0.4848],\n",
      "         [0.4930, 0.5461, 0.5253, 0.4631],\n",
      "         [0.5094, 0.5570, 0.5416, 0.4837],\n",
      "         [0.4938, 0.5623, 0.5291, 0.4863],\n",
      "         [0.4834, 0.5500, 0.5331, 0.4737],\n",
      "         [0.4982, 0.5532, 0.5215, 0.4854],\n",
      "         [0.4938, 0.5576, 0.5283, 0.4925],\n",
      "         [0.4885, 0.5445, 0.5307, 0.4819],\n",
      "         [0.4861, 0.5528, 0.5375, 0.4760],\n",
      "         [0.4955, 0.5566, 0.5206, 0.4707],\n",
      "         [0.5008, 0.5472, 0.5342, 0.4816],\n",
      "         [0.4992, 0.5571, 0.5365, 0.4765],\n",
      "         [0.4866, 0.5510, 0.5336, 0.4780],\n",
      "         [0.4883, 0.5639, 0.5326, 0.4790],\n",
      "         [0.4947, 0.5590, 0.5297, 0.4734],\n",
      "         [0.4787, 0.5606, 0.5262, 0.4679],\n",
      "         [0.4917, 0.5581, 0.5335, 0.4872],\n",
      "         [0.4953, 0.5573, 0.5313, 0.4833],\n",
      "         [0.4865, 0.5651, 0.5378, 0.4849],\n",
      "         [0.4973, 0.5580, 0.5322, 0.4799]]], device='cuda:0'), 'pred_obj_boxes': tensor([[[0.4643, 0.4676, 0.5235, 0.5097],\n",
      "         [0.4813, 0.4630, 0.5478, 0.5108],\n",
      "         [0.4721, 0.4579, 0.5379, 0.5155],\n",
      "         [0.4875, 0.4704, 0.5151, 0.5062],\n",
      "         [0.4707, 0.4757, 0.5254, 0.5032],\n",
      "         [0.4767, 0.4728, 0.5221, 0.4968],\n",
      "         [0.4719, 0.4586, 0.5325, 0.5090],\n",
      "         [0.4776, 0.4817, 0.5239, 0.5003],\n",
      "         [0.4604, 0.4708, 0.5259, 0.4959],\n",
      "         [0.4746, 0.4675, 0.5305, 0.4942],\n",
      "         [0.4717, 0.4685, 0.5292, 0.5121],\n",
      "         [0.4675, 0.4679, 0.5351, 0.5081],\n",
      "         [0.4845, 0.4719, 0.5426, 0.5102],\n",
      "         [0.4718, 0.4673, 0.5202, 0.5054],\n",
      "         [0.4907, 0.4666, 0.5178, 0.5184],\n",
      "         [0.4840, 0.4698, 0.5235, 0.5109],\n",
      "         [0.4729, 0.4674, 0.5260, 0.4937],\n",
      "         [0.4865, 0.4750, 0.5460, 0.5024],\n",
      "         [0.4766, 0.4726, 0.5390, 0.5110],\n",
      "         [0.4742, 0.4736, 0.5359, 0.5061],\n",
      "         [0.4828, 0.4763, 0.5387, 0.5010],\n",
      "         [0.4638, 0.4666, 0.5354, 0.4988],\n",
      "         [0.4869, 0.4745, 0.5307, 0.5149],\n",
      "         [0.4966, 0.4664, 0.5399, 0.5116],\n",
      "         [0.4757, 0.4783, 0.5367, 0.5221],\n",
      "         [0.4641, 0.4490, 0.5272, 0.5044],\n",
      "         [0.4635, 0.4758, 0.5322, 0.5078],\n",
      "         [0.4828, 0.4770, 0.5348, 0.5120],\n",
      "         [0.4778, 0.4696, 0.5320, 0.5062],\n",
      "         [0.4660, 0.4707, 0.5250, 0.4966],\n",
      "         [0.4632, 0.4636, 0.5221, 0.4939],\n",
      "         [0.4783, 0.4623, 0.5227, 0.4987],\n",
      "         [0.4727, 0.4620, 0.5241, 0.5021],\n",
      "         [0.4776, 0.4641, 0.5436, 0.5035],\n",
      "         [0.4657, 0.4638, 0.5400, 0.5064],\n",
      "         [0.4778, 0.4789, 0.5354, 0.5096],\n",
      "         [0.4662, 0.4740, 0.5285, 0.5177],\n",
      "         [0.4748, 0.4641, 0.5283, 0.5055],\n",
      "         [0.4777, 0.4770, 0.5341, 0.4997],\n",
      "         [0.4713, 0.4741, 0.5281, 0.4985],\n",
      "         [0.4830, 0.4693, 0.5221, 0.5083],\n",
      "         [0.4758, 0.4800, 0.5161, 0.5092],\n",
      "         [0.4817, 0.4782, 0.5237, 0.5086],\n",
      "         [0.4775, 0.4773, 0.5126, 0.5092],\n",
      "         [0.4676, 0.4733, 0.5178, 0.5170],\n",
      "         [0.4886, 0.4712, 0.5358, 0.5067],\n",
      "         [0.4707, 0.4660, 0.5314, 0.5143],\n",
      "         [0.4692, 0.4661, 0.5179, 0.5053],\n",
      "         [0.4678, 0.4642, 0.5250, 0.4951],\n",
      "         [0.4815, 0.4685, 0.5283, 0.5205],\n",
      "         [0.4751, 0.4675, 0.5175, 0.5044],\n",
      "         [0.4799, 0.4636, 0.5196, 0.4985],\n",
      "         [0.4788, 0.4642, 0.5283, 0.5101],\n",
      "         [0.4563, 0.4679, 0.5138, 0.5038],\n",
      "         [0.4812, 0.4713, 0.5371, 0.5070],\n",
      "         [0.4759, 0.4615, 0.5270, 0.4977],\n",
      "         [0.4708, 0.4490, 0.5192, 0.4901],\n",
      "         [0.4722, 0.4643, 0.5348, 0.5129],\n",
      "         [0.4699, 0.4607, 0.5271, 0.5115],\n",
      "         [0.4685, 0.4718, 0.5358, 0.4994],\n",
      "         [0.4722, 0.4675, 0.5314, 0.5180],\n",
      "         [0.4654, 0.4840, 0.5181, 0.5099],\n",
      "         [0.4802, 0.4698, 0.5378, 0.5122],\n",
      "         [0.4672, 0.4902, 0.5319, 0.5144],\n",
      "         [0.4820, 0.4701, 0.5280, 0.5084],\n",
      "         [0.4810, 0.4681, 0.5171, 0.5127],\n",
      "         [0.4675, 0.4698, 0.5260, 0.5138],\n",
      "         [0.4763, 0.4753, 0.5191, 0.5088],\n",
      "         [0.4797, 0.4675, 0.5263, 0.4952],\n",
      "         [0.4650, 0.4714, 0.5312, 0.5099],\n",
      "         [0.4737, 0.4697, 0.5280, 0.5036],\n",
      "         [0.4706, 0.4771, 0.5182, 0.5032],\n",
      "         [0.4813, 0.4705, 0.5234, 0.5029],\n",
      "         [0.4763, 0.4649, 0.5336, 0.5104],\n",
      "         [0.4754, 0.4715, 0.5458, 0.5050],\n",
      "         [0.4661, 0.4715, 0.5278, 0.5017],\n",
      "         [0.4983, 0.4653, 0.5227, 0.5076],\n",
      "         [0.4809, 0.4677, 0.5369, 0.5030],\n",
      "         [0.4744, 0.4772, 0.5339, 0.5160],\n",
      "         [0.4854, 0.4794, 0.5109, 0.4956],\n",
      "         [0.4780, 0.4699, 0.5259, 0.5052],\n",
      "         [0.4840, 0.4756, 0.5341, 0.5163],\n",
      "         [0.4630, 0.4612, 0.5375, 0.5283],\n",
      "         [0.4824, 0.4741, 0.5394, 0.5036],\n",
      "         [0.4771, 0.4803, 0.5326, 0.5076],\n",
      "         [0.4742, 0.4747, 0.5253, 0.5046],\n",
      "         [0.4610, 0.4654, 0.5306, 0.5080],\n",
      "         [0.4641, 0.4683, 0.5294, 0.5107],\n",
      "         [0.4800, 0.4805, 0.5289, 0.5128],\n",
      "         [0.4697, 0.4702, 0.5223, 0.5139],\n",
      "         [0.4876, 0.4534, 0.5297, 0.5107],\n",
      "         [0.4789, 0.4676, 0.5224, 0.5001],\n",
      "         [0.4916, 0.4810, 0.5398, 0.5215],\n",
      "         [0.4804, 0.4699, 0.5186, 0.5017],\n",
      "         [0.4588, 0.4693, 0.5188, 0.5152],\n",
      "         [0.4812, 0.4652, 0.5287, 0.5078],\n",
      "         [0.4743, 0.4620, 0.5206, 0.5052],\n",
      "         [0.4734, 0.4721, 0.5337, 0.5215],\n",
      "         [0.4896, 0.4764, 0.5192, 0.5048],\n",
      "         [0.4735, 0.4710, 0.5297, 0.5037]]], device='cuda:0')}]}\n"
     ]
    }
   ],
   "source": [
    "t = []\n",
    "t.append(transforms.Resize(800, interpolation=Image.BICUBIC))\n",
    "transform = transforms.Compose(t)\n",
    "pixel_mean = torch.Tensor([123.675, 116.280, 103.530]).view(-1, 1, 1).cuda()\n",
    "pixel_std = torch.Tensor([58.395, 57.120, 57.375]).view(-1, 1, 1).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_ori = Image.open(image_pth).convert('RGB')\n",
    "    width = image_ori.size[0]\n",
    "    height = image_ori.size[1]\n",
    "    image = transform(image_ori)\n",
    "    image = np.asarray(image)\n",
    "    image_ori = np.asarray(image_ori)\n",
    "    images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "    batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "    \n",
    "    images = [x[\"image\"].to(\"cuda\") for x in batch_inputs]\n",
    "    images = [(x - pixel_mean) / pixel_std for x in images]\n",
    "    images = ImageList.from_tensors(images, 32)\n",
    "\n",
    "    bs, c, h, w = images.tensor.shape\n",
    "    # pos_embed = position_embedding.flatten(2).permute(2, 0, 1)\n",
    "    _query_embed = query_embed.weight.unsqueeze(1).repeat(1, bs, 1)\n",
    "\n",
    "    features = backbone(images.tensor)\n",
    "    encoder_features, pos = hoi_encoder(features)\n",
    "    pos_embed = pos.flatten(2).permute(2, 0, 1)\n",
    "    hopd_out, interaction_decoder_out, memory = hoi_decoder(encoder_features, None, _query_embed, pos_embed)\n",
    "    \n",
    "    outputs_sub_coord = sub_bbox_embed(hopd_out).sigmoid()\n",
    "    outputs_obj_coord = obj_bbox_embed(hopd_out).sigmoid()\n",
    "    outputs_obj_class = obj_class_embed(hopd_out)\n",
    "    outputs_verb_class = verb_class_embed(interaction_decoder_out)\n",
    "\n",
    "    out = {\n",
    "        'pred_obj_logits': outputs_obj_class[-1], \n",
    "        'pred_verb_logits': outputs_verb_class[-1],\n",
    "        'pred_sub_boxes': outputs_sub_coord[-1], \n",
    "        'pred_obj_boxes': outputs_obj_coord[-1]}        \n",
    "                                     \n",
    "    out['aux_outputs'] = _set_aux_loss(\n",
    "        outputs_obj_class, \n",
    "        outputs_verb_class,\n",
    "        outputs_sub_coord,\n",
    "        outputs_obj_coord)\n",
    "    \n",
    "    print(out)\n",
    "    # loss_dict = criterion(out, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_obj_logits': tensor([[[ 0.0716, -0.1811,  0.8853,  ..., -1.3349,  0.9013,  0.7508],\n",
       "          [ 0.1343, -0.1103,  0.5826,  ..., -1.0912,  1.0660,  0.8378],\n",
       "          [ 0.1196,  0.1614,  0.8855,  ..., -1.0058,  0.9663,  1.0236],\n",
       "          ...,\n",
       "          [-0.1798,  0.0443,  0.9540,  ..., -1.3170,  0.9375,  1.0993],\n",
       "          [ 0.2225, -0.1904,  0.7245,  ..., -0.9400,  0.6332,  0.7639],\n",
       "          [-0.0591,  0.2582,  1.0210,  ..., -1.0246,  1.1216,  0.8750]]],\n",
       "        device='cuda:0'),\n",
       " 'pred_verb_logits': tensor([[[-0.0118, -0.2418,  0.2691,  ..., -0.7811, -0.6395,  0.2486],\n",
       "          [-0.4491, -0.1349,  0.2583,  ..., -1.1000, -1.0485,  0.1363],\n",
       "          [-0.1654, -0.2236,  0.1395,  ..., -0.8794, -0.5250, -0.3411],\n",
       "          ...,\n",
       "          [-0.1160, -0.2981,  0.2313,  ..., -0.8096, -0.4913, -0.0524],\n",
       "          [-0.1175, -0.3126,  0.1487,  ..., -0.7988, -0.4732, -0.2040],\n",
       "          [ 0.2337, -0.1062,  0.1518,  ..., -0.5899, -0.3760, -0.1633]]],\n",
       "        device='cuda:0'),\n",
       " 'pred_sub_boxes': tensor([[[0.4903, 0.5543, 0.5423, 0.4823],\n",
       "          [0.4993, 0.5583, 0.5435, 0.4773],\n",
       "          [0.5018, 0.5532, 0.5425, 0.4750],\n",
       "          [0.4970, 0.5531, 0.5402, 0.4801],\n",
       "          [0.4967, 0.5543, 0.5400, 0.4772],\n",
       "          [0.4892, 0.5556, 0.5353, 0.4673],\n",
       "          [0.4907, 0.5465, 0.5282, 0.4730],\n",
       "          [0.4899, 0.5578, 0.5446, 0.4763],\n",
       "          [0.4839, 0.5548, 0.5355, 0.4791],\n",
       "          [0.4902, 0.5629, 0.5352, 0.4639],\n",
       "          [0.5008, 0.5499, 0.5442, 0.4706],\n",
       "          [0.4932, 0.5591, 0.5311, 0.4669],\n",
       "          [0.4931, 0.5665, 0.5446, 0.4794],\n",
       "          [0.4845, 0.5583, 0.5259, 0.4763],\n",
       "          [0.4955, 0.5563, 0.5369, 0.4683],\n",
       "          [0.4887, 0.5509, 0.5404, 0.4768],\n",
       "          [0.4963, 0.5542, 0.5459, 0.4673],\n",
       "          [0.4902, 0.5575, 0.5338, 0.4829],\n",
       "          [0.4960, 0.5464, 0.5391, 0.4687],\n",
       "          [0.4973, 0.5573, 0.5409, 0.4790],\n",
       "          [0.5059, 0.5527, 0.5264, 0.4786],\n",
       "          [0.4938, 0.5483, 0.5391, 0.4686],\n",
       "          [0.5032, 0.5555, 0.5416, 0.4790],\n",
       "          [0.5039, 0.5428, 0.5473, 0.4647],\n",
       "          [0.4983, 0.5501, 0.5379, 0.4644],\n",
       "          [0.4988, 0.5569, 0.5402, 0.4712],\n",
       "          [0.5025, 0.5588, 0.5430, 0.4734],\n",
       "          [0.4974, 0.5534, 0.5340, 0.4835],\n",
       "          [0.4922, 0.5589, 0.5445, 0.4882],\n",
       "          [0.4921, 0.5558, 0.5361, 0.4766],\n",
       "          [0.4910, 0.5654, 0.5333, 0.4756],\n",
       "          [0.5061, 0.5640, 0.5400, 0.4678],\n",
       "          [0.5022, 0.5554, 0.5361, 0.4701],\n",
       "          [0.4940, 0.5421, 0.5284, 0.4723],\n",
       "          [0.4917, 0.5567, 0.5362, 0.4775],\n",
       "          [0.4888, 0.5692, 0.5352, 0.4772],\n",
       "          [0.4953, 0.5547, 0.5422, 0.4766],\n",
       "          [0.4868, 0.5495, 0.5437, 0.4748],\n",
       "          [0.4992, 0.5449, 0.5353, 0.4852],\n",
       "          [0.4982, 0.5548, 0.5406, 0.4714],\n",
       "          [0.4853, 0.5498, 0.5493, 0.4791],\n",
       "          [0.4954, 0.5600, 0.5310, 0.4800],\n",
       "          [0.4965, 0.5630, 0.5353, 0.4635],\n",
       "          [0.5013, 0.5592, 0.5461, 0.4791],\n",
       "          [0.4952, 0.5542, 0.5385, 0.4693],\n",
       "          [0.5081, 0.5591, 0.5341, 0.4745],\n",
       "          [0.5111, 0.5475, 0.5359, 0.4742],\n",
       "          [0.4971, 0.5540, 0.5425, 0.4774],\n",
       "          [0.4970, 0.5523, 0.5481, 0.4678],\n",
       "          [0.4926, 0.5553, 0.5433, 0.4716],\n",
       "          [0.4931, 0.5591, 0.5463, 0.4642],\n",
       "          [0.4938, 0.5605, 0.5383, 0.4753],\n",
       "          [0.5079, 0.5498, 0.5425, 0.4720],\n",
       "          [0.4912, 0.5637, 0.5327, 0.4959],\n",
       "          [0.4909, 0.5638, 0.5468, 0.4779],\n",
       "          [0.4897, 0.5523, 0.5361, 0.4740],\n",
       "          [0.5110, 0.5623, 0.5365, 0.4794],\n",
       "          [0.4933, 0.5468, 0.5420, 0.4701],\n",
       "          [0.4948, 0.5508, 0.5403, 0.4743],\n",
       "          [0.5007, 0.5637, 0.5391, 0.4766],\n",
       "          [0.5040, 0.5543, 0.5436, 0.4785],\n",
       "          [0.4924, 0.5439, 0.5345, 0.4766],\n",
       "          [0.4929, 0.5646, 0.5319, 0.4773],\n",
       "          [0.4842, 0.5466, 0.5383, 0.4783],\n",
       "          [0.4965, 0.5534, 0.5485, 0.4703],\n",
       "          [0.4860, 0.5651, 0.5324, 0.4833],\n",
       "          [0.4953, 0.5482, 0.5303, 0.4844],\n",
       "          [0.4959, 0.5488, 0.5309, 0.4694],\n",
       "          [0.4918, 0.5459, 0.5571, 0.4834],\n",
       "          [0.4913, 0.5478, 0.5267, 0.4710],\n",
       "          [0.5028, 0.5547, 0.5319, 0.4749],\n",
       "          [0.4899, 0.5592, 0.5327, 0.4706],\n",
       "          [0.5011, 0.5631, 0.5280, 0.4783],\n",
       "          [0.4921, 0.5572, 0.5373, 0.4715],\n",
       "          [0.4896, 0.5601, 0.5428, 0.4724],\n",
       "          [0.4888, 0.5625, 0.5357, 0.4841],\n",
       "          [0.4959, 0.5569, 0.5406, 0.4752],\n",
       "          [0.4931, 0.5614, 0.5317, 0.4632],\n",
       "          [0.4941, 0.5505, 0.5337, 0.4675],\n",
       "          [0.4976, 0.5553, 0.5456, 0.4683],\n",
       "          [0.4947, 0.5591, 0.5381, 0.4767],\n",
       "          [0.4868, 0.5494, 0.5294, 0.4693],\n",
       "          [0.5047, 0.5547, 0.5402, 0.4796],\n",
       "          [0.4928, 0.5609, 0.5344, 0.4808],\n",
       "          [0.4804, 0.5477, 0.5424, 0.4742],\n",
       "          [0.5099, 0.5519, 0.5450, 0.4811],\n",
       "          [0.5002, 0.5577, 0.5384, 0.4879],\n",
       "          [0.4929, 0.5433, 0.5408, 0.4767],\n",
       "          [0.4786, 0.5543, 0.5402, 0.4636],\n",
       "          [0.5003, 0.5563, 0.5334, 0.4781],\n",
       "          [0.5022, 0.5502, 0.5371, 0.4717],\n",
       "          [0.4998, 0.5568, 0.5434, 0.4677],\n",
       "          [0.4866, 0.5506, 0.5362, 0.4767],\n",
       "          [0.4893, 0.5592, 0.5364, 0.4796],\n",
       "          [0.4919, 0.5601, 0.5390, 0.4784],\n",
       "          [0.4889, 0.5591, 0.5332, 0.4720],\n",
       "          [0.4915, 0.5568, 0.5370, 0.4799],\n",
       "          [0.5049, 0.5584, 0.5427, 0.4762],\n",
       "          [0.4905, 0.5657, 0.5324, 0.4847],\n",
       "          [0.4977, 0.5613, 0.5369, 0.4676]]], device='cuda:0'),\n",
       " 'pred_obj_boxes': tensor([[[0.4790, 0.4694, 0.5207, 0.5094],\n",
       "          [0.4847, 0.4657, 0.5358, 0.5085],\n",
       "          [0.4821, 0.4651, 0.5315, 0.5165],\n",
       "          [0.4945, 0.4728, 0.5133, 0.5079],\n",
       "          [0.4827, 0.4762, 0.5221, 0.5015],\n",
       "          [0.4829, 0.4705, 0.5291, 0.4995],\n",
       "          [0.4768, 0.4584, 0.5373, 0.5093],\n",
       "          [0.4797, 0.4746, 0.5215, 0.5017],\n",
       "          [0.4770, 0.4704, 0.5248, 0.4978],\n",
       "          [0.4920, 0.4697, 0.5326, 0.5068],\n",
       "          [0.4780, 0.4669, 0.5256, 0.5134],\n",
       "          [0.4708, 0.4604, 0.5348, 0.5146],\n",
       "          [0.4883, 0.4793, 0.5449, 0.5096],\n",
       "          [0.4776, 0.4661, 0.5232, 0.5037],\n",
       "          [0.4910, 0.4679, 0.5195, 0.5076],\n",
       "          [0.4936, 0.4651, 0.5297, 0.5085],\n",
       "          [0.4839, 0.4661, 0.5255, 0.5046],\n",
       "          [0.4873, 0.4697, 0.5415, 0.5032],\n",
       "          [0.4823, 0.4721, 0.5389, 0.5102],\n",
       "          [0.4835, 0.4775, 0.5384, 0.5047],\n",
       "          [0.4886, 0.4714, 0.5325, 0.5043],\n",
       "          [0.4810, 0.4664, 0.5337, 0.4998],\n",
       "          [0.4818, 0.4687, 0.5227, 0.5105],\n",
       "          [0.4985, 0.4697, 0.5331, 0.5080],\n",
       "          [0.4868, 0.4756, 0.5375, 0.5106],\n",
       "          [0.4724, 0.4496, 0.5322, 0.5081],\n",
       "          [0.4739, 0.4722, 0.5396, 0.5093],\n",
       "          [0.4900, 0.4721, 0.5317, 0.5145],\n",
       "          [0.4882, 0.4761, 0.5291, 0.5160],\n",
       "          [0.4791, 0.4717, 0.5236, 0.4972],\n",
       "          [0.4753, 0.4684, 0.5209, 0.5036],\n",
       "          [0.4864, 0.4644, 0.5179, 0.4913],\n",
       "          [0.4788, 0.4697, 0.5246, 0.5149],\n",
       "          [0.4853, 0.4595, 0.5431, 0.5001],\n",
       "          [0.4732, 0.4676, 0.5339, 0.5004],\n",
       "          [0.4871, 0.4751, 0.5364, 0.5101],\n",
       "          [0.4782, 0.4775, 0.5215, 0.5173],\n",
       "          [0.4875, 0.4693, 0.5314, 0.5064],\n",
       "          [0.4841, 0.4774, 0.5328, 0.5057],\n",
       "          [0.4786, 0.4750, 0.5265, 0.5012],\n",
       "          [0.4967, 0.4723, 0.5282, 0.5028],\n",
       "          [0.4852, 0.4725, 0.5253, 0.5027],\n",
       "          [0.4898, 0.4762, 0.5324, 0.5105],\n",
       "          [0.4826, 0.4776, 0.5296, 0.5080],\n",
       "          [0.4791, 0.4700, 0.5168, 0.5079],\n",
       "          [0.4911, 0.4666, 0.5333, 0.5055],\n",
       "          [0.4733, 0.4636, 0.5310, 0.5060],\n",
       "          [0.4762, 0.4667, 0.5226, 0.5079],\n",
       "          [0.4745, 0.4658, 0.5162, 0.5061],\n",
       "          [0.4825, 0.4630, 0.5337, 0.5195],\n",
       "          [0.4901, 0.4737, 0.5206, 0.5088],\n",
       "          [0.4983, 0.4664, 0.5271, 0.5005],\n",
       "          [0.4833, 0.4664, 0.5234, 0.5117],\n",
       "          [0.4803, 0.4654, 0.5143, 0.4964],\n",
       "          [0.4854, 0.4675, 0.5429, 0.5068],\n",
       "          [0.4755, 0.4643, 0.5265, 0.4968],\n",
       "          [0.4816, 0.4605, 0.5204, 0.4945],\n",
       "          [0.4856, 0.4740, 0.5347, 0.5128],\n",
       "          [0.4771, 0.4637, 0.5280, 0.5160],\n",
       "          [0.4732, 0.4738, 0.5298, 0.5084],\n",
       "          [0.4797, 0.4676, 0.5303, 0.5118],\n",
       "          [0.4732, 0.4765, 0.5214, 0.5080],\n",
       "          [0.4890, 0.4710, 0.5323, 0.5072],\n",
       "          [0.4781, 0.4846, 0.5374, 0.5163],\n",
       "          [0.4959, 0.4673, 0.5307, 0.5055],\n",
       "          [0.4805, 0.4728, 0.5187, 0.5157],\n",
       "          [0.4730, 0.4689, 0.5238, 0.5128],\n",
       "          [0.4844, 0.4749, 0.5213, 0.5043],\n",
       "          [0.4878, 0.4716, 0.5335, 0.5049],\n",
       "          [0.4788, 0.4743, 0.5314, 0.5058],\n",
       "          [0.4876, 0.4701, 0.5329, 0.5072],\n",
       "          [0.4725, 0.4739, 0.5277, 0.5082],\n",
       "          [0.4929, 0.4639, 0.5250, 0.5043],\n",
       "          [0.4827, 0.4688, 0.5363, 0.5085],\n",
       "          [0.4801, 0.4670, 0.5437, 0.5119],\n",
       "          [0.4768, 0.4736, 0.5243, 0.5008],\n",
       "          [0.4942, 0.4654, 0.5221, 0.5007],\n",
       "          [0.4862, 0.4645, 0.5349, 0.5031],\n",
       "          [0.4808, 0.4791, 0.5267, 0.5124],\n",
       "          [0.4937, 0.4771, 0.5207, 0.5063],\n",
       "          [0.4832, 0.4624, 0.5262, 0.5011],\n",
       "          [0.4861, 0.4767, 0.5248, 0.5083],\n",
       "          [0.4802, 0.4673, 0.5367, 0.5202],\n",
       "          [0.4835, 0.4736, 0.5381, 0.5039],\n",
       "          [0.4826, 0.4852, 0.5304, 0.5099],\n",
       "          [0.4799, 0.4736, 0.5254, 0.5141],\n",
       "          [0.4750, 0.4709, 0.5305, 0.5097],\n",
       "          [0.4783, 0.4710, 0.5294, 0.5063],\n",
       "          [0.4890, 0.4715, 0.5290, 0.5141],\n",
       "          [0.4737, 0.4662, 0.5216, 0.5170],\n",
       "          [0.4879, 0.4563, 0.5279, 0.4990],\n",
       "          [0.4870, 0.4676, 0.5204, 0.4993],\n",
       "          [0.4958, 0.4778, 0.5334, 0.5140],\n",
       "          [0.4840, 0.4641, 0.5180, 0.5016],\n",
       "          [0.4689, 0.4738, 0.5215, 0.5113],\n",
       "          [0.4809, 0.4567, 0.5299, 0.4981],\n",
       "          [0.4731, 0.4659, 0.5148, 0.5079],\n",
       "          [0.4868, 0.4697, 0.5291, 0.5160],\n",
       "          [0.4866, 0.4762, 0.5155, 0.5114],\n",
       "          [0.4910, 0.4758, 0.5339, 0.5001]]], device='cuda:0'),\n",
       " 'aux_outputs': [{'pred_obj_logits': tensor([[[ 0.3469,  0.0989,  1.0289,  ..., -1.0790,  0.3442,  0.8141],\n",
       "            [ 0.1822,  0.0415,  0.8959,  ..., -0.7206,  0.5333,  0.8793],\n",
       "            [ 0.4709,  0.0617,  1.0882,  ..., -0.5101,  0.7647,  1.1641],\n",
       "            ...,\n",
       "            [ 0.1614,  0.1135,  1.3755,  ..., -0.8849,  0.2213,  0.7746],\n",
       "            [ 0.6440, -0.1822,  0.8851,  ..., -0.9143,  0.3402,  0.9611],\n",
       "            [ 0.3961,  0.4988,  1.1562,  ..., -0.6978,  0.3984,  0.8118]]],\n",
       "          device='cuda:0'),\n",
       "   'pred_verb_logits': tensor([[[ 0.2996, -0.1676,  0.6638,  ..., -1.0358, -0.0691,  0.4931],\n",
       "            [-0.0439, -0.0985,  0.7872,  ..., -1.2696, -0.6336,  0.2326],\n",
       "            [ 0.0566, -0.3592,  0.2030,  ..., -1.2317, -0.1917, -0.0510],\n",
       "            ...,\n",
       "            [ 0.3777, -0.3419,  0.2900,  ..., -1.2692, -0.2374, -0.0530],\n",
       "            [ 0.2408, -0.4264,  0.1533,  ..., -1.0392, -0.3548,  0.0619],\n",
       "            [ 0.5086, -0.0269,  0.2744,  ..., -0.8423, -0.0497, -0.1352]]],\n",
       "          device='cuda:0'),\n",
       "   'pred_sub_boxes': tensor([[[0.4976, 0.5543, 0.5153, 0.4759],\n",
       "            [0.4995, 0.5627, 0.5272, 0.4742],\n",
       "            [0.5003, 0.5597, 0.5217, 0.4828],\n",
       "            [0.4844, 0.5624, 0.5239, 0.4783],\n",
       "            [0.5038, 0.5550, 0.5192, 0.4833],\n",
       "            [0.4870, 0.5630, 0.5148, 0.4792],\n",
       "            [0.4913, 0.5457, 0.5120, 0.4677],\n",
       "            [0.4876, 0.5596, 0.5262, 0.4825],\n",
       "            [0.4862, 0.5641, 0.5159, 0.4756],\n",
       "            [0.5030, 0.5669, 0.5028, 0.4800],\n",
       "            [0.4935, 0.5513, 0.5140, 0.4859],\n",
       "            [0.4871, 0.5577, 0.5165, 0.4565],\n",
       "            [0.4895, 0.5642, 0.5234, 0.4840],\n",
       "            [0.4971, 0.5643, 0.5123, 0.4843],\n",
       "            [0.4935, 0.5511, 0.5030, 0.4672],\n",
       "            [0.5022, 0.5519, 0.5177, 0.4765],\n",
       "            [0.4910, 0.5596, 0.5217, 0.4677],\n",
       "            [0.4994, 0.5540, 0.5261, 0.4847],\n",
       "            [0.4910, 0.5530, 0.5195, 0.4673],\n",
       "            [0.5068, 0.5635, 0.4998, 0.4857],\n",
       "            [0.5037, 0.5528, 0.5125, 0.4714],\n",
       "            [0.4953, 0.5453, 0.5113, 0.4717],\n",
       "            [0.4916, 0.5627, 0.5020, 0.4721],\n",
       "            [0.5057, 0.5508, 0.5145, 0.4792],\n",
       "            [0.4881, 0.5631, 0.5162, 0.4748],\n",
       "            [0.5023, 0.5585, 0.5153, 0.4663],\n",
       "            [0.4964, 0.5530, 0.5385, 0.4665],\n",
       "            [0.5008, 0.5525, 0.5044, 0.4845],\n",
       "            [0.4890, 0.5601, 0.5179, 0.4899],\n",
       "            [0.4929, 0.5513, 0.5020, 0.4768],\n",
       "            [0.4890, 0.5645, 0.5165, 0.4928],\n",
       "            [0.4989, 0.5667, 0.5096, 0.4575],\n",
       "            [0.5021, 0.5592, 0.5160, 0.4726],\n",
       "            [0.4940, 0.5472, 0.5055, 0.4782],\n",
       "            [0.4911, 0.5547, 0.5194, 0.4831],\n",
       "            [0.5008, 0.5592, 0.5198, 0.4830],\n",
       "            [0.4937, 0.5681, 0.5189, 0.4778],\n",
       "            [0.4824, 0.5479, 0.5232, 0.4689],\n",
       "            [0.4997, 0.5519, 0.5129, 0.4990],\n",
       "            [0.4963, 0.5539, 0.5156, 0.4816],\n",
       "            [0.4901, 0.5515, 0.5104, 0.4858],\n",
       "            [0.4945, 0.5603, 0.5144, 0.4706],\n",
       "            [0.5020, 0.5632, 0.5058, 0.4652],\n",
       "            [0.5020, 0.5636, 0.5068, 0.4772],\n",
       "            [0.4870, 0.5638, 0.5129, 0.4671],\n",
       "            [0.4948, 0.5539, 0.5173, 0.4791],\n",
       "            [0.5052, 0.5394, 0.5190, 0.4753],\n",
       "            [0.4983, 0.5592, 0.5122, 0.4790],\n",
       "            [0.5010, 0.5541, 0.5221, 0.4789],\n",
       "            [0.4923, 0.5476, 0.5226, 0.4729],\n",
       "            [0.4853, 0.5623, 0.5279, 0.4673],\n",
       "            [0.5032, 0.5637, 0.5215, 0.4823],\n",
       "            [0.5071, 0.5495, 0.5199, 0.4630],\n",
       "            [0.4934, 0.5639, 0.5045, 0.5191],\n",
       "            [0.4987, 0.5613, 0.5218, 0.4870],\n",
       "            [0.4866, 0.5553, 0.5175, 0.4693],\n",
       "            [0.5031, 0.5579, 0.5187, 0.4804],\n",
       "            [0.4797, 0.5457, 0.5097, 0.4671],\n",
       "            [0.4980, 0.5521, 0.5304, 0.4874],\n",
       "            [0.5033, 0.5599, 0.5226, 0.4785],\n",
       "            [0.4834, 0.5582, 0.5138, 0.4766],\n",
       "            [0.4834, 0.5363, 0.5148, 0.4760],\n",
       "            [0.4756, 0.5616, 0.5030, 0.4700],\n",
       "            [0.4890, 0.5468, 0.5242, 0.4841],\n",
       "            [0.5051, 0.5562, 0.5249, 0.4691],\n",
       "            [0.4842, 0.5601, 0.5069, 0.4765],\n",
       "            [0.4851, 0.5488, 0.5167, 0.4761],\n",
       "            [0.4868, 0.5474, 0.5093, 0.4837],\n",
       "            [0.4900, 0.5513, 0.5362, 0.4840],\n",
       "            [0.4866, 0.5418, 0.5104, 0.4838],\n",
       "            [0.4881, 0.5591, 0.5118, 0.4939],\n",
       "            [0.4885, 0.5665, 0.5098, 0.4751],\n",
       "            [0.4943, 0.5512, 0.5077, 0.4800],\n",
       "            [0.4969, 0.5519, 0.5035, 0.4745],\n",
       "            [0.4984, 0.5531, 0.5106, 0.4770],\n",
       "            [0.4974, 0.5637, 0.5207, 0.4840],\n",
       "            [0.5046, 0.5599, 0.5156, 0.4746],\n",
       "            [0.4926, 0.5549, 0.5179, 0.4750],\n",
       "            [0.4857, 0.5487, 0.5140, 0.4728],\n",
       "            [0.5014, 0.5596, 0.5162, 0.4683],\n",
       "            [0.4954, 0.5610, 0.5180, 0.4818],\n",
       "            [0.5039, 0.5471, 0.5105, 0.4566],\n",
       "            [0.5139, 0.5553, 0.5224, 0.4727],\n",
       "            [0.4996, 0.5650, 0.5172, 0.4896],\n",
       "            [0.4889, 0.5504, 0.5063, 0.4677],\n",
       "            [0.4989, 0.5530, 0.5097, 0.4958],\n",
       "            [0.4985, 0.5671, 0.5175, 0.4800],\n",
       "            [0.4901, 0.5443, 0.5127, 0.4804],\n",
       "            [0.4864, 0.5579, 0.5190, 0.4843],\n",
       "            [0.4907, 0.5581, 0.5157, 0.4753],\n",
       "            [0.4902, 0.5449, 0.5307, 0.4821],\n",
       "            [0.4960, 0.5565, 0.5180, 0.4725],\n",
       "            [0.4861, 0.5575, 0.5157, 0.4775],\n",
       "            [0.4946, 0.5568, 0.5195, 0.4730],\n",
       "            [0.5057, 0.5580, 0.5109, 0.4717],\n",
       "            [0.4828, 0.5536, 0.5148, 0.4684],\n",
       "            [0.4991, 0.5671, 0.5224, 0.4799],\n",
       "            [0.4929, 0.5538, 0.5169, 0.4820],\n",
       "            [0.4878, 0.5627, 0.5264, 0.4846],\n",
       "            [0.4881, 0.5557, 0.5201, 0.4850]]], device='cuda:0'),\n",
       "   'pred_obj_boxes': tensor([[[0.4602, 0.4726, 0.5152, 0.4949],\n",
       "            [0.4692, 0.4615, 0.5351, 0.5045],\n",
       "            [0.4529, 0.4538, 0.5305, 0.4973],\n",
       "            [0.4753, 0.4682, 0.5126, 0.4998],\n",
       "            [0.4598, 0.4745, 0.5325, 0.4980],\n",
       "            [0.4632, 0.4736, 0.5212, 0.4946],\n",
       "            [0.4595, 0.4614, 0.5246, 0.5079],\n",
       "            [0.4589, 0.4755, 0.5134, 0.4971],\n",
       "            [0.4523, 0.4724, 0.5276, 0.4860],\n",
       "            [0.4621, 0.4581, 0.5212, 0.4933],\n",
       "            [0.4571, 0.4691, 0.5242, 0.4938],\n",
       "            [0.4593, 0.4721, 0.5302, 0.5016],\n",
       "            [0.4760, 0.4658, 0.5314, 0.5057],\n",
       "            [0.4599, 0.4647, 0.5166, 0.5017],\n",
       "            [0.4750, 0.4624, 0.5070, 0.5069],\n",
       "            [0.4714, 0.4671, 0.5141, 0.5042],\n",
       "            [0.4627, 0.4669, 0.5216, 0.4906],\n",
       "            [0.4796, 0.4805, 0.5316, 0.4983],\n",
       "            [0.4624, 0.4711, 0.5257, 0.4938],\n",
       "            [0.4716, 0.4725, 0.5321, 0.4964],\n",
       "            [0.4726, 0.4755, 0.5410, 0.4933],\n",
       "            [0.4496, 0.4705, 0.5252, 0.4943],\n",
       "            [0.4833, 0.4661, 0.5219, 0.5096],\n",
       "            [0.4783, 0.4632, 0.5266, 0.4995],\n",
       "            [0.4597, 0.4824, 0.5311, 0.5066],\n",
       "            [0.4572, 0.4554, 0.5140, 0.4965],\n",
       "            [0.4616, 0.4749, 0.5291, 0.4941],\n",
       "            [0.4720, 0.4786, 0.5275, 0.4965],\n",
       "            [0.4597, 0.4642, 0.5205, 0.5070],\n",
       "            [0.4581, 0.4733, 0.5207, 0.4935],\n",
       "            [0.4554, 0.4603, 0.5186, 0.4897],\n",
       "            [0.4630, 0.4628, 0.5243, 0.4972],\n",
       "            [0.4575, 0.4640, 0.5159, 0.5021],\n",
       "            [0.4763, 0.4691, 0.5394, 0.5047],\n",
       "            [0.4538, 0.4653, 0.5271, 0.4990],\n",
       "            [0.4630, 0.4678, 0.5280, 0.4991],\n",
       "            [0.4545, 0.4705, 0.5262, 0.5065],\n",
       "            [0.4603, 0.4562, 0.5251, 0.4943],\n",
       "            [0.4661, 0.4737, 0.5370, 0.4963],\n",
       "            [0.4491, 0.4744, 0.5263, 0.5001],\n",
       "            [0.4719, 0.4653, 0.5106, 0.4931],\n",
       "            [0.4609, 0.4789, 0.5088, 0.4979],\n",
       "            [0.4674, 0.4792, 0.5123, 0.5040],\n",
       "            [0.4747, 0.4730, 0.5136, 0.5008],\n",
       "            [0.4612, 0.4754, 0.5111, 0.5094],\n",
       "            [0.4705, 0.4674, 0.5221, 0.5038],\n",
       "            [0.4615, 0.4776, 0.5308, 0.5101],\n",
       "            [0.4550, 0.4600, 0.5147, 0.4882],\n",
       "            [0.4616, 0.4634, 0.5224, 0.4826],\n",
       "            [0.4672, 0.4634, 0.5181, 0.5076],\n",
       "            [0.4672, 0.4693, 0.5166, 0.5028],\n",
       "            [0.4608, 0.4680, 0.5082, 0.4924],\n",
       "            [0.4644, 0.4589, 0.5238, 0.5038],\n",
       "            [0.4467, 0.4632, 0.5054, 0.5070],\n",
       "            [0.4766, 0.4641, 0.5368, 0.4984],\n",
       "            [0.4654, 0.4615, 0.5185, 0.4908],\n",
       "            [0.4588, 0.4527, 0.5150, 0.4834],\n",
       "            [0.4632, 0.4566, 0.5344, 0.5061],\n",
       "            [0.4654, 0.4685, 0.5183, 0.4962],\n",
       "            [0.4623, 0.4716, 0.5259, 0.5003],\n",
       "            [0.4529, 0.4724, 0.5264, 0.5133],\n",
       "            [0.4575, 0.4830, 0.5131, 0.4930],\n",
       "            [0.4735, 0.4723, 0.5298, 0.5070],\n",
       "            [0.4497, 0.4854, 0.5252, 0.5040],\n",
       "            [0.4771, 0.4635, 0.5300, 0.5082],\n",
       "            [0.4681, 0.4731, 0.5115, 0.4982],\n",
       "            [0.4571, 0.4639, 0.5139, 0.5050],\n",
       "            [0.4669, 0.4678, 0.5065, 0.5088],\n",
       "            [0.4643, 0.4717, 0.5110, 0.4860],\n",
       "            [0.4550, 0.4769, 0.5228, 0.4984],\n",
       "            [0.4685, 0.4758, 0.5315, 0.5071],\n",
       "            [0.4584, 0.4597, 0.5066, 0.4969],\n",
       "            [0.4829, 0.4666, 0.5220, 0.4968],\n",
       "            [0.4632, 0.4630, 0.5257, 0.4978],\n",
       "            [0.4623, 0.4749, 0.5459, 0.4999],\n",
       "            [0.4524, 0.4683, 0.5188, 0.4869],\n",
       "            [0.4768, 0.4653, 0.5166, 0.5078],\n",
       "            [0.4643, 0.4690, 0.5361, 0.4890],\n",
       "            [0.4644, 0.4735, 0.5287, 0.5182],\n",
       "            [0.4787, 0.4683, 0.5090, 0.4899],\n",
       "            [0.4663, 0.4643, 0.5276, 0.4882],\n",
       "            [0.4647, 0.4727, 0.5229, 0.5149],\n",
       "            [0.4596, 0.4691, 0.5263, 0.5128],\n",
       "            [0.4675, 0.4767, 0.5334, 0.5025],\n",
       "            [0.4641, 0.4709, 0.5266, 0.5078],\n",
       "            [0.4685, 0.4849, 0.5102, 0.4914],\n",
       "            [0.4603, 0.4576, 0.5189, 0.5034],\n",
       "            [0.4596, 0.4692, 0.5112, 0.5060],\n",
       "            [0.4673, 0.4845, 0.5224, 0.5066],\n",
       "            [0.4617, 0.4605, 0.5186, 0.5059],\n",
       "            [0.4703, 0.4550, 0.5278, 0.5020],\n",
       "            [0.4701, 0.4643, 0.5204, 0.4983],\n",
       "            [0.4705, 0.4783, 0.5281, 0.5130],\n",
       "            [0.4699, 0.4658, 0.5144, 0.4958],\n",
       "            [0.4518, 0.4647, 0.5189, 0.5058],\n",
       "            [0.4775, 0.4673, 0.5234, 0.5069],\n",
       "            [0.4561, 0.4702, 0.5166, 0.5015],\n",
       "            [0.4598, 0.4787, 0.5280, 0.5073],\n",
       "            [0.4846, 0.4720, 0.5156, 0.4997],\n",
       "            [0.4513, 0.4689, 0.5182, 0.4995]]], device='cuda:0')},\n",
       "  {'pred_obj_logits': tensor([[[ 0.2263, -0.1164,  0.9439,  ..., -1.2918,  0.6344,  0.8358],\n",
       "            [ 0.0838, -0.0398,  0.8194,  ..., -1.0357,  0.8891,  0.8758],\n",
       "            [ 0.2560,  0.1001,  1.1790,  ..., -0.8226,  0.8820,  1.0817],\n",
       "            ...,\n",
       "            [ 0.0364,  0.2323,  1.2409,  ..., -1.2653,  0.6948,  0.9809],\n",
       "            [ 0.4467, -0.1387,  0.8609,  ..., -0.9690,  0.5519,  0.9025],\n",
       "            [ 0.2022,  0.3861,  1.1066,  ..., -0.9011,  0.8307,  0.8169]]],\n",
       "          device='cuda:0'),\n",
       "   'pred_verb_logits': tensor([[[ 0.0663, -0.0840,  0.6324,  ..., -1.0489, -0.3523,  0.3327],\n",
       "            [-0.2148, -0.0668,  0.5156,  ..., -1.3765, -0.8039,  0.1306],\n",
       "            [ 0.0074, -0.1018,  0.1363,  ..., -1.0319, -0.3095, -0.2575],\n",
       "            ...,\n",
       "            [ 0.0434, -0.2021,  0.3288,  ..., -1.0422, -0.4842, -0.0709],\n",
       "            [-0.0987, -0.4024,  0.0166,  ..., -0.9137, -0.3995, -0.1081],\n",
       "            [ 0.3832,  0.0316,  0.2188,  ..., -0.8098, -0.2016, -0.1933]]],\n",
       "          device='cuda:0'),\n",
       "   'pred_sub_boxes': tensor([[[0.4888, 0.5539, 0.5311, 0.4866],\n",
       "            [0.4890, 0.5588, 0.5380, 0.4747],\n",
       "            [0.5011, 0.5533, 0.5329, 0.4789],\n",
       "            [0.4865, 0.5562, 0.5296, 0.4805],\n",
       "            [0.5005, 0.5582, 0.5353, 0.4896],\n",
       "            [0.4853, 0.5582, 0.5296, 0.4747],\n",
       "            [0.4934, 0.5488, 0.5314, 0.4685],\n",
       "            [0.4854, 0.5569, 0.5364, 0.4880],\n",
       "            [0.4893, 0.5637, 0.5301, 0.4801],\n",
       "            [0.4952, 0.5669, 0.5253, 0.4703],\n",
       "            [0.4955, 0.5487, 0.5283, 0.4849],\n",
       "            [0.4919, 0.5546, 0.5275, 0.4687],\n",
       "            [0.4922, 0.5700, 0.5384, 0.4904],\n",
       "            [0.4838, 0.5545, 0.5262, 0.4811],\n",
       "            [0.4843, 0.5539, 0.5206, 0.4683],\n",
       "            [0.4928, 0.5518, 0.5329, 0.4894],\n",
       "            [0.4907, 0.5615, 0.5393, 0.4668],\n",
       "            [0.4875, 0.5559, 0.5348, 0.4869],\n",
       "            [0.4985, 0.5487, 0.5380, 0.4735],\n",
       "            [0.4927, 0.5612, 0.5219, 0.4824],\n",
       "            [0.5035, 0.5550, 0.5236, 0.4768],\n",
       "            [0.4888, 0.5495, 0.5301, 0.4741],\n",
       "            [0.4902, 0.5555, 0.5236, 0.4774],\n",
       "            [0.5034, 0.5472, 0.5291, 0.4748],\n",
       "            [0.4912, 0.5566, 0.5342, 0.4684],\n",
       "            [0.5001, 0.5590, 0.5289, 0.4737],\n",
       "            [0.4927, 0.5492, 0.5448, 0.4711],\n",
       "            [0.5002, 0.5483, 0.5233, 0.4889],\n",
       "            [0.4882, 0.5638, 0.5342, 0.4862],\n",
       "            [0.4933, 0.5486, 0.5271, 0.4697],\n",
       "            [0.4874, 0.5616, 0.5303, 0.4842],\n",
       "            [0.4966, 0.5618, 0.5298, 0.4676],\n",
       "            [0.4999, 0.5607, 0.5278, 0.4668],\n",
       "            [0.4907, 0.5414, 0.5224, 0.4781],\n",
       "            [0.4863, 0.5585, 0.5290, 0.4741],\n",
       "            [0.4911, 0.5671, 0.5249, 0.4802],\n",
       "            [0.4949, 0.5595, 0.5396, 0.4751],\n",
       "            [0.4855, 0.5467, 0.5415, 0.4727],\n",
       "            [0.4931, 0.5532, 0.5307, 0.4895],\n",
       "            [0.4949, 0.5543, 0.5341, 0.4812],\n",
       "            [0.4815, 0.5491, 0.5354, 0.4904],\n",
       "            [0.4884, 0.5594, 0.5248, 0.4847],\n",
       "            [0.4965, 0.5552, 0.5301, 0.4715],\n",
       "            [0.4936, 0.5621, 0.5321, 0.4829],\n",
       "            [0.4904, 0.5562, 0.5325, 0.4712],\n",
       "            [0.5045, 0.5592, 0.5279, 0.4818],\n",
       "            [0.5043, 0.5403, 0.5314, 0.4735],\n",
       "            [0.5016, 0.5531, 0.5332, 0.4758],\n",
       "            [0.4976, 0.5544, 0.5333, 0.4769],\n",
       "            [0.4876, 0.5496, 0.5418, 0.4824],\n",
       "            [0.4774, 0.5600, 0.5425, 0.4669],\n",
       "            [0.4987, 0.5639, 0.5341, 0.4770],\n",
       "            [0.5057, 0.5529, 0.5352, 0.4670],\n",
       "            [0.4902, 0.5689, 0.5238, 0.5138],\n",
       "            [0.4988, 0.5594, 0.5376, 0.4769],\n",
       "            [0.4935, 0.5615, 0.5309, 0.4758],\n",
       "            [0.5093, 0.5658, 0.5284, 0.4822],\n",
       "            [0.4770, 0.5467, 0.5284, 0.4731],\n",
       "            [0.4956, 0.5547, 0.5370, 0.4792],\n",
       "            [0.5069, 0.5643, 0.5387, 0.4814],\n",
       "            [0.4867, 0.5521, 0.5300, 0.4814],\n",
       "            [0.4886, 0.5391, 0.5270, 0.4767],\n",
       "            [0.4878, 0.5657, 0.5197, 0.4720],\n",
       "            [0.4839, 0.5458, 0.5396, 0.4763],\n",
       "            [0.4917, 0.5573, 0.5386, 0.4735],\n",
       "            [0.4808, 0.5657, 0.5279, 0.4773],\n",
       "            [0.4897, 0.5451, 0.5245, 0.4842],\n",
       "            [0.4888, 0.5444, 0.5225, 0.4798],\n",
       "            [0.4920, 0.5453, 0.5542, 0.4892],\n",
       "            [0.4815, 0.5397, 0.5257, 0.4719],\n",
       "            [0.4881, 0.5547, 0.5236, 0.4831],\n",
       "            [0.4824, 0.5593, 0.5260, 0.4826],\n",
       "            [0.5001, 0.5557, 0.5255, 0.4769],\n",
       "            [0.4907, 0.5516, 0.5285, 0.4717],\n",
       "            [0.4921, 0.5646, 0.5324, 0.4809],\n",
       "            [0.4864, 0.5657, 0.5318, 0.4876],\n",
       "            [0.5014, 0.5507, 0.5348, 0.4794],\n",
       "            [0.4959, 0.5628, 0.5373, 0.4740],\n",
       "            [0.4931, 0.5423, 0.5252, 0.4681],\n",
       "            [0.4978, 0.5527, 0.5383, 0.4711],\n",
       "            [0.4866, 0.5644, 0.5303, 0.4848],\n",
       "            [0.4930, 0.5461, 0.5253, 0.4631],\n",
       "            [0.5094, 0.5570, 0.5416, 0.4837],\n",
       "            [0.4938, 0.5623, 0.5291, 0.4863],\n",
       "            [0.4834, 0.5500, 0.5331, 0.4737],\n",
       "            [0.4982, 0.5532, 0.5215, 0.4854],\n",
       "            [0.4938, 0.5576, 0.5283, 0.4925],\n",
       "            [0.4885, 0.5445, 0.5307, 0.4819],\n",
       "            [0.4861, 0.5528, 0.5375, 0.4760],\n",
       "            [0.4955, 0.5566, 0.5206, 0.4707],\n",
       "            [0.5008, 0.5472, 0.5342, 0.4816],\n",
       "            [0.4992, 0.5571, 0.5365, 0.4765],\n",
       "            [0.4866, 0.5510, 0.5336, 0.4780],\n",
       "            [0.4883, 0.5639, 0.5326, 0.4790],\n",
       "            [0.4947, 0.5590, 0.5297, 0.4734],\n",
       "            [0.4787, 0.5606, 0.5262, 0.4679],\n",
       "            [0.4917, 0.5581, 0.5335, 0.4872],\n",
       "            [0.4953, 0.5573, 0.5313, 0.4833],\n",
       "            [0.4865, 0.5651, 0.5378, 0.4849],\n",
       "            [0.4973, 0.5580, 0.5322, 0.4799]]], device='cuda:0'),\n",
       "   'pred_obj_boxes': tensor([[[0.4643, 0.4676, 0.5235, 0.5097],\n",
       "            [0.4813, 0.4630, 0.5478, 0.5108],\n",
       "            [0.4721, 0.4579, 0.5379, 0.5155],\n",
       "            [0.4875, 0.4704, 0.5151, 0.5062],\n",
       "            [0.4707, 0.4757, 0.5254, 0.5032],\n",
       "            [0.4767, 0.4728, 0.5221, 0.4968],\n",
       "            [0.4719, 0.4586, 0.5325, 0.5090],\n",
       "            [0.4776, 0.4817, 0.5239, 0.5003],\n",
       "            [0.4604, 0.4708, 0.5259, 0.4959],\n",
       "            [0.4746, 0.4675, 0.5305, 0.4942],\n",
       "            [0.4717, 0.4685, 0.5292, 0.5121],\n",
       "            [0.4675, 0.4679, 0.5351, 0.5081],\n",
       "            [0.4845, 0.4719, 0.5426, 0.5102],\n",
       "            [0.4718, 0.4673, 0.5202, 0.5054],\n",
       "            [0.4907, 0.4666, 0.5178, 0.5184],\n",
       "            [0.4840, 0.4698, 0.5235, 0.5109],\n",
       "            [0.4729, 0.4674, 0.5260, 0.4937],\n",
       "            [0.4865, 0.4750, 0.5460, 0.5024],\n",
       "            [0.4766, 0.4726, 0.5390, 0.5110],\n",
       "            [0.4742, 0.4736, 0.5359, 0.5061],\n",
       "            [0.4828, 0.4763, 0.5387, 0.5010],\n",
       "            [0.4638, 0.4666, 0.5354, 0.4988],\n",
       "            [0.4869, 0.4745, 0.5307, 0.5149],\n",
       "            [0.4966, 0.4664, 0.5399, 0.5116],\n",
       "            [0.4757, 0.4783, 0.5367, 0.5221],\n",
       "            [0.4641, 0.4490, 0.5272, 0.5044],\n",
       "            [0.4635, 0.4758, 0.5322, 0.5078],\n",
       "            [0.4828, 0.4770, 0.5348, 0.5120],\n",
       "            [0.4778, 0.4696, 0.5320, 0.5062],\n",
       "            [0.4660, 0.4707, 0.5250, 0.4966],\n",
       "            [0.4632, 0.4636, 0.5221, 0.4939],\n",
       "            [0.4783, 0.4623, 0.5227, 0.4987],\n",
       "            [0.4727, 0.4620, 0.5241, 0.5021],\n",
       "            [0.4776, 0.4641, 0.5436, 0.5035],\n",
       "            [0.4657, 0.4638, 0.5400, 0.5064],\n",
       "            [0.4778, 0.4789, 0.5354, 0.5096],\n",
       "            [0.4662, 0.4740, 0.5285, 0.5177],\n",
       "            [0.4748, 0.4641, 0.5283, 0.5055],\n",
       "            [0.4777, 0.4770, 0.5341, 0.4997],\n",
       "            [0.4713, 0.4741, 0.5281, 0.4985],\n",
       "            [0.4830, 0.4693, 0.5221, 0.5083],\n",
       "            [0.4758, 0.4800, 0.5161, 0.5092],\n",
       "            [0.4817, 0.4782, 0.5237, 0.5086],\n",
       "            [0.4775, 0.4773, 0.5126, 0.5092],\n",
       "            [0.4676, 0.4733, 0.5178, 0.5170],\n",
       "            [0.4886, 0.4712, 0.5358, 0.5067],\n",
       "            [0.4707, 0.4660, 0.5314, 0.5143],\n",
       "            [0.4692, 0.4661, 0.5179, 0.5053],\n",
       "            [0.4678, 0.4642, 0.5250, 0.4951],\n",
       "            [0.4815, 0.4685, 0.5283, 0.5205],\n",
       "            [0.4751, 0.4675, 0.5175, 0.5044],\n",
       "            [0.4799, 0.4636, 0.5196, 0.4985],\n",
       "            [0.4788, 0.4642, 0.5283, 0.5101],\n",
       "            [0.4563, 0.4679, 0.5138, 0.5038],\n",
       "            [0.4812, 0.4713, 0.5371, 0.5070],\n",
       "            [0.4759, 0.4615, 0.5270, 0.4977],\n",
       "            [0.4708, 0.4490, 0.5192, 0.4901],\n",
       "            [0.4722, 0.4643, 0.5348, 0.5129],\n",
       "            [0.4699, 0.4607, 0.5271, 0.5115],\n",
       "            [0.4685, 0.4718, 0.5358, 0.4994],\n",
       "            [0.4722, 0.4675, 0.5314, 0.5180],\n",
       "            [0.4654, 0.4840, 0.5181, 0.5099],\n",
       "            [0.4802, 0.4698, 0.5378, 0.5122],\n",
       "            [0.4672, 0.4902, 0.5319, 0.5144],\n",
       "            [0.4820, 0.4701, 0.5280, 0.5084],\n",
       "            [0.4810, 0.4681, 0.5171, 0.5127],\n",
       "            [0.4675, 0.4698, 0.5260, 0.5138],\n",
       "            [0.4763, 0.4753, 0.5191, 0.5088],\n",
       "            [0.4797, 0.4675, 0.5263, 0.4952],\n",
       "            [0.4650, 0.4714, 0.5312, 0.5099],\n",
       "            [0.4737, 0.4697, 0.5280, 0.5036],\n",
       "            [0.4706, 0.4771, 0.5182, 0.5032],\n",
       "            [0.4813, 0.4705, 0.5234, 0.5029],\n",
       "            [0.4763, 0.4649, 0.5336, 0.5104],\n",
       "            [0.4754, 0.4715, 0.5458, 0.5050],\n",
       "            [0.4661, 0.4715, 0.5278, 0.5017],\n",
       "            [0.4983, 0.4653, 0.5227, 0.5076],\n",
       "            [0.4809, 0.4677, 0.5369, 0.5030],\n",
       "            [0.4744, 0.4772, 0.5339, 0.5160],\n",
       "            [0.4854, 0.4794, 0.5109, 0.4956],\n",
       "            [0.4780, 0.4699, 0.5259, 0.5052],\n",
       "            [0.4840, 0.4756, 0.5341, 0.5163],\n",
       "            [0.4630, 0.4612, 0.5375, 0.5283],\n",
       "            [0.4824, 0.4741, 0.5394, 0.5036],\n",
       "            [0.4771, 0.4803, 0.5326, 0.5076],\n",
       "            [0.4742, 0.4747, 0.5253, 0.5046],\n",
       "            [0.4610, 0.4654, 0.5306, 0.5080],\n",
       "            [0.4641, 0.4683, 0.5294, 0.5107],\n",
       "            [0.4800, 0.4805, 0.5289, 0.5128],\n",
       "            [0.4697, 0.4702, 0.5223, 0.5139],\n",
       "            [0.4876, 0.4534, 0.5297, 0.5107],\n",
       "            [0.4789, 0.4676, 0.5224, 0.5001],\n",
       "            [0.4916, 0.4810, 0.5398, 0.5215],\n",
       "            [0.4804, 0.4699, 0.5186, 0.5017],\n",
       "            [0.4588, 0.4693, 0.5188, 0.5152],\n",
       "            [0.4812, 0.4652, 0.5287, 0.5078],\n",
       "            [0.4743, 0.4620, 0.5206, 0.5052],\n",
       "            [0.4734, 0.4721, 0.5337, 0.5215],\n",
       "            [0.4896, 0.4764, 0.5192, 0.5048],\n",
       "            [0.4735, 0.4710, 0.5297, 0.5037]]], device='cuda:0')}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
