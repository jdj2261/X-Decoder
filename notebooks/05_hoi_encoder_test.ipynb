{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  2.0 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "os.environ[\"DATASET\"] = \"../datasets\"\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures import Boxes, ImageList, Instances, BitMasks, BoxMode\n",
    "\n",
    "from xdecoder.BaseModel import BaseModel\n",
    "from xdecoder import build_model\n",
    "\n",
    "from utils.arguments import load_opt_command\n",
    "from utils.misc import hook_metadata, hook_switcher, hook_opt\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "# cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/svlp_focalt_lang.yaml\")]\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/hdecoder/test_vcoco.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', '../checkpoints/xdecoder_focalt_best_openseg.pt'] \n",
    "cmdline_args.overrides\n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "config_dict\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../checkpoints/xdecoder_focalt_best_openseg.pt\n"
     ]
    }
   ],
   "source": [
    "pretrained_pth = os.path.join(opt['WEIGHT'])\n",
    "output_root = './output'\n",
    "image_pth = '../images/animals.png'\n",
    "print(pretrained_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*UNLOADED* sem_seg_head.predictor.pos_embed_caping.weight, Model Shape: torch.Size([77, 512])\n",
      "$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])\n",
      "$UNUSED$ sem_seg_head.predictor.query_feat_caping.weight, Ckpt Shape: torch.Size([77, 512])\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained_pth).eval().cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone (FocalNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = model.model.backbone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['layer_4.weight', 'layer_4.norm.weight', 'layer_4.norm.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hdecoder.body.encoder.transformer_encoder_hoi import TransformerEncoderHOI\n",
    "from hdecoder.body.encoder.registry import register_encoder\n",
    "\n",
    "hoi_encoder = TransformerEncoderHOI(opt, backbone.output_shape()).cuda()\n",
    "state_dict = model.model.sem_seg_head.pixel_decoder.state_dict()\n",
    "hoi_encoder.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdecoder.body.decoder.hdecoder import HDecoder\n",
    "hoi_decoder = HDecoder(opt, return_intermediate_dec=True).cuda()\n",
    "hidden_dim = hoi_decoder.d_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "query_embed = nn.Embedding(100, hidden_dim).cuda()\n",
    "query_embed.weight.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdecoder.body.decoder.modules import MLP\n",
    "num_obj_classes = 81\n",
    "num_verb_classes = 29\n",
    "\n",
    "obj_class_embed = nn.Linear(hidden_dim, num_obj_classes + 1).cuda()\n",
    "verb_class_embed = nn.Linear(hidden_dim, num_verb_classes).cuda()\n",
    "sub_bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3).cuda()\n",
    "obj_bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_layers_hopd = 3\n",
    "dec_layers_interaction = 3\n",
    "@torch.jit.unused\n",
    "def _set_aux_loss(outputs_obj_class, outputs_verb_class, outputs_sub_coord, outputs_obj_coord, outputs_matching=None):\n",
    "    min_dec_layers_num = min(dec_layers_hopd, dec_layers_interaction)\n",
    "    return [{'pred_obj_logits': a, 'pred_verb_logits': b, 'pred_sub_boxes': c, 'pred_obj_boxes': d}\n",
    "            for a, b, c, d in zip(outputs_obj_class[-min_dec_layers_num : -1], outputs_verb_class[-min_dec_layers_num : -1], \\\n",
    "                                    outputs_sub_coord[-min_dec_layers_num : -1], outputs_obj_coord[-min_dec_layers_num : -1])]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import nn\n",
    "\n",
    "from utils.box_ops import box_cxcywh_to_xyxy, generalized_box_iou\n",
    "\n",
    "class HungarianMatcherHOI(nn.Module):\n",
    "    def __init__(self, cost_obj_class: float = 1, cost_verb_class: float = 1, cost_bbox: float = 1,\n",
    "                 cost_giou: float = 1, cost_matching: float = 1, use_matching: bool = False):\n",
    "        super().__init__()\n",
    "        self.cost_obj_class = cost_obj_class\n",
    "        self.cost_verb_class = cost_verb_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "        self.cost_matching = cost_matching\n",
    "        self.use_matching = use_matching\n",
    "        assert cost_obj_class != 0 or cost_verb_class != 0 or cost_bbox != 0 or cost_giou != 0 or cost_matching != 0, 'all costs cant be 0'\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        bs, num_queries = outputs['pred_obj_logits'].shape[:2]\n",
    "        out_obj_prob = outputs['pred_obj_logits'].flatten(0, 1).softmax(-1)\n",
    "        out_verb_prob = outputs['pred_verb_logits'].flatten(0, 1).sigmoid()\n",
    "        out_sub_bbox = outputs['pred_sub_boxes'].flatten(0, 1)\n",
    "        out_obj_bbox = outputs['pred_obj_boxes'].flatten(0, 1)\n",
    "\n",
    "        tgt_obj_labels = torch.cat([v['obj_labels'] for v in targets])\n",
    "        tgt_verb_labels = torch.cat([v['verb_labels'] for v in targets])\n",
    "        tgt_verb_labels_permute = tgt_verb_labels.permute(1, 0)\n",
    "        tgt_sub_boxes = torch.cat([v['sub_boxes'] for v in targets])\n",
    "        tgt_obj_boxes = torch.cat([v['obj_boxes'] for v in targets])\n",
    "\n",
    "        cost_obj_class = -out_obj_prob[:, tgt_obj_labels]\n",
    "\n",
    "        tgt_verb_labels_permute = tgt_verb_labels.permute(1, 0)\n",
    "        cost_verb_class = -(out_verb_prob.matmul(tgt_verb_labels_permute) / \\\n",
    "                            (tgt_verb_labels_permute.sum(dim=0, keepdim=True) + 1e-4) + \\\n",
    "                            (1 - out_verb_prob).matmul(1 - tgt_verb_labels_permute) / \\\n",
    "                            ((1 - tgt_verb_labels_permute).sum(dim=0, keepdim=True) + 1e-4)) / 2\n",
    "\n",
    "        cost_sub_bbox = torch.cdist(out_sub_bbox, tgt_sub_boxes, p=1)\n",
    "        cost_obj_bbox = torch.cdist(out_obj_bbox, tgt_obj_boxes, p=1) * (tgt_obj_boxes != 0).any(dim=1).unsqueeze(0)\n",
    "        if cost_sub_bbox.shape[1] == 0:\n",
    "            cost_bbox = cost_sub_bbox\n",
    "        else:\n",
    "            cost_bbox = torch.stack((cost_sub_bbox, cost_obj_bbox)).max(dim=0)[0]\n",
    "\n",
    "        cost_sub_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_sub_bbox), box_cxcywh_to_xyxy(tgt_sub_boxes))\n",
    "        cost_obj_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_obj_bbox), box_cxcywh_to_xyxy(tgt_obj_boxes)) + \\\n",
    "                        cost_sub_giou * (tgt_obj_boxes == 0).all(dim=1).unsqueeze(0)\n",
    "        if cost_sub_giou.shape[1] == 0:\n",
    "            cost_giou = cost_sub_giou\n",
    "        else:\n",
    "            cost_giou = torch.stack((cost_sub_giou, cost_obj_giou)).max(dim=0)[0]\n",
    "\n",
    "        C = self.cost_obj_class * cost_obj_class + self.cost_verb_class * cost_verb_class + \\\n",
    "            self.cost_bbox * cost_bbox + self.cost_giou * cost_giou\n",
    "\n",
    "        if self.use_matching:\n",
    "            tgt_matching_labels = torch.cat([v['matching_labels'] for v in targets])\n",
    "            out_matching_prob = outputs['pred_matching_logits'].flatten(0, 1).softmax(-1)\n",
    "            cost_matching = -out_matching_prob[:, tgt_matching_labels]\n",
    "            C += self.cost_matching * cost_matching\n",
    "\n",
    "\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v['obj_labels']) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcherHOI(\n",
    "    cost_obj_class=1, \n",
    "    cost_verb_class=1,\n",
    "    cost_bbox=2.5, \n",
    "    cost_giou=1, \n",
    "    cost_matching=1)\n",
    "\n",
    "weight_dict = {}\n",
    "weight_dict['loss_obj_ce'] = 1\n",
    "weight_dict['loss_verb_ce'] = 2\n",
    "weight_dict['loss_sub_bbox'] = 2.5\n",
    "weight_dict['loss_obj_bbox'] = 2.5\n",
    "weight_dict['loss_sub_giou'] = 1\n",
    "weight_dict['loss_obj_giou'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dec_layers_num = min(dec_layers_hopd, dec_layers_interaction)\n",
    "aux_weight_dict = {}\n",
    "for i in range(min_dec_layers_num - 1):\n",
    "    aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
    "weight_dict.update(aux_weight_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "import math\n",
    "from utils.misc import accuracy, is_dist_avail_and_initialized, get_world_size\n",
    "\n",
    "class SetCriterionHOI(nn.Module):\n",
    "\n",
    "    def __init__(self, num_obj_classes, num_queries, num_verb_classes, matcher, weight_dict, eos_coef, losses):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_obj_classes = num_obj_classes\n",
    "        self.num_queries = num_queries\n",
    "        self.num_verb_classes = num_verb_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "        empty_weight = torch.ones(self.num_obj_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "        self.alpha = 0.5\n",
    "        self.obj_nums_init = [5397, 238, 332, 321, 5, 6, 45, 90, 59, 20, \\\n",
    "                                13, 5, 6, 313, 28, 25, 46, 277, 20, 16, \\\n",
    "                                154, 0, 7, 13, 356, 191, 458, 66, 337, 1364, \\\n",
    "                                1382, 958, 1166, 68, 258, 221, 1317, 1428, 759, 201, \\\n",
    "                                190, 444, 274, 587, 124, 107, 102, 37, 226, 16, \\\n",
    "                                30, 22, 187, 320, 222, 465, 893, 213, 56, 322, \\\n",
    "                                306, 13, 55, 834, 23, 104, 38, 861, 11, 27, \\\n",
    "                                0, 16, 22, 405, 50, 14, 145, 63, 9, 11]\n",
    "\n",
    "\n",
    "        self.obj_nums_init.append(3 * sum(self.obj_nums_init))  # 3 times fg for bg init\n",
    "        self.verb_nums_init = [4001, 4598, 1989, 488, 656, 3825, 367, 367, 677, 677, \\\n",
    "                                700, 471, 354, 498, 300, 313, 300, 300, 622, 458, \\\n",
    "                                500, 498, 489, 1545, 133, 142, 38, 116, 388]\n",
    "\n",
    "\n",
    "        self.verb_nums_init.append(3 * sum(self.verb_nums_init))\n",
    "\n",
    "        self.obj_reweight = False\n",
    "        self.verb_reweight = False\n",
    "        self.use_static_weights = False\n",
    "        \n",
    "        Maxsize = 4704*1.0\n",
    "\n",
    "        if self.obj_reweight:\n",
    "            self.q_obj = Queue(maxsize=Maxsize)\n",
    "            self.p_obj = 0.7\n",
    "            self.obj_weights_init = self.cal_weights(self.obj_nums_init, p=self.p_obj)\n",
    "\n",
    "        if self.verb_reweight:\n",
    "            self.q_verb = Queue(maxsize=Maxsize)\n",
    "            self.p_verb = 0.7\n",
    "            self.verb_weights_init = self.cal_weights(self.verb_nums_init, p=self.p_verb)\n",
    "\n",
    "    def cal_weights(self, label_nums, p=0.5):\n",
    "        num_fgs = len(label_nums[:-1])\n",
    "        weight = [0] * (num_fgs + 1)\n",
    "        num_all = sum(label_nums[:-1])\n",
    "\n",
    "        for index in range(num_fgs):\n",
    "            if label_nums[index] == 0: continue\n",
    "            weight[index] = np.power(num_all/label_nums[index], p)\n",
    "\n",
    "        weight = np.array(weight)\n",
    "        weight = weight / np.mean(weight[weight>0])\n",
    "\n",
    "        weight[-1] = np.power(num_all/label_nums[-1], p) if label_nums[-1] != 0 else 0\n",
    "\n",
    "        weight = torch.FloatTensor(weight).cuda()\n",
    "        return weight\n",
    "\n",
    "    def loss_obj_labels(self, outputs, targets, indices, num_interactions, log=True):\n",
    "        assert 'pred_obj_logits' in outputs\n",
    "        src_logits = outputs['pred_obj_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t['obj_labels'][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_obj_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        if not self.obj_reweight:\n",
    "            obj_weights = self.empty_weight\n",
    "        elif self.use_static_weights:\n",
    "            obj_weights = self.obj_weights_init\n",
    "        else:\n",
    "            obj_label_nums_in_batch = [0] * (self.num_obj_classes + 1)\n",
    "            for target_class in target_classes:\n",
    "                for label in target_class:\n",
    "                    obj_label_nums_in_batch[label] += 1\n",
    "\n",
    "            if self.q_obj.full(): self.q_obj.get()\n",
    "            self.q_obj.put(np.array(obj_label_nums_in_batch))\n",
    "            accumulated_obj_label_nums = np.sum(self.q_obj.queue, axis=0)\n",
    "            obj_weights = self.cal_weights(accumulated_obj_label_nums, p=self.p_obj)\n",
    "\n",
    "            aphal = min(math.pow(0.999, self.q_obj.qsize()),0.9)\n",
    "            obj_weights = aphal * self.obj_weights_init + (1 - aphal) * obj_weights\n",
    "\n",
    "        loss_obj_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, obj_weights)\n",
    "        losses = {'loss_obj_ce': loss_obj_ce}\n",
    "\n",
    "        if log:\n",
    "            losses['obj_class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def loss_obj_cardinality(self, outputs, targets, indices, num_interactions):\n",
    "        pred_logits = outputs['pred_obj_logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor([len(v['obj_labels']) for v in targets], device=device)\n",
    "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'obj_cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_verb_labels(self, outputs, targets, indices, num_interactions):\n",
    "        assert 'pred_verb_logits' in outputs\n",
    "        src_logits = outputs['pred_verb_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t['verb_labels'][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.zeros_like(src_logits)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        if not self.verb_reweight:\n",
    "            verb_weights = None\n",
    "        elif self.use_static_weights:\n",
    "            verb_weights = self.verb_weights_init\n",
    "        else:\n",
    "            verb_label_nums_in_batch = [0] * (self.num_verb_classes + 1)\n",
    "            for target_class in target_classes:\n",
    "                for label in target_class:\n",
    "                    label_classes = torch.where(label > 0)[0]\n",
    "                    if len(label_classes) == 0:\n",
    "                        verb_label_nums_in_batch[-1] += 1\n",
    "                    else:\n",
    "                        for label_class in label_classes:\n",
    "                            verb_label_nums_in_batch[label_class] += 1\n",
    "\n",
    "            if self.q_verb.full(): self.q_verb.get()\n",
    "            self.q_verb.put(np.array(verb_label_nums_in_batch))\n",
    "            accumulated_verb_label_nums = np.sum(self.q_verb.queue, axis=0)\n",
    "            verb_weights = self.cal_weights(accumulated_verb_label_nums, p=self.p_verb)\n",
    "\n",
    "            aphal = min(math.pow(0.999, self.q_verb.qsize()),0.9)\n",
    "            verb_weights = aphal * self.verb_weights_init + (1 - aphal) * verb_weights\n",
    "\n",
    "        src_logits = src_logits.sigmoid()\n",
    "        loss_verb_ce = self._neg_loss(src_logits, target_classes, weights=verb_weights, alpha=self.alpha)\n",
    "\n",
    "        losses = {'loss_verb_ce': loss_verb_ce}\n",
    "        return losses\n",
    "\n",
    "    def loss_sub_obj_boxes(self, outputs, targets, indices, num_interactions):\n",
    "        assert 'pred_sub_boxes' in outputs and 'pred_obj_boxes' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_sub_boxes = outputs['pred_sub_boxes'][idx]\n",
    "        src_obj_boxes = outputs['pred_obj_boxes'][idx]\n",
    "        target_sub_boxes = torch.cat([t['sub_boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "        target_obj_boxes = torch.cat([t['obj_boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        exist_obj_boxes = (target_obj_boxes != 0).any(dim=1)\n",
    "\n",
    "        losses = {}\n",
    "        if src_sub_boxes.shape[0] == 0:\n",
    "            losses['loss_sub_bbox'] = src_sub_boxes.sum()\n",
    "            losses['loss_obj_bbox'] = src_obj_boxes.sum()\n",
    "            losses['loss_sub_giou'] = src_sub_boxes.sum()\n",
    "            losses['loss_obj_giou'] = src_obj_boxes.sum()\n",
    "        else:\n",
    "            loss_sub_bbox = F.l1_loss(src_sub_boxes, target_sub_boxes, reduction='none')\n",
    "            loss_obj_bbox = F.l1_loss(src_obj_boxes, target_obj_boxes, reduction='none')\n",
    "            losses['loss_sub_bbox'] = loss_sub_bbox.sum() / num_interactions\n",
    "            losses['loss_obj_bbox'] = (loss_obj_bbox * exist_obj_boxes.unsqueeze(1)).sum() / (exist_obj_boxes.sum() + 1e-4)\n",
    "            loss_sub_giou = 1 - torch.diag(generalized_box_iou(box_cxcywh_to_xyxy(src_sub_boxes),\n",
    "                                                               box_cxcywh_to_xyxy(target_sub_boxes)))\n",
    "            loss_obj_giou = 1 - torch.diag(generalized_box_iou(box_cxcywh_to_xyxy(src_obj_boxes),\n",
    "                                                               box_cxcywh_to_xyxy(target_obj_boxes)))\n",
    "            losses['loss_sub_giou'] = loss_sub_giou.sum() / num_interactions\n",
    "            losses['loss_obj_giou'] = (loss_obj_giou * exist_obj_boxes).sum() / (exist_obj_boxes.sum() + 1e-4)\n",
    "        return losses\n",
    "\n",
    "    def loss_matching_labels(self, outputs, targets, indices, num_interactions, log=True):\n",
    "        assert 'pred_matching_logits' in outputs\n",
    "        src_logits = outputs['pred_matching_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t['matching_labels'][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], 0,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        loss_matching = F.cross_entropy(src_logits.transpose(1, 2), target_classes)\n",
    "        losses = {'loss_matching': loss_matching}\n",
    "\n",
    "        if log:\n",
    "            losses['matching_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses\n",
    "\n",
    "    def _neg_loss(self, pred, gt, weights=None, alpha=0.25):\n",
    "        pos_inds = gt.eq(1).float()\n",
    "        neg_inds = gt.lt(1).float()\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        pos_loss = alpha * torch.log(pred) * torch.pow(1 - pred, 2) * pos_inds\n",
    "        if weights is not None:\n",
    "            pos_loss = pos_loss * weights[:-1]\n",
    "\n",
    "        neg_loss = (1 - alpha) * torch.log(1 - pred) * torch.pow(pred, 2) * neg_inds\n",
    "\n",
    "        num_pos  = pos_inds.float().sum()\n",
    "        pos_loss = pos_loss.sum()\n",
    "        neg_loss = neg_loss.sum()\n",
    "\n",
    "        if num_pos == 0:\n",
    "            loss = loss - neg_loss\n",
    "        else:\n",
    "            loss = loss - (pos_loss + neg_loss) / num_pos\n",
    "        return loss\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num, **kwargs):\n",
    "        loss_map = {\n",
    "            'obj_labels': self.loss_obj_labels,\n",
    "            'obj_cardinality': self.loss_obj_cardinality,\n",
    "            'verb_labels': self.loss_verb_labels,\n",
    "            'sub_obj_boxes': self.loss_sub_obj_boxes,\n",
    "            'matching_labels': self.loss_matching_labels\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        num_interactions = sum(len(t['obj_labels']) for t in targets)\n",
    "        num_interactions = torch.as_tensor([num_interactions], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "        if is_dist_avail_and_initialized():\n",
    "            torch.distributed.all_reduce(num_interactions)\n",
    "        num_interactions = torch.clamp(num_interactions / get_world_size(), min=1).item()\n",
    "\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_interactions))\n",
    "\n",
    "        if 'aux_outputs' in outputs:\n",
    "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
    "                indices = self.matcher(aux_outputs, targets)\n",
    "                for loss in self.losses:\n",
    "                    kwargs = {}\n",
    "                    if loss == 'obj_labels':\n",
    "                        kwargs = {'log': False}\n",
    "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_interactions, **kwargs)\n",
    "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
    "                    losses.update(l_dict)\n",
    "\n",
    "        return losses\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostProcessHOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessHOI(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.subject_category_id = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, target_sizes):\n",
    "        out_obj_logits = outputs['pred_obj_logits']\n",
    "        out_verb_logits = outputs['pred_verb_logits']\n",
    "        out_sub_boxes = outputs['pred_sub_boxes']\n",
    "        out_obj_boxes = outputs['pred_obj_boxes']\n",
    "\n",
    "        assert len(out_obj_logits) == len(target_sizes)\n",
    "        assert target_sizes.shape[1] == 2\n",
    "\n",
    "        obj_prob = F.softmax(out_obj_logits, -1)\n",
    "        obj_scores, obj_labels = obj_prob[..., :-1].max(-1)\n",
    "\n",
    "        verb_scores = out_verb_logits.sigmoid()\n",
    "\n",
    "        img_h, img_w = target_sizes.unbind(1)\n",
    "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(verb_scores.device)\n",
    "        sub_boxes = box_cxcywh_to_xyxy(out_sub_boxes)\n",
    "        sub_boxes = sub_boxes * scale_fct[:, None, :]\n",
    "        obj_boxes = box_cxcywh_to_xyxy(out_obj_boxes)\n",
    "        obj_boxes = obj_boxes * scale_fct[:, None, :]\n",
    "\n",
    "        results = []\n",
    "        for index in range(len(obj_scores)):\n",
    "            os, ol, vs, sb, ob =  obj_scores[index], obj_labels[index], verb_scores[index], sub_boxes[index], obj_boxes[index]\n",
    "            sl = torch.full_like(ol, self.subject_category_id)\n",
    "            l = torch.cat((sl, ol))\n",
    "            b = torch.cat((sb, ob))\n",
    "            results.append({'labels': l.to('cpu'), 'boxes': b.to('cpu')})\n",
    "\n",
    "            vs = vs * os.unsqueeze(1)\n",
    "            ids = torch.arange(b.shape[0])\n",
    "\n",
    "            results[-1].update({'verb_scores': vs.to('cpu'), 'sub_ids': ids[:ids.shape[0] // 2],\n",
    "                                'obj_ids': ids[ids.shape[0] // 2:]})\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['obj_labels', 'verb_labels', 'sub_obj_boxes', 'obj_cardinality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obj_classes = 80\n",
    "num_queries = 100\n",
    "num_verb_classes = 27\n",
    "eos_coef = 0.1\n",
    "\n",
    "criterion = SetCriterionHOI(num_obj_classes, num_queries, num_verb_classes, matcher=matcher,\n",
    "                            weight_dict=weight_dict, eos_coef=eos_coef, losses=losses)\n",
    "criterion.to('cuda')\n",
    "postprocessors = {'hoi': PostProcessHOI()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, criterion, postprocessors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 512])\n",
      "torch.Size([1, 768, 25, 42])\n",
      "torch.Size([1, 512, 25, 42])\n",
      "torch.Size([1050, 1, 512])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "t = []\n",
    "t.append(transforms.Resize(800, interpolation=Image.BICUBIC))\n",
    "transform = transforms.Compose(t)\n",
    "pixel_mean = torch.Tensor([123.675, 116.280, 103.530]).view(-1, 1, 1).cuda()\n",
    "pixel_std = torch.Tensor([58.395, 57.120, 57.375]).view(-1, 1, 1).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_ori = Image.open(image_pth).convert('RGB')\n",
    "    width = image_ori.size[0]\n",
    "    height = image_ori.size[1]\n",
    "    image = transform(image_ori)\n",
    "    image = np.asarray(image)\n",
    "    image_ori = np.asarray(image_ori)\n",
    "    images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "    batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "    \n",
    "    images = [x[\"image\"].to(\"cuda\") for x in batch_inputs]\n",
    "    images = [(x - pixel_mean) / pixel_std for x in images]\n",
    "    images = ImageList.from_tensors(images, 32)\n",
    "\n",
    "    bs, c, h, w = images.tensor.shape\n",
    "    # pos_embed = position_embedding.flatten(2).permute(2, 0, 1)\n",
    "    _query_embed = query_embed.weight.unsqueeze(1).repeat(1, bs, 1)\n",
    "    print(_query_embed.shape)\n",
    "    features = backbone(images.tensor)\n",
    "    print(features[\"res5\"].shape)\n",
    "    encoder_features, pos = hoi_encoder(features)\n",
    "    print(pos.shape)\n",
    "    pos_embed = pos.flatten(2).permute(2, 0, 1)\n",
    "    print(pos_embed.shape)\n",
    "    hopd_out, interaction_decoder_out, memory = hoi_decoder(encoder_features, None, _query_embed, pos_embed)\n",
    "    \n",
    "    outputs_sub_coord = sub_bbox_embed(hopd_out).sigmoid()\n",
    "    outputs_obj_coord = obj_bbox_embed(hopd_out).sigmoid()\n",
    "    outputs_obj_class = obj_class_embed(hopd_out)\n",
    "    outputs_verb_class = verb_class_embed(interaction_decoder_out)\n",
    "\n",
    "    out = {\n",
    "        'pred_obj_logits': outputs_obj_class[-1], \n",
    "        'pred_verb_logits': outputs_verb_class[-1],\n",
    "        'pred_sub_boxes': outputs_sub_coord[-1], \n",
    "        'pred_obj_boxes': outputs_obj_coord[-1]}        \n",
    "                                     \n",
    "    out['aux_outputs'] = _set_aux_loss(\n",
    "        outputs_obj_class, \n",
    "        outputs_verb_class,\n",
    "        outputs_sub_coord,\n",
    "        outputs_obj_coord)\n",
    "    \n",
    "    # print(out)\n",
    "    # loss_dict = criterion(out, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_obj_logits': tensor([[[ 0.4088, -0.7261, -1.0347,  ..., -1.0373,  0.6305,  0.3333],\n",
       "          [ 0.5335, -0.6930, -1.2654,  ..., -1.1899,  0.4498, -0.2976],\n",
       "          [ 0.3143, -0.8311, -1.1712,  ..., -0.8149,  0.3436,  0.0016],\n",
       "          ...,\n",
       "          [ 0.4042, -0.8826, -1.0913,  ..., -0.9087,  0.4033,  0.0572],\n",
       "          [ 0.6852, -0.8224, -1.2558,  ..., -1.0208,  0.5305, -0.1910],\n",
       "          [ 0.5301, -0.9090, -1.0765,  ..., -0.8131,  0.6192, -0.2542]]],\n",
       "        device='cuda:0'),\n",
       " 'pred_verb_logits': tensor([[[-0.0867, -0.0387, -0.2133,  ...,  0.4311,  0.4036, -0.3234],\n",
       "          [-0.2399, -0.1327, -0.3399,  ...,  0.3578,  0.0607, -0.0678],\n",
       "          [ 0.0331, -0.0092, -0.1543,  ...,  0.6400,  0.2914, -0.2073],\n",
       "          ...,\n",
       "          [ 0.2477, -0.1616, -0.3159,  ...,  0.3882,  0.3455, -0.0210],\n",
       "          [ 0.0389, -0.1524, -0.0391,  ...,  0.5083,  0.3709, -0.0832],\n",
       "          [-0.0449,  0.1330, -0.4385,  ...,  0.6013,  0.3870, -0.3943]]],\n",
       "        device='cuda:0'),\n",
       " 'pred_sub_boxes': tensor([[[0.5682, 0.5007, 0.4976, 0.4900],\n",
       "          [0.5758, 0.4977, 0.5037, 0.4851],\n",
       "          [0.5751, 0.4982, 0.4934, 0.4894],\n",
       "          [0.5708, 0.4924, 0.5013, 0.4805],\n",
       "          [0.5765, 0.4937, 0.5020, 0.4933],\n",
       "          [0.5741, 0.4998, 0.4940, 0.4836],\n",
       "          [0.5782, 0.4856, 0.4980, 0.4945],\n",
       "          [0.5704, 0.4787, 0.4990, 0.4907],\n",
       "          [0.5758, 0.4853, 0.5051, 0.5007],\n",
       "          [0.5730, 0.4842, 0.4931, 0.4963],\n",
       "          [0.5839, 0.4859, 0.5063, 0.4959],\n",
       "          [0.5778, 0.4877, 0.5055, 0.4942],\n",
       "          [0.5795, 0.4796, 0.4994, 0.5032],\n",
       "          [0.5693, 0.4841, 0.4993, 0.4916],\n",
       "          [0.5773, 0.4932, 0.4922, 0.4939],\n",
       "          [0.5815, 0.5007, 0.5018, 0.4946],\n",
       "          [0.5783, 0.4946, 0.4962, 0.4947],\n",
       "          [0.5693, 0.4858, 0.5033, 0.4865],\n",
       "          [0.5722, 0.4916, 0.4949, 0.5004],\n",
       "          [0.5720, 0.4855, 0.4937, 0.4968],\n",
       "          [0.5679, 0.4814, 0.4991, 0.4970],\n",
       "          [0.5694, 0.4903, 0.4959, 0.4824],\n",
       "          [0.5719, 0.4843, 0.4960, 0.4956],\n",
       "          [0.5791, 0.4868, 0.4965, 0.4904],\n",
       "          [0.5669, 0.4854, 0.4964, 0.4843],\n",
       "          [0.5789, 0.4939, 0.4885, 0.4874],\n",
       "          [0.5736, 0.4952, 0.5073, 0.5011],\n",
       "          [0.5834, 0.4915, 0.4965, 0.4944],\n",
       "          [0.5723, 0.4860, 0.5051, 0.4964],\n",
       "          [0.5738, 0.4840, 0.5035, 0.4920],\n",
       "          [0.5744, 0.4868, 0.4948, 0.4845],\n",
       "          [0.5698, 0.4837, 0.5037, 0.4872],\n",
       "          [0.5820, 0.4830, 0.4993, 0.4925],\n",
       "          [0.5760, 0.4807, 0.5030, 0.4933],\n",
       "          [0.5785, 0.4847, 0.5005, 0.4913],\n",
       "          [0.5744, 0.4955, 0.5035, 0.4944],\n",
       "          [0.5791, 0.4925, 0.5029, 0.4935],\n",
       "          [0.5649, 0.4823, 0.5063, 0.4956],\n",
       "          [0.5776, 0.4854, 0.4968, 0.4851],\n",
       "          [0.5660, 0.4894, 0.5047, 0.4881],\n",
       "          [0.5824, 0.4840, 0.4943, 0.4954],\n",
       "          [0.5781, 0.4934, 0.5074, 0.4851],\n",
       "          [0.5773, 0.4933, 0.5040, 0.5036],\n",
       "          [0.5838, 0.4941, 0.5154, 0.4814],\n",
       "          [0.5764, 0.4959, 0.5078, 0.4976],\n",
       "          [0.5824, 0.4823, 0.4953, 0.4956],\n",
       "          [0.5736, 0.4893, 0.4947, 0.4961],\n",
       "          [0.5759, 0.4862, 0.4919, 0.4924],\n",
       "          [0.5746, 0.4939, 0.5014, 0.4972],\n",
       "          [0.5763, 0.4916, 0.4944, 0.4934],\n",
       "          [0.5762, 0.4765, 0.5018, 0.4969],\n",
       "          [0.5723, 0.4746, 0.4982, 0.5039],\n",
       "          [0.5720, 0.4863, 0.5121, 0.4964],\n",
       "          [0.5646, 0.4859, 0.5044, 0.5003],\n",
       "          [0.5797, 0.4866, 0.5003, 0.4946],\n",
       "          [0.5773, 0.4895, 0.4919, 0.4831],\n",
       "          [0.5704, 0.4943, 0.5070, 0.4907],\n",
       "          [0.5726, 0.4880, 0.4975, 0.4993],\n",
       "          [0.5754, 0.4882, 0.5004, 0.4845],\n",
       "          [0.5849, 0.4909, 0.5071, 0.4889],\n",
       "          [0.5720, 0.4827, 0.4910, 0.4978],\n",
       "          [0.5680, 0.4818, 0.5005, 0.4936],\n",
       "          [0.5747, 0.4918, 0.5019, 0.4876],\n",
       "          [0.5723, 0.4940, 0.5073, 0.4991],\n",
       "          [0.5705, 0.4800, 0.4845, 0.4940],\n",
       "          [0.5908, 0.4838, 0.5019, 0.4909],\n",
       "          [0.5777, 0.4858, 0.4965, 0.4956],\n",
       "          [0.5844, 0.4789, 0.4912, 0.4817],\n",
       "          [0.5733, 0.4832, 0.4953, 0.4914],\n",
       "          [0.5742, 0.4844, 0.4930, 0.4934],\n",
       "          [0.5813, 0.4818, 0.5010, 0.4914],\n",
       "          [0.5727, 0.4796, 0.4999, 0.4979],\n",
       "          [0.5779, 0.4869, 0.4990, 0.4885],\n",
       "          [0.5770, 0.4903, 0.4895, 0.5000],\n",
       "          [0.5621, 0.4882, 0.5002, 0.4981],\n",
       "          [0.5730, 0.4829, 0.4959, 0.4991],\n",
       "          [0.5773, 0.4904, 0.4952, 0.4865],\n",
       "          [0.5901, 0.4808, 0.4989, 0.4917],\n",
       "          [0.5696, 0.4913, 0.4967, 0.4891],\n",
       "          [0.5774, 0.4837, 0.5010, 0.5011],\n",
       "          [0.5785, 0.4827, 0.5011, 0.4802],\n",
       "          [0.5844, 0.4924, 0.5066, 0.4815],\n",
       "          [0.5822, 0.4882, 0.5030, 0.4992],\n",
       "          [0.5857, 0.4813, 0.4955, 0.4940],\n",
       "          [0.5692, 0.4781, 0.4863, 0.4974],\n",
       "          [0.5716, 0.4728, 0.5002, 0.4990],\n",
       "          [0.5688, 0.4897, 0.5003, 0.4943],\n",
       "          [0.5732, 0.4820, 0.4900, 0.4877],\n",
       "          [0.5829, 0.4837, 0.4829, 0.5044],\n",
       "          [0.5863, 0.4767, 0.5034, 0.4957],\n",
       "          [0.5767, 0.4939, 0.4957, 0.4958],\n",
       "          [0.5724, 0.4894, 0.4990, 0.4847],\n",
       "          [0.5728, 0.4804, 0.5005, 0.4951],\n",
       "          [0.5799, 0.4980, 0.4977, 0.4984],\n",
       "          [0.5767, 0.4932, 0.4958, 0.4920],\n",
       "          [0.5741, 0.4847, 0.4978, 0.5000],\n",
       "          [0.5806, 0.4927, 0.5001, 0.4918],\n",
       "          [0.5730, 0.4935, 0.4947, 0.4863],\n",
       "          [0.5837, 0.4817, 0.4984, 0.4871],\n",
       "          [0.5808, 0.4955, 0.5055, 0.4905]]], device='cuda:0'),\n",
       " 'pred_obj_boxes': tensor([[[0.5272, 0.5146, 0.4998, 0.5378],\n",
       "          [0.5272, 0.5161, 0.4896, 0.5478],\n",
       "          [0.5285, 0.5127, 0.4830, 0.5280],\n",
       "          [0.5297, 0.5121, 0.4817, 0.5431],\n",
       "          [0.5295, 0.5206, 0.4832, 0.5343],\n",
       "          [0.5179, 0.4987, 0.4989, 0.5315],\n",
       "          [0.5250, 0.5015, 0.4940, 0.5311],\n",
       "          [0.5293, 0.5174, 0.4947, 0.5339],\n",
       "          [0.5283, 0.5186, 0.4891, 0.5289],\n",
       "          [0.5341, 0.5152, 0.4887, 0.5372],\n",
       "          [0.5422, 0.5108, 0.4873, 0.5423],\n",
       "          [0.5252, 0.5143, 0.4983, 0.5267],\n",
       "          [0.5383, 0.5078, 0.4910, 0.5351],\n",
       "          [0.5229, 0.5089, 0.4837, 0.5334],\n",
       "          [0.5341, 0.5164, 0.4974, 0.5423],\n",
       "          [0.5384, 0.5187, 0.4935, 0.5411],\n",
       "          [0.5284, 0.5212, 0.4908, 0.5467],\n",
       "          [0.5365, 0.5078, 0.4816, 0.5386],\n",
       "          [0.5333, 0.5149, 0.4865, 0.5422],\n",
       "          [0.5236, 0.5121, 0.4932, 0.5385],\n",
       "          [0.5462, 0.5119, 0.4831, 0.5380],\n",
       "          [0.5187, 0.5002, 0.4982, 0.5298],\n",
       "          [0.5221, 0.5228, 0.4841, 0.5459],\n",
       "          [0.5209, 0.5137, 0.4956, 0.5428],\n",
       "          [0.5343, 0.5124, 0.4894, 0.5248],\n",
       "          [0.5394, 0.5192, 0.4876, 0.5371],\n",
       "          [0.5311, 0.5159, 0.4843, 0.5450],\n",
       "          [0.5237, 0.5209, 0.4963, 0.5382],\n",
       "          [0.5284, 0.5121, 0.4925, 0.5381],\n",
       "          [0.5345, 0.5191, 0.4821, 0.5271],\n",
       "          [0.5203, 0.5063, 0.4933, 0.5427],\n",
       "          [0.5348, 0.5120, 0.4857, 0.5412],\n",
       "          [0.5255, 0.5072, 0.4884, 0.5459],\n",
       "          [0.5243, 0.5093, 0.4794, 0.5352],\n",
       "          [0.5376, 0.4989, 0.4887, 0.5246],\n",
       "          [0.5463, 0.5172, 0.4967, 0.5431],\n",
       "          [0.5218, 0.5197, 0.4939, 0.5411],\n",
       "          [0.5332, 0.5250, 0.4929, 0.5245],\n",
       "          [0.5249, 0.5211, 0.4959, 0.5416],\n",
       "          [0.5288, 0.5087, 0.4989, 0.5399],\n",
       "          [0.5217, 0.5152, 0.4954, 0.5294],\n",
       "          [0.5245, 0.5111, 0.4988, 0.5311],\n",
       "          [0.5144, 0.5203, 0.4852, 0.5421],\n",
       "          [0.5353, 0.5109, 0.4841, 0.5269],\n",
       "          [0.5270, 0.5222, 0.4937, 0.5350],\n",
       "          [0.5154, 0.5143, 0.4828, 0.5363],\n",
       "          [0.5280, 0.5092, 0.4954, 0.5446],\n",
       "          [0.5329, 0.5121, 0.4869, 0.5244],\n",
       "          [0.5275, 0.5098, 0.4792, 0.5410],\n",
       "          [0.5301, 0.5154, 0.4942, 0.5355],\n",
       "          [0.5265, 0.5155, 0.4974, 0.5358],\n",
       "          [0.5318, 0.5118, 0.4806, 0.5371],\n",
       "          [0.5254, 0.5138, 0.4853, 0.5441],\n",
       "          [0.5269, 0.5188, 0.4929, 0.5333],\n",
       "          [0.5205, 0.5127, 0.4971, 0.5416],\n",
       "          [0.5235, 0.5164, 0.4844, 0.5435],\n",
       "          [0.5224, 0.5171, 0.4958, 0.5433],\n",
       "          [0.5293, 0.5197, 0.4869, 0.5454],\n",
       "          [0.5177, 0.5256, 0.4967, 0.5450],\n",
       "          [0.5252, 0.5166, 0.4871, 0.5371],\n",
       "          [0.5319, 0.5064, 0.4928, 0.5231],\n",
       "          [0.5239, 0.5175, 0.4908, 0.5344],\n",
       "          [0.5271, 0.5195, 0.4963, 0.5377],\n",
       "          [0.5287, 0.5193, 0.4887, 0.5305],\n",
       "          [0.5339, 0.5200, 0.4897, 0.5392],\n",
       "          [0.5280, 0.5200, 0.4909, 0.5362],\n",
       "          [0.5185, 0.5040, 0.4849, 0.5403],\n",
       "          [0.5358, 0.4992, 0.4823, 0.5381],\n",
       "          [0.5251, 0.5177, 0.4999, 0.5409],\n",
       "          [0.5213, 0.5148, 0.4829, 0.5348],\n",
       "          [0.5269, 0.5137, 0.4804, 0.5331],\n",
       "          [0.5354, 0.5089, 0.4903, 0.5255],\n",
       "          [0.5355, 0.5154, 0.4900, 0.5397],\n",
       "          [0.5370, 0.5067, 0.4902, 0.5288],\n",
       "          [0.5305, 0.5200, 0.4957, 0.5376],\n",
       "          [0.5380, 0.5136, 0.4919, 0.5502],\n",
       "          [0.5323, 0.5158, 0.4908, 0.5375],\n",
       "          [0.5304, 0.5112, 0.4787, 0.5375],\n",
       "          [0.5294, 0.5126, 0.4897, 0.5355],\n",
       "          [0.5286, 0.5081, 0.4867, 0.5385],\n",
       "          [0.5377, 0.5206, 0.4908, 0.5356],\n",
       "          [0.5159, 0.5260, 0.4864, 0.5469],\n",
       "          [0.5334, 0.5099, 0.4883, 0.5304],\n",
       "          [0.5227, 0.5186, 0.5015, 0.5312],\n",
       "          [0.5382, 0.5181, 0.4933, 0.5347],\n",
       "          [0.5281, 0.5248, 0.4814, 0.5356],\n",
       "          [0.5127, 0.5133, 0.4965, 0.5311],\n",
       "          [0.5395, 0.5116, 0.4937, 0.5435],\n",
       "          [0.5182, 0.5253, 0.4849, 0.5308],\n",
       "          [0.5251, 0.5114, 0.4894, 0.5325],\n",
       "          [0.5350, 0.5243, 0.4998, 0.5354],\n",
       "          [0.5237, 0.5185, 0.4977, 0.5297],\n",
       "          [0.5330, 0.5128, 0.4918, 0.5436],\n",
       "          [0.5341, 0.5196, 0.4869, 0.5388],\n",
       "          [0.5147, 0.5101, 0.4907, 0.5442],\n",
       "          [0.5288, 0.5096, 0.4941, 0.5293],\n",
       "          [0.5394, 0.5241, 0.4908, 0.5462],\n",
       "          [0.5291, 0.5032, 0.5033, 0.5441],\n",
       "          [0.5206, 0.5096, 0.4896, 0.5371],\n",
       "          [0.5235, 0.5055, 0.4981, 0.5410]]], device='cuda:0'),\n",
       " 'aux_outputs': [{'pred_obj_logits': tensor([[[-0.0210, -0.4074, -0.9477,  ..., -1.1379,  0.3001,  0.6125],\n",
       "            [-0.0599, -0.5296, -0.9449,  ..., -1.5230,  0.4165,  0.3760],\n",
       "            [-0.3418, -0.6448, -0.9303,  ..., -1.0857,  0.4159,  0.4858],\n",
       "            ...,\n",
       "            [-0.1171, -0.5352, -1.0376,  ..., -0.9337,  0.5671,  0.4685],\n",
       "            [ 0.2746, -0.7879, -1.0633,  ..., -1.3253,  0.3665,  0.2297],\n",
       "            [-0.3414, -0.3417, -0.9903,  ..., -0.8318,  0.7961,  0.1364]]],\n",
       "          device='cuda:0'),\n",
       "   'pred_verb_logits': tensor([[[ 0.1148, -0.3591, -0.5911,  ...,  0.3700,  0.2614, -0.4315],\n",
       "            [-0.0625, -0.4670, -0.7569,  ...,  0.5619, -0.1642, -0.1745],\n",
       "            [ 0.2152, -0.2791, -0.7323,  ...,  0.4429,  0.0541, -0.1073],\n",
       "            ...,\n",
       "            [ 0.5507, -0.4376, -0.7864,  ...,  0.3613,  0.1427, -0.0032],\n",
       "            [ 0.3201, -0.4818, -0.4222,  ...,  0.7548,  0.2963, -0.3308],\n",
       "            [ 0.3523, -0.0047, -0.8478,  ...,  0.9259,  0.1450, -0.6428]]],\n",
       "          device='cuda:0'),\n",
       "   'pred_sub_boxes': tensor([[[0.5607, 0.4975, 0.4856, 0.4974],\n",
       "            [0.5739, 0.4974, 0.5084, 0.4877],\n",
       "            [0.5661, 0.4986, 0.4928, 0.4918],\n",
       "            [0.5695, 0.5119, 0.5016, 0.4865],\n",
       "            [0.5633, 0.5062, 0.4928, 0.4974],\n",
       "            [0.5669, 0.5013, 0.4922, 0.4952],\n",
       "            [0.5608, 0.4955, 0.4974, 0.5012],\n",
       "            [0.5503, 0.4899, 0.5004, 0.4930],\n",
       "            [0.5644, 0.4886, 0.5047, 0.5034],\n",
       "            [0.5742, 0.4952, 0.4948, 0.4985],\n",
       "            [0.5669, 0.5005, 0.4962, 0.4986],\n",
       "            [0.5615, 0.4854, 0.4950, 0.5030],\n",
       "            [0.5611, 0.4935, 0.4912, 0.5068],\n",
       "            [0.5577, 0.4935, 0.4987, 0.5019],\n",
       "            [0.5745, 0.4972, 0.4894, 0.4989],\n",
       "            [0.5765, 0.5126, 0.5144, 0.4977],\n",
       "            [0.5635, 0.5036, 0.4906, 0.4963],\n",
       "            [0.5684, 0.5015, 0.4919, 0.5004],\n",
       "            [0.5738, 0.4954, 0.4916, 0.5018],\n",
       "            [0.5646, 0.4862, 0.5027, 0.4994],\n",
       "            [0.5689, 0.4965, 0.4917, 0.4995],\n",
       "            [0.5563, 0.5078, 0.4901, 0.5038],\n",
       "            [0.5601, 0.4958, 0.4966, 0.4957],\n",
       "            [0.5696, 0.5028, 0.4928, 0.5051],\n",
       "            [0.5607, 0.4921, 0.4897, 0.4892],\n",
       "            [0.5716, 0.5034, 0.4832, 0.5046],\n",
       "            [0.5688, 0.5035, 0.4991, 0.5080],\n",
       "            [0.5687, 0.4941, 0.5033, 0.4966],\n",
       "            [0.5588, 0.4979, 0.4974, 0.4999],\n",
       "            [0.5631, 0.5007, 0.4928, 0.4975],\n",
       "            [0.5665, 0.4869, 0.4899, 0.5019],\n",
       "            [0.5626, 0.4848, 0.5000, 0.5014],\n",
       "            [0.5705, 0.4998, 0.5083, 0.4957],\n",
       "            [0.5555, 0.4826, 0.4963, 0.4939],\n",
       "            [0.5693, 0.5029, 0.4902, 0.5005],\n",
       "            [0.5645, 0.4953, 0.4928, 0.4917],\n",
       "            [0.5779, 0.5004, 0.4959, 0.4996],\n",
       "            [0.5543, 0.4977, 0.5011, 0.5017],\n",
       "            [0.5730, 0.4908, 0.4927, 0.4953],\n",
       "            [0.5698, 0.4983, 0.4921, 0.5019],\n",
       "            [0.5504, 0.4965, 0.4940, 0.5024],\n",
       "            [0.5639, 0.5102, 0.5055, 0.4963],\n",
       "            [0.5663, 0.4972, 0.5023, 0.5093],\n",
       "            [0.5737, 0.5027, 0.4984, 0.5008],\n",
       "            [0.5616, 0.5066, 0.5000, 0.5045],\n",
       "            [0.5591, 0.4949, 0.4963, 0.5025],\n",
       "            [0.5625, 0.4949, 0.5003, 0.5105],\n",
       "            [0.5619, 0.4989, 0.4924, 0.5041],\n",
       "            [0.5734, 0.5164, 0.4896, 0.5004],\n",
       "            [0.5639, 0.4889, 0.4936, 0.5036],\n",
       "            [0.5649, 0.4920, 0.4906, 0.4994],\n",
       "            [0.5638, 0.4856, 0.5015, 0.5016],\n",
       "            [0.5610, 0.4863, 0.5029, 0.4900],\n",
       "            [0.5663, 0.4947, 0.5083, 0.5069],\n",
       "            [0.5719, 0.4985, 0.4978, 0.4944],\n",
       "            [0.5692, 0.5000, 0.4844, 0.4942],\n",
       "            [0.5610, 0.5079, 0.4866, 0.4937],\n",
       "            [0.5612, 0.4941, 0.4976, 0.5032],\n",
       "            [0.5784, 0.4862, 0.4974, 0.4871],\n",
       "            [0.5740, 0.5043, 0.4954, 0.4976],\n",
       "            [0.5682, 0.5032, 0.4900, 0.5005],\n",
       "            [0.5531, 0.4945, 0.5027, 0.5033],\n",
       "            [0.5635, 0.4987, 0.4967, 0.4981],\n",
       "            [0.5653, 0.4938, 0.5019, 0.5048],\n",
       "            [0.5677, 0.4915, 0.4827, 0.5006],\n",
       "            [0.5797, 0.4906, 0.4987, 0.5005],\n",
       "            [0.5726, 0.4919, 0.4932, 0.5000],\n",
       "            [0.5721, 0.4926, 0.4865, 0.4924],\n",
       "            [0.5667, 0.4932, 0.4910, 0.5050],\n",
       "            [0.5649, 0.4986, 0.4867, 0.5010],\n",
       "            [0.5786, 0.4961, 0.4967, 0.4945],\n",
       "            [0.5628, 0.4831, 0.4897, 0.5014],\n",
       "            [0.5646, 0.4904, 0.4915, 0.4953],\n",
       "            [0.5699, 0.4968, 0.4852, 0.5015],\n",
       "            [0.5492, 0.4986, 0.4907, 0.4943],\n",
       "            [0.5572, 0.4817, 0.5002, 0.5031],\n",
       "            [0.5767, 0.5140, 0.4970, 0.4939],\n",
       "            [0.5745, 0.4945, 0.4936, 0.4989],\n",
       "            [0.5646, 0.5055, 0.4966, 0.4977],\n",
       "            [0.5640, 0.4929, 0.5003, 0.5041],\n",
       "            [0.5707, 0.4891, 0.4948, 0.4930],\n",
       "            [0.5737, 0.5019, 0.4963, 0.4836],\n",
       "            [0.5738, 0.5063, 0.4969, 0.5063],\n",
       "            [0.5630, 0.4950, 0.4938, 0.5053],\n",
       "            [0.5567, 0.4904, 0.4876, 0.5027],\n",
       "            [0.5611, 0.4794, 0.4991, 0.4933],\n",
       "            [0.5609, 0.4953, 0.4873, 0.4965],\n",
       "            [0.5574, 0.4895, 0.4961, 0.4943],\n",
       "            [0.5632, 0.4949, 0.4852, 0.5059],\n",
       "            [0.5740, 0.4894, 0.5067, 0.5187],\n",
       "            [0.5633, 0.5005, 0.4919, 0.5016],\n",
       "            [0.5607, 0.4971, 0.4994, 0.5000],\n",
       "            [0.5617, 0.4839, 0.5062, 0.4982],\n",
       "            [0.5625, 0.4996, 0.4991, 0.5057],\n",
       "            [0.5649, 0.5056, 0.4944, 0.5002],\n",
       "            [0.5547, 0.4982, 0.4938, 0.5047],\n",
       "            [0.5722, 0.5037, 0.5005, 0.4980],\n",
       "            [0.5563, 0.5048, 0.4946, 0.4916],\n",
       "            [0.5703, 0.4966, 0.4854, 0.5017],\n",
       "            [0.5782, 0.5045, 0.4954, 0.5014]]], device='cuda:0'),\n",
       "   'pred_obj_boxes': tensor([[[0.5430, 0.5228, 0.4977, 0.5454],\n",
       "            [0.5489, 0.5026, 0.4824, 0.5468],\n",
       "            [0.5528, 0.5115, 0.4788, 0.5353],\n",
       "            [0.5469, 0.5138, 0.4796, 0.5472],\n",
       "            [0.5539, 0.5221, 0.4742, 0.5540],\n",
       "            [0.5460, 0.5074, 0.5055, 0.5375],\n",
       "            [0.5456, 0.5075, 0.4811, 0.5464],\n",
       "            [0.5556, 0.5190, 0.4954, 0.5494],\n",
       "            [0.5499, 0.5180, 0.4819, 0.5336],\n",
       "            [0.5471, 0.5064, 0.4755, 0.5492],\n",
       "            [0.5514, 0.5057, 0.4812, 0.5447],\n",
       "            [0.5434, 0.5140, 0.4851, 0.5279],\n",
       "            [0.5643, 0.5119, 0.4892, 0.5415],\n",
       "            [0.5533, 0.5143, 0.4840, 0.5440],\n",
       "            [0.5433, 0.5175, 0.4934, 0.5470],\n",
       "            [0.5492, 0.5114, 0.4858, 0.5529],\n",
       "            [0.5455, 0.5186, 0.4846, 0.5515],\n",
       "            [0.5513, 0.5099, 0.4697, 0.5398],\n",
       "            [0.5586, 0.5181, 0.4928, 0.5389],\n",
       "            [0.5481, 0.5312, 0.4937, 0.5530],\n",
       "            [0.5670, 0.5099, 0.4722, 0.5450],\n",
       "            [0.5467, 0.5132, 0.4868, 0.5488],\n",
       "            [0.5348, 0.5249, 0.4833, 0.5469],\n",
       "            [0.5414, 0.5262, 0.4876, 0.5470],\n",
       "            [0.5586, 0.5144, 0.4913, 0.5314],\n",
       "            [0.5543, 0.5269, 0.4865, 0.5486],\n",
       "            [0.5535, 0.5240, 0.4782, 0.5488],\n",
       "            [0.5505, 0.5159, 0.4920, 0.5436],\n",
       "            [0.5512, 0.5215, 0.4877, 0.5474],\n",
       "            [0.5486, 0.5144, 0.4775, 0.5351],\n",
       "            [0.5498, 0.5196, 0.4837, 0.5456],\n",
       "            [0.5603, 0.5032, 0.4947, 0.5482],\n",
       "            [0.5455, 0.5054, 0.4855, 0.5396],\n",
       "            [0.5529, 0.5062, 0.4713, 0.5441],\n",
       "            [0.5448, 0.5084, 0.4843, 0.5378],\n",
       "            [0.5558, 0.5350, 0.4960, 0.5507],\n",
       "            [0.5316, 0.5205, 0.4908, 0.5521],\n",
       "            [0.5455, 0.5323, 0.4954, 0.5304],\n",
       "            [0.5576, 0.5272, 0.4908, 0.5495],\n",
       "            [0.5487, 0.5081, 0.4887, 0.5419],\n",
       "            [0.5391, 0.5163, 0.4902, 0.5371],\n",
       "            [0.5350, 0.5122, 0.4975, 0.5400],\n",
       "            [0.5467, 0.5297, 0.4774, 0.5427],\n",
       "            [0.5463, 0.5020, 0.4801, 0.5286],\n",
       "            [0.5465, 0.5182, 0.4889, 0.5384],\n",
       "            [0.5460, 0.5135, 0.4979, 0.5411],\n",
       "            [0.5399, 0.5168, 0.4903, 0.5343],\n",
       "            [0.5505, 0.5180, 0.4876, 0.5300],\n",
       "            [0.5491, 0.5202, 0.4705, 0.5401],\n",
       "            [0.5410, 0.5258, 0.4899, 0.5347],\n",
       "            [0.5500, 0.5210, 0.4899, 0.5431],\n",
       "            [0.5552, 0.5152, 0.4740, 0.5461],\n",
       "            [0.5346, 0.5266, 0.4768, 0.5576],\n",
       "            [0.5499, 0.5202, 0.4840, 0.5513],\n",
       "            [0.5464, 0.5127, 0.4898, 0.5382],\n",
       "            [0.5427, 0.5240, 0.4793, 0.5397],\n",
       "            [0.5523, 0.5174, 0.4777, 0.5398],\n",
       "            [0.5542, 0.5113, 0.4869, 0.5394],\n",
       "            [0.5468, 0.5128, 0.4949, 0.5499],\n",
       "            [0.5477, 0.5081, 0.4831, 0.5311],\n",
       "            [0.5537, 0.5116, 0.4812, 0.5253],\n",
       "            [0.5514, 0.5271, 0.4852, 0.5407],\n",
       "            [0.5620, 0.5153, 0.4817, 0.5467],\n",
       "            [0.5310, 0.5190, 0.4873, 0.5431],\n",
       "            [0.5505, 0.5230, 0.4892, 0.5518],\n",
       "            [0.5523, 0.5146, 0.4842, 0.5462],\n",
       "            [0.5323, 0.5031, 0.4767, 0.5351],\n",
       "            [0.5636, 0.5134, 0.4773, 0.5363],\n",
       "            [0.5535, 0.5245, 0.4997, 0.5438],\n",
       "            [0.5337, 0.5225, 0.4735, 0.5434],\n",
       "            [0.5607, 0.5142, 0.4742, 0.5345],\n",
       "            [0.5581, 0.5033, 0.4887, 0.5403],\n",
       "            [0.5587, 0.5113, 0.4820, 0.5395],\n",
       "            [0.5541, 0.5166, 0.4836, 0.5314],\n",
       "            [0.5465, 0.5122, 0.4892, 0.5414],\n",
       "            [0.5528, 0.5156, 0.4907, 0.5540],\n",
       "            [0.5579, 0.5105, 0.4806, 0.5412],\n",
       "            [0.5551, 0.5117, 0.4891, 0.5354],\n",
       "            [0.5524, 0.5010, 0.4825, 0.5441],\n",
       "            [0.5435, 0.5152, 0.4890, 0.5413],\n",
       "            [0.5568, 0.5141, 0.4835, 0.5471],\n",
       "            [0.5449, 0.5083, 0.4767, 0.5499],\n",
       "            [0.5543, 0.5133, 0.4747, 0.5403],\n",
       "            [0.5409, 0.5186, 0.4984, 0.5352],\n",
       "            [0.5429, 0.5043, 0.4777, 0.5418],\n",
       "            [0.5495, 0.5232, 0.4799, 0.5420],\n",
       "            [0.5435, 0.5181, 0.4936, 0.5366],\n",
       "            [0.5647, 0.5239, 0.4996, 0.5479],\n",
       "            [0.5381, 0.5225, 0.4783, 0.5318],\n",
       "            [0.5476, 0.5143, 0.4859, 0.5363],\n",
       "            [0.5589, 0.5281, 0.4872, 0.5466],\n",
       "            [0.5487, 0.5064, 0.4938, 0.5425],\n",
       "            [0.5565, 0.5113, 0.4870, 0.5444],\n",
       "            [0.5642, 0.5219, 0.4862, 0.5412],\n",
       "            [0.5392, 0.5089, 0.4809, 0.5465],\n",
       "            [0.5550, 0.5133, 0.4919, 0.5427],\n",
       "            [0.5617, 0.5054, 0.4773, 0.5636],\n",
       "            [0.5632, 0.5105, 0.4950, 0.5363],\n",
       "            [0.5367, 0.5012, 0.4941, 0.5443],\n",
       "            [0.5510, 0.5026, 0.4902, 0.5378]]], device='cuda:0')},\n",
       "  {'pred_obj_logits': tensor([[[ 0.2233, -0.6826, -0.9804,  ..., -1.1117,  0.4420,  0.3681],\n",
       "            [ 0.4527, -0.7853, -1.1360,  ..., -1.2881,  0.4368,  0.0611],\n",
       "            [ 0.1227, -0.7017, -1.1020,  ..., -0.9703,  0.4297,  0.2694],\n",
       "            ...,\n",
       "            [ 0.2737, -0.7329, -1.1113,  ..., -0.9662,  0.6387,  0.2585],\n",
       "            [ 0.5595, -0.9503, -1.2348,  ..., -1.2422,  0.5528,  0.0727],\n",
       "            [ 0.2001, -0.6027, -1.0874,  ..., -0.8923,  0.6809, -0.1035]]],\n",
       "          device='cuda:0'),\n",
       "   'pred_verb_logits': tensor([[[ 0.0101, -0.3069, -0.2460,  ...,  0.4764,  0.3748, -0.3527],\n",
       "            [-0.2978, -0.3104, -0.5128,  ...,  0.4782, -0.1317, -0.0380],\n",
       "            [ 0.0727, -0.1780, -0.4037,  ...,  0.5862,  0.2060, -0.2943],\n",
       "            ...,\n",
       "            [ 0.4161, -0.3267, -0.5819,  ...,  0.4259,  0.1764, -0.0901],\n",
       "            [ 0.0734, -0.4471, -0.2389,  ...,  0.5825,  0.3893, -0.1042],\n",
       "            [ 0.1865,  0.0280, -0.5501,  ...,  0.8562,  0.2218, -0.5599]]],\n",
       "          device='cuda:0'),\n",
       "   'pred_sub_boxes': tensor([[[0.5670, 0.4998, 0.4972, 0.4945],\n",
       "            [0.5760, 0.5029, 0.5072, 0.4825],\n",
       "            [0.5714, 0.5011, 0.4960, 0.4945],\n",
       "            [0.5733, 0.5002, 0.5033, 0.4824],\n",
       "            [0.5762, 0.5033, 0.4906, 0.4976],\n",
       "            [0.5705, 0.5067, 0.4989, 0.4895],\n",
       "            [0.5717, 0.4949, 0.4981, 0.5047],\n",
       "            [0.5608, 0.4916, 0.4985, 0.4935],\n",
       "            [0.5740, 0.4847, 0.5074, 0.5056],\n",
       "            [0.5744, 0.4914, 0.4940, 0.4983],\n",
       "            [0.5799, 0.4882, 0.5031, 0.4985],\n",
       "            [0.5722, 0.4867, 0.4993, 0.4953],\n",
       "            [0.5771, 0.4893, 0.4956, 0.5055],\n",
       "            [0.5672, 0.4885, 0.4965, 0.4977],\n",
       "            [0.5785, 0.4986, 0.4922, 0.4945],\n",
       "            [0.5854, 0.5025, 0.5130, 0.4956],\n",
       "            [0.5730, 0.4948, 0.4966, 0.4979],\n",
       "            [0.5715, 0.4941, 0.4918, 0.4897],\n",
       "            [0.5739, 0.4963, 0.4888, 0.5026],\n",
       "            [0.5669, 0.4919, 0.5018, 0.4997],\n",
       "            [0.5714, 0.4884, 0.4986, 0.4945],\n",
       "            [0.5702, 0.4958, 0.4926, 0.4881],\n",
       "            [0.5703, 0.4954, 0.4976, 0.4939],\n",
       "            [0.5708, 0.4970, 0.4920, 0.5022],\n",
       "            [0.5658, 0.4875, 0.4957, 0.4877],\n",
       "            [0.5804, 0.5068, 0.4809, 0.4917],\n",
       "            [0.5749, 0.5002, 0.5047, 0.5042],\n",
       "            [0.5808, 0.4962, 0.4988, 0.5019],\n",
       "            [0.5710, 0.4927, 0.5013, 0.5069],\n",
       "            [0.5675, 0.4923, 0.4935, 0.4980],\n",
       "            [0.5775, 0.4877, 0.4961, 0.4900],\n",
       "            [0.5674, 0.4897, 0.5016, 0.4897],\n",
       "            [0.5743, 0.4998, 0.5068, 0.4946],\n",
       "            [0.5694, 0.4787, 0.4976, 0.4895],\n",
       "            [0.5783, 0.4991, 0.4919, 0.4943],\n",
       "            [0.5728, 0.4940, 0.5048, 0.4978],\n",
       "            [0.5813, 0.4952, 0.4973, 0.4973],\n",
       "            [0.5595, 0.4893, 0.5057, 0.5084],\n",
       "            [0.5771, 0.4922, 0.4927, 0.4903],\n",
       "            [0.5730, 0.4980, 0.4965, 0.4956],\n",
       "            [0.5696, 0.4950, 0.4891, 0.5018],\n",
       "            [0.5716, 0.4975, 0.5073, 0.4900],\n",
       "            [0.5810, 0.4979, 0.5020, 0.5099],\n",
       "            [0.5852, 0.4983, 0.5059, 0.4905],\n",
       "            [0.5715, 0.5070, 0.4993, 0.5045],\n",
       "            [0.5712, 0.4907, 0.4940, 0.5029],\n",
       "            [0.5714, 0.4899, 0.4939, 0.5082],\n",
       "            [0.5694, 0.4999, 0.4949, 0.4991],\n",
       "            [0.5766, 0.5052, 0.4987, 0.4960],\n",
       "            [0.5739, 0.4890, 0.4966, 0.5001],\n",
       "            [0.5766, 0.4821, 0.4993, 0.4928],\n",
       "            [0.5749, 0.4760, 0.5021, 0.5076],\n",
       "            [0.5662, 0.4843, 0.5051, 0.4926],\n",
       "            [0.5669, 0.4914, 0.5047, 0.5066],\n",
       "            [0.5748, 0.4906, 0.4960, 0.4955],\n",
       "            [0.5731, 0.4965, 0.4863, 0.4882],\n",
       "            [0.5647, 0.4990, 0.4952, 0.4942],\n",
       "            [0.5662, 0.4910, 0.4986, 0.5026],\n",
       "            [0.5813, 0.4890, 0.5021, 0.4775],\n",
       "            [0.5837, 0.4965, 0.5030, 0.4944],\n",
       "            [0.5742, 0.4903, 0.4948, 0.5023],\n",
       "            [0.5598, 0.4950, 0.4999, 0.5022],\n",
       "            [0.5698, 0.5009, 0.5042, 0.4931],\n",
       "            [0.5707, 0.4939, 0.5096, 0.5023],\n",
       "            [0.5783, 0.4875, 0.4838, 0.5001],\n",
       "            [0.5881, 0.4952, 0.4972, 0.4938],\n",
       "            [0.5837, 0.4919, 0.4985, 0.4976],\n",
       "            [0.5842, 0.4865, 0.4895, 0.4884],\n",
       "            [0.5700, 0.4928, 0.4972, 0.4946],\n",
       "            [0.5695, 0.4912, 0.4886, 0.5013],\n",
       "            [0.5807, 0.4908, 0.4987, 0.4944],\n",
       "            [0.5707, 0.4858, 0.4938, 0.5064],\n",
       "            [0.5691, 0.4944, 0.4976, 0.4871],\n",
       "            [0.5737, 0.4991, 0.4866, 0.4982],\n",
       "            [0.5603, 0.4909, 0.5039, 0.4977],\n",
       "            [0.5613, 0.4879, 0.5012, 0.5008],\n",
       "            [0.5798, 0.5085, 0.4926, 0.4900],\n",
       "            [0.5859, 0.4860, 0.4957, 0.4961],\n",
       "            [0.5647, 0.5039, 0.4914, 0.4956],\n",
       "            [0.5714, 0.4898, 0.5003, 0.5015],\n",
       "            [0.5799, 0.4876, 0.4979, 0.4841],\n",
       "            [0.5817, 0.5017, 0.5020, 0.4836],\n",
       "            [0.5785, 0.4963, 0.5027, 0.5075],\n",
       "            [0.5785, 0.4869, 0.4916, 0.5064],\n",
       "            [0.5677, 0.4848, 0.4878, 0.5036],\n",
       "            [0.5704, 0.4805, 0.5039, 0.4961],\n",
       "            [0.5673, 0.4982, 0.4929, 0.4947],\n",
       "            [0.5684, 0.4871, 0.4892, 0.4916],\n",
       "            [0.5716, 0.4953, 0.4862, 0.5097],\n",
       "            [0.5852, 0.4823, 0.5049, 0.5076],\n",
       "            [0.5747, 0.5011, 0.4883, 0.5030],\n",
       "            [0.5743, 0.4958, 0.4964, 0.4930],\n",
       "            [0.5706, 0.4892, 0.4994, 0.4947],\n",
       "            [0.5746, 0.5015, 0.4980, 0.5098],\n",
       "            [0.5746, 0.5038, 0.4950, 0.4969],\n",
       "            [0.5669, 0.4891, 0.4934, 0.5007],\n",
       "            [0.5814, 0.5023, 0.4989, 0.4901],\n",
       "            [0.5643, 0.5042, 0.4989, 0.4954],\n",
       "            [0.5735, 0.4847, 0.4891, 0.4940],\n",
       "            [0.5832, 0.4949, 0.4981, 0.5001]]], device='cuda:0'),\n",
       "   'pred_obj_boxes': tensor([[[0.5356, 0.5228, 0.4895, 0.5340],\n",
       "            [0.5353, 0.5116, 0.4853, 0.5458],\n",
       "            [0.5424, 0.5156, 0.4796, 0.5275],\n",
       "            [0.5339, 0.5073, 0.4783, 0.5402],\n",
       "            [0.5380, 0.5260, 0.4834, 0.5466],\n",
       "            [0.5288, 0.5043, 0.4950, 0.5315],\n",
       "            [0.5360, 0.5039, 0.4876, 0.5393],\n",
       "            [0.5390, 0.5205, 0.4972, 0.5396],\n",
       "            [0.5381, 0.5140, 0.4858, 0.5373],\n",
       "            [0.5364, 0.5040, 0.4880, 0.5394],\n",
       "            [0.5460, 0.5099, 0.4817, 0.5440],\n",
       "            [0.5326, 0.5131, 0.4921, 0.5266],\n",
       "            [0.5486, 0.5079, 0.4894, 0.5391],\n",
       "            [0.5366, 0.5149, 0.4797, 0.5385],\n",
       "            [0.5365, 0.5210, 0.4998, 0.5419],\n",
       "            [0.5410, 0.5172, 0.4824, 0.5444],\n",
       "            [0.5346, 0.5196, 0.4838, 0.5487],\n",
       "            [0.5452, 0.5062, 0.4765, 0.5394],\n",
       "            [0.5446, 0.5161, 0.4819, 0.5386],\n",
       "            [0.5342, 0.5214, 0.4879, 0.5438],\n",
       "            [0.5459, 0.5158, 0.4751, 0.5431],\n",
       "            [0.5263, 0.5012, 0.4893, 0.5395],\n",
       "            [0.5284, 0.5260, 0.4837, 0.5450],\n",
       "            [0.5312, 0.5195, 0.4993, 0.5498],\n",
       "            [0.5514, 0.5137, 0.4881, 0.5293],\n",
       "            [0.5457, 0.5254, 0.4903, 0.5434],\n",
       "            [0.5386, 0.5147, 0.4815, 0.5469],\n",
       "            [0.5316, 0.5228, 0.4936, 0.5379],\n",
       "            [0.5345, 0.5140, 0.4909, 0.5444],\n",
       "            [0.5418, 0.5183, 0.4765, 0.5329],\n",
       "            [0.5370, 0.5062, 0.4868, 0.5430],\n",
       "            [0.5448, 0.5023, 0.4873, 0.5427],\n",
       "            [0.5385, 0.5117, 0.4858, 0.5461],\n",
       "            [0.5335, 0.5065, 0.4718, 0.5402],\n",
       "            [0.5400, 0.5001, 0.4822, 0.5286],\n",
       "            [0.5501, 0.5192, 0.4927, 0.5488],\n",
       "            [0.5257, 0.5209, 0.4858, 0.5474],\n",
       "            [0.5414, 0.5255, 0.4956, 0.5284],\n",
       "            [0.5379, 0.5311, 0.4942, 0.5443],\n",
       "            [0.5347, 0.5088, 0.4907, 0.5443],\n",
       "            [0.5305, 0.5147, 0.4980, 0.5338],\n",
       "            [0.5252, 0.5101, 0.4961, 0.5344],\n",
       "            [0.5307, 0.5243, 0.4761, 0.5387],\n",
       "            [0.5341, 0.5029, 0.4796, 0.5256],\n",
       "            [0.5317, 0.5138, 0.4894, 0.5421],\n",
       "            [0.5253, 0.5158, 0.4847, 0.5392],\n",
       "            [0.5358, 0.5107, 0.4841, 0.5431],\n",
       "            [0.5365, 0.5148, 0.4889, 0.5204],\n",
       "            [0.5394, 0.5121, 0.4735, 0.5395],\n",
       "            [0.5323, 0.5244, 0.4972, 0.5387],\n",
       "            [0.5309, 0.5189, 0.4900, 0.5417],\n",
       "            [0.5416, 0.5121, 0.4731, 0.5447],\n",
       "            [0.5251, 0.5177, 0.4800, 0.5466],\n",
       "            [0.5389, 0.5184, 0.4906, 0.5402],\n",
       "            [0.5324, 0.5172, 0.4903, 0.5484],\n",
       "            [0.5338, 0.5168, 0.4790, 0.5443],\n",
       "            [0.5424, 0.5213, 0.4858, 0.5377],\n",
       "            [0.5405, 0.5174, 0.4887, 0.5460],\n",
       "            [0.5291, 0.5211, 0.4926, 0.5466],\n",
       "            [0.5340, 0.5134, 0.4824, 0.5400],\n",
       "            [0.5396, 0.5115, 0.4862, 0.5267],\n",
       "            [0.5364, 0.5254, 0.4862, 0.5372],\n",
       "            [0.5404, 0.5217, 0.4926, 0.5412],\n",
       "            [0.5302, 0.5170, 0.4870, 0.5406],\n",
       "            [0.5369, 0.5210, 0.4878, 0.5433],\n",
       "            [0.5386, 0.5203, 0.4899, 0.5445],\n",
       "            [0.5238, 0.5035, 0.4822, 0.5389],\n",
       "            [0.5457, 0.5058, 0.4772, 0.5440],\n",
       "            [0.5319, 0.5213, 0.4966, 0.5440],\n",
       "            [0.5281, 0.5158, 0.4757, 0.5395],\n",
       "            [0.5446, 0.5175, 0.4794, 0.5308],\n",
       "            [0.5471, 0.5055, 0.4879, 0.5311],\n",
       "            [0.5515, 0.5138, 0.4808, 0.5446],\n",
       "            [0.5456, 0.5084, 0.4874, 0.5286],\n",
       "            [0.5379, 0.5155, 0.4911, 0.5342],\n",
       "            [0.5347, 0.5100, 0.4901, 0.5553],\n",
       "            [0.5415, 0.5168, 0.4906, 0.5420],\n",
       "            [0.5425, 0.5133, 0.4800, 0.5330],\n",
       "            [0.5376, 0.5080, 0.4854, 0.5424],\n",
       "            [0.5350, 0.5109, 0.4879, 0.5414],\n",
       "            [0.5424, 0.5142, 0.4902, 0.5424],\n",
       "            [0.5268, 0.5204, 0.4786, 0.5536],\n",
       "            [0.5449, 0.5115, 0.4751, 0.5364],\n",
       "            [0.5294, 0.5249, 0.5005, 0.5374],\n",
       "            [0.5410, 0.5136, 0.4873, 0.5439],\n",
       "            [0.5381, 0.5212, 0.4799, 0.5356],\n",
       "            [0.5295, 0.5189, 0.4972, 0.5311],\n",
       "            [0.5470, 0.5192, 0.4945, 0.5510],\n",
       "            [0.5251, 0.5252, 0.4824, 0.5353],\n",
       "            [0.5391, 0.5164, 0.4835, 0.5397],\n",
       "            [0.5473, 0.5247, 0.4960, 0.5375],\n",
       "            [0.5360, 0.5179, 0.4971, 0.5385],\n",
       "            [0.5444, 0.5052, 0.4884, 0.5396],\n",
       "            [0.5446, 0.5231, 0.4817, 0.5411],\n",
       "            [0.5272, 0.5127, 0.4897, 0.5454],\n",
       "            [0.5355, 0.5134, 0.4906, 0.5370],\n",
       "            [0.5437, 0.5134, 0.4863, 0.5533],\n",
       "            [0.5440, 0.5077, 0.4991, 0.5419],\n",
       "            [0.5230, 0.5059, 0.4890, 0.5392],\n",
       "            [0.5405, 0.5051, 0.4918, 0.5448]]], device='cuda:0')}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
