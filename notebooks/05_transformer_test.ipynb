{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjin/anaconda3/envs/conda_visual_HPE/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  1.13 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djjin/Mygit/X-Decoder\n",
      "['xdecoder', 'body', 'decoder', 'xdecoder']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid MIT-MAGIC-COOKIE-1 keyWARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
      "WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/ADE20K_2021_17_01/images_detectron2/training\n",
      "datasets/ADE20K_2021_17_01/images_detectron2/validation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "pth = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.insert(0, pth)\n",
    "\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "home_dir = os.path.abspath(os.getcwd()+\"/../\")\n",
    "sys.path.append(home_dir)\n",
    "print(home_dir)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils.arguments import load_opt_command\n",
    "from utils.misc import hook_metadata, hook_switcher, hook_opt\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.structures import Boxes, ImageList, Instances, BitMasks, BoxMode\n",
    "from xdecoder.BaseModel import BaseModel\n",
    "from xdecoder import build_model\n",
    "from utils.visualizer import Visualizer\n",
    "from utils.distributed import init_distributed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from utils.arguments import load_opt_from_config_files, load_config_dict_to_opt\n",
    "from datasets import build_evaluator, build_eval_dataloader\n",
    "from xdecoder.utils import get_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Pretrain or fine-tune models for NLP tasks.')\n",
    "parser.add_argument('--command', default=\"evaluate\", help='Command: train/evaluate/train-and-evaluate')\n",
    "parser.add_argument('--conf_files', nargs='+', help='Path(s) to the config file(s).')\n",
    "parser.add_argument('--user_dir', help='Path to the user defined module for tasks (models, criteria), optimizers, and lr schedulers.')\n",
    "parser.add_argument('--config_overrides', nargs='*', help='Override parameters on config with a json style string, e.g. {\"<PARAM_NAME_1>\": <PARAM_VALUE_1>, \"<PARAM_GROUP_2>.<PARAM_SUBGROUP_2>.<PARAM_2>\": <PARAM_VALUE_2>}. A key with \".\" updates the object in the corresponding nested dict. Remember to escape \" in command line.')\n",
    "parser.add_argument('--overrides', help='arguments that used to override the config file in cmdline', nargs=argparse.REMAINDER)\n",
    "\n",
    "cmdline_args = parser.parse_args('')\n",
    "cmdline_args.conf_files = [os.path.join(home_dir, \"configs/xdecoder/svlp_focalt_lang.yaml\")]\n",
    "cmdline_args.overrides = ['WEIGHT', '../checkpoints/xdecoder_focalt_best_openseg.pt'] \n",
    "cmdline_args.overrides\n",
    "\n",
    "opt = load_opt_from_config_files(cmdline_args.conf_files)\n",
    "\n",
    "keys = [cmdline_args.overrides[idx*2] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [cmdline_args.overrides[idx*2+1] for idx in range(len(cmdline_args.overrides)//2)]\n",
    "vals = [val.replace('false', '').replace('False','') if len(val.replace(' ', '')) == 5 else val for val in vals]\n",
    "types = []\n",
    "for key in keys:\n",
    "    key = key.split('.')\n",
    "    ele = opt.copy()\n",
    "    while len(key) > 0:\n",
    "        ele = ele[key.pop(0)]\n",
    "    types.append(type(ele))\n",
    "\n",
    "config_dict = {x:z(y) for x,y,z in zip(keys, vals, types)}\n",
    "config_dict\n",
    "\n",
    "load_config_dict_to_opt(opt, config_dict)\n",
    "for key, val in cmdline_args.__dict__.items():\n",
    "    if val is not None:\n",
    "        opt[key] = val\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../checkpoints/xdecoder_focalt_best_openseg.pt\n"
     ]
    }
   ],
   "source": [
    "pretrained_pth = os.path.join(opt['WEIGHT'])\n",
    "output_root = './output'\n",
    "image_pth = '../images/animals.png'\n",
    "print(pretrained_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_image(file_name, figsize=(16,10)):\n",
    "    raw_image = Image.open(file_name)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(raw_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = home_dir + '/images'\n",
    "# images = [os.path.join(img_root, f ) for f in os.listdir(img_root)]\n",
    "# coco_image_paths = []\n",
    "\n",
    "# for image_pth in images:\n",
    "#     if os.path.isdir(image_pth):\n",
    "#         continue\n",
    "#     if \"COCO\" in image_pth:\n",
    "#         coco_image_paths.append(image_pth)\n",
    "#         plot_image(image_pth, figsize=(4, 4))\n",
    "coco_image_paths = [os.path.join(img_root, 'COCO_val2014_000000385029.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/djjin/Mygit/X-Decoder/images/COCO_val2014_000000385029.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Optional, List\n",
    "# from torch import Tensor\n",
    "# import datasets.transforms as T\n",
    "\n",
    "# def _max_by_axis(the_list):\n",
    "#     # type: (List[List[int]]) -> List[int]\n",
    "#     maxes = the_list[0]\n",
    "#     for sublist in the_list[1:]:\n",
    "#         for index, item in enumerate(sublist):\n",
    "#             maxes[index] = max(maxes[index], item)\n",
    "#     return maxes\n",
    "\n",
    "# class NestedTensor(object):\n",
    "#     def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "#         self.tensors = tensors\n",
    "#         self.mask = mask\n",
    "\n",
    "#     def to(self, device):\n",
    "#         cast_tensor = self.tensors.to(device)\n",
    "#         mask = self.mask\n",
    "#         if mask is not None:\n",
    "#             assert mask is not None\n",
    "#             cast_mask = mask.to(device)\n",
    "#         else:\n",
    "#             cast_mask = None\n",
    "#         return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "#     def decompose(self):\n",
    "#         return self.tensors, self.mask\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return str(self.tensors)\n",
    "\n",
    "\n",
    "# def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
    "#     if tensor_list[0].ndim == 3:\n",
    "#         max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
    "#         batch_shape = [len(tensor_list)] + max_size\n",
    "#         b, c, h, w = batch_shape\n",
    "#         dtype = tensor_list[0].dtype\n",
    "#         device = tensor_list[0].device\n",
    "#         tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "#         mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
    "#         for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
    "#             pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "#             m[: img.shape[1], : img.shape[2]] = False\n",
    "#     else:\n",
    "#         raise ValueError(\"not supported\")\n",
    "#     return NestedTensor(tensor, mask)\n",
    "\n",
    "# def load_image(transform, file_path_list, device):\n",
    "#     raw_image_list = []\n",
    "#     size_list = []\n",
    "#     for file_path in file_path_list:\n",
    "#         raw_image = Image.open(file_path).convert(\"RGB\")\n",
    "#         img_w, img_h = raw_image.size\n",
    "#         raw_image_list.append(raw_image)  \n",
    "#         size_list.append(torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32).to(device))\n",
    "\n",
    "#     image = [transform(raw_image)[0].to(device) for raw_image in raw_image_list]\n",
    "#     image = nested_tensor_from_tensor_list(image)\n",
    "#     size = torch.stack(size_list, dim = 0)\n",
    "#     return image, raw_image_list, size\n",
    "\n",
    "# def make_vcoco_transforms(image_set):\n",
    "\n",
    "#     normalize = T.Compose([\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "\n",
    "#     if image_set == 'train':\n",
    "#         return T.Compose([\n",
    "#             T.RandomHorizontalFlip(),\n",
    "#             T.ColorJitter(.4, .4, .4),\n",
    "#             T.RandomSelect(\n",
    "#                 T.RandomResize(scales, max_size=1333),\n",
    "#                 T.Compose([\n",
    "#                     T.RandomResize([400, 500, 600]),\n",
    "#                     T.RandomSizeCrop(384, 600),\n",
    "#                     T.RandomResize(scales, max_size=1333),\n",
    "#                 ])\n",
    "#             ),\n",
    "#             normalize,\n",
    "#         ])\n",
    "\n",
    "#     if image_set == 'val':\n",
    "#         return T.Compose([\n",
    "#             T.RandomResize([800], max_size=1333),\n",
    "#             normalize,\n",
    "#         ])\n",
    "\n",
    "#     raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "# transform = make_vcoco_transforms(image_set = 'val')\n",
    "# samples, org_images, orig_target_sizes = load_image(transform, coco_image_paths, device='cuda')\n",
    "# samples.tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*UNLOADED* sem_seg_head.predictor.pos_embed_caping.weight, Model Shape: torch.Size([77, 512])\n",
      "$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])\n",
      "$UNUSED$ sem_seg_head.predictor.query_feat_caping.weight, Ckpt Shape: torch.Size([77, 512])\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained_pth).eval().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 800, 1088])\n",
      "torch.Size([1, 768, 25, 34])\n",
      "torch.Size([1, 384, 50, 68])\n",
      "torch.Size([1, 192, 100, 136])\n",
      "torch.Size([1, 96, 200, 272])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "focal_backbone = deepcopy(model.model.backbone)\n",
    "t = []\n",
    "t.append(transforms.Resize(800, interpolation=Image.BICUBIC))\n",
    "transform = transforms.Compose(t)\n",
    "\n",
    "pixel_mean = torch.Tensor([123.675, 116.280, 103.530]).view(-1, 1, 1).cuda()\n",
    "pixel_std = torch.Tensor([58.395, 57.120, 57.375]).view(-1, 1, 1).cuda()\n",
    "\n",
    "# focal_backbone.eval()\n",
    "# evaluate() function in xdecoder_model.py\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_ori = Image.open(coco_image_paths[-1]).convert('RGB')\n",
    "    width = image_ori.size[0]\n",
    "    height = image_ori.size[1]\n",
    "    image = transform(image_ori)\n",
    "    image = np.asarray(image)\n",
    "    image_ori = np.asarray(image_ori)\n",
    "    images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "    batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "    images = [x[\"image\"].to('cuda') for x in batch_inputs]\n",
    "    images = [(x - pixel_mean) / pixel_std for x in images]\n",
    "    \n",
    "    images = ImageList.from_tensors(images, 32)\n",
    "    img_bs = images.tensor.shape[0]\n",
    "\n",
    "    print(images.tensor.shape)\n",
    "    features = focal_backbone(images.tensor)\n",
    "    for idx, f in enumerate(list(features.keys())[::-1]):\n",
    "        x = features[f]\n",
    "        print(x.shape)\n",
    "        # lateral_conv = self.lateral_convs[idx]\n",
    "        # output_conv = self.output_convs[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "t.append(transforms.Resize(800, interpolation=Image.BICUBIC))\n",
    "transform = transforms.Compose(t)\n",
    "\n",
    "thing_classes = ['zebra','antelope','giraffe','ostrich','sky','water','grass','sand','tree']\n",
    "thing_colors = [random_color(rgb=True, maximum=255).astype(np.int).tolist() for _ in range(len(thing_classes))]\n",
    "thing_dataset_id_to_contiguous_id = {x:x for x in range(len(thing_classes))}\n",
    "\n",
    "MetadataCatalog.get(\"demo\").set(\n",
    "    thing_colors=thing_colors,\n",
    "    thing_classes=thing_classes,\n",
    "    thing_dataset_id_to_contiguous_id=thing_dataset_id_to_contiguous_id,\n",
    ")\n",
    "\n",
    "model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(thing_classes + [\"background\"], is_eval=False)\n",
    "metadata = MetadataCatalog.get('demo')\n",
    "model.model.metadata = metadata\n",
    "model.model.sem_seg_head.num_classes = len(thing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['res5', 'res4', 'res3', 'res2']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    image_ori = Image.open(image_pth).convert('RGB')\n",
    "    width = image_ori.size[0]\n",
    "    height = image_ori.size[1]\n",
    "    image = transform(image_ori)\n",
    "    image = np.asarray(image)\n",
    "    image_ori = np.asarray(image_ori)\n",
    "    images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "    batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "    outputs = model.forward(batch_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.sem_seg_head.state_dict().keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "\n",
    "# # from xdecoder.backbone.focal import D2FocalNet\n",
    "# focal_backbone = deepcopy(model.model.backbone)\n",
    "\n",
    "# # from xdecoder.body.encoder.transformer_encoder_fpn import TransformerEncoderPixelDecoder\n",
    "# pixel_decoder = deepcopy(model.model.sem_seg_head.pixel_decoder)\n",
    "\n",
    "# # from xdecoder.body.decoder import TransformerEncoderPixelDecoder\n",
    "# predictor = model.model.sem_seg_head.predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.sem_seg_head.predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel_mean = torch.Tensor([123.675, 116.280, 103.530]).view(-1, 1, 1).cuda()\n",
    "# pixel_std = torch.Tensor([58.395, 57.120, 57.375]).view(-1, 1, 1).cuda()\n",
    "\n",
    "# # focal_backbone.eval()\n",
    "# # evaluate() function in xdecoder_model.py\n",
    "# with torch.no_grad():\n",
    "#     image_ori = Image.open(image_pth).convert('RGB')\n",
    "#     width = image_ori.size[0]\n",
    "#     height = image_ori.size[1]\n",
    "#     image = transform(image_ori)\n",
    "#     image = np.asarray(image)\n",
    "#     image_ori = np.asarray(image_ori)\n",
    "#     images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "#     batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "#     images = [x[\"image\"].to('cuda') for x in batch_inputs]\n",
    "#     images = [(x - pixel_mean) / pixel_std for x in images]\n",
    "    \n",
    "#     images = ImageList.from_tensors(images, 32)\n",
    "#     img_bs = images.tensor.shape[0]\n",
    "\n",
    "#     # Process\n",
    "#     # 1. image -> focal_backbone -> features\n",
    "#     # 2. features -> TransformerEncoderPixelDecoder -> mask_features, transformer_encoder_features, multi_scale_features\n",
    "\n",
    "#     features = focal_backbone(images.tensor)\n",
    "#     print(features['res5'].shape)\n",
    "#     mask_features, transformer_encoder_features, multi_scale_features = pixel_decoder(features)\n",
    "    \n",
    "#     print(f\"mask_features {mask_features.shape}\")\n",
    "#     print(f\"transformer_encoder_features {transformer_encoder_features.shape}\")\n",
    "#     print(f\"multi_scale_features {multi_scale_features[0].shape}\")\n",
    "\n",
    "#     # predictor를 수정하면 될듯\n",
    "#     predictions = predictor(multi_scale_features, mask_features, None, None, None, \"seg\", {})\n",
    "#     for key, value in predictions.items():\n",
    "#         if not isinstance(predictions[key], list):\n",
    "#             print(key, predictions[key].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = []\n",
    "# t.append(transforms.Resize(800, interpolation=Image.BICUBIC))\n",
    "# transform = transforms.Compose(t)\n",
    "\n",
    "# thing_classes = ['zebra','antelope','giraffe','ostrich','sky','water','grass','sand','tree']\n",
    "# thing_colors = [random_color(rgb=True, maximum=255).astype(np.int).tolist() for _ in range(len(thing_classes))]\n",
    "# thing_dataset_id_to_contiguous_id = {x:x for x in range(len(thing_classes))}\n",
    "\n",
    "# MetadataCatalog.get(\"demo\").set(\n",
    "#     thing_colors=thing_colors,\n",
    "#     thing_classes=thing_classes,\n",
    "#     thing_dataset_id_to_contiguous_id=thing_dataset_id_to_contiguous_id,\n",
    "# )\n",
    "# model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(thing_classes + [\"background\"], is_eval=False)\n",
    "# metadata = MetadataCatalog.get('demo')\n",
    "# model.model.metadata = metadata\n",
    "# model.model.sem_seg_head.num_classes = len(thing_classes)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     image_ori = Image.open(image_pth).convert('RGB')\n",
    "#     width = image_ori.size[0]\n",
    "#     height = image_ori.size[1]\n",
    "#     image = transform(image_ori)\n",
    "#     image = np.asarray(image)\n",
    "#     image_ori = np.asarray(image_ori)\n",
    "#     images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "\n",
    "#     batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "#     outputs = model.forward(batch_inputs)\n",
    "#     visual = Visualizer(image_ori, metadata=metadata)\n",
    "\n",
    "#     inst_seg = outputs[-1]['instances']\n",
    "#     inst_seg.pred_masks = inst_seg.pred_masks.cpu()\n",
    "#     inst_seg.pred_boxes = BitMasks(inst_seg.pred_masks > 0).get_bounding_boxes()\n",
    "#     demo = visual.draw_instance_predictions(inst_seg) # rgb Image\n",
    "\n",
    "#     if not os.path.exists(output_root):\n",
    "#         os.makedirs(output_root)\n",
    "#     demo.save(os.path.join(output_root, 'inst.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_X_decoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
